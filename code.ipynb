{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Loaded Successfully!\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 29531 entries, 0 to 29530\n",
      "Data columns (total 16 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   City        29531 non-null  object \n",
      " 1   Date        29531 non-null  object \n",
      " 2   PM2.5       24933 non-null  float64\n",
      " 3   PM10        18391 non-null  float64\n",
      " 4   NO          25949 non-null  float64\n",
      " 5   NO2         25946 non-null  float64\n",
      " 6   NOx         25346 non-null  float64\n",
      " 7   NH3         19203 non-null  float64\n",
      " 8   CO          27472 non-null  float64\n",
      " 9   SO2         25677 non-null  float64\n",
      " 10  O3          25509 non-null  float64\n",
      " 11  Benzene     23908 non-null  float64\n",
      " 12  Toluene     21490 non-null  float64\n",
      " 13  Xylene      11422 non-null  float64\n",
      " 14  AQI         24850 non-null  float64\n",
      " 15  AQI_Bucket  24850 non-null  object \n",
      "dtypes: float64(13), object(3)\n",
      "memory usage: 3.6+ MB\n",
      "None\n",
      "        City        Date  PM2.5  PM10     NO    NO2    NOx  NH3     CO    SO2  \\\n",
      "0  Ahmedabad  2015-01-01    NaN   NaN   0.92  18.22  17.15  NaN   0.92  27.64   \n",
      "1  Ahmedabad  2015-01-02    NaN   NaN   0.97  15.69  16.46  NaN   0.97  24.55   \n",
      "2  Ahmedabad  2015-01-03    NaN   NaN  17.40  19.30  29.70  NaN  17.40  29.07   \n",
      "3  Ahmedabad  2015-01-04    NaN   NaN   1.70  18.48  17.97  NaN   1.70  18.59   \n",
      "4  Ahmedabad  2015-01-05    NaN   NaN  22.10  21.42  37.76  NaN  22.10  39.33   \n",
      "\n",
      "       O3  Benzene  Toluene  Xylene  AQI AQI_Bucket  \n",
      "0  133.36     0.00     0.02    0.00  NaN        NaN  \n",
      "1   34.06     3.68     5.50    3.77  NaN        NaN  \n",
      "2   30.70     6.80    16.40    2.25  NaN        NaN  \n",
      "3   36.08     4.43    10.14    1.00  NaN        NaN  \n",
      "4   39.31     7.01    18.89    2.78  NaN        NaN  \n",
      "\n",
      "Missing Values Per Column:\n",
      "City              0\n",
      "Date              0\n",
      "PM2.5          4598\n",
      "PM10          11140\n",
      "NO             3582\n",
      "NO2            3585\n",
      "NOx            4185\n",
      "NH3           10328\n",
      "CO             2059\n",
      "SO2            3854\n",
      "O3             4022\n",
      "Benzene        5623\n",
      "Toluene        8041\n",
      "Xylene        18109\n",
      "AQI            4681\n",
      "AQI_Bucket     4681\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('/Users/me.pranesh/Desktop/new clean/city_day.csv')\n",
    "\n",
    "print(\"Dataset Loaded Successfully!\")\n",
    "print(df.info())\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nMissing Values Per Column:\")\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing Values After Dropping Unwanted Rows:\n",
      "City             0\n",
      "Date             0\n",
      "PM2.5          759\n",
      "PM10          7301\n",
      "NO             385\n",
      "NO2            395\n",
      "NOx           2006\n",
      "NH3           6814\n",
      "CO             448\n",
      "SO2            593\n",
      "O3             840\n",
      "AQI            902\n",
      "AQI_Bucket     902\n",
      "dtype: int64\n",
      "\n",
      "Updated Dataset Shape: (25692, 13)\n"
     ]
    }
   ],
   "source": [
    "df.drop(columns=['Xylene', 'Toluene', 'Benzene'], inplace=True)\n",
    "\n",
    "pollutants = ['PM2.5', 'PM10', 'NO', 'NO2', 'NOx', 'NH3', 'CO', 'SO2', 'O3', 'AQI']\n",
    "\n",
    "df.dropna(subset=pollutants, how='all', inplace=True)\n",
    "\n",
    "df.dropna(subset=['PM2.5', 'PM10'], how='all', inplace=True)\n",
    "\n",
    "print(\"\\nMissing Values After Dropping Unwanted Rows:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(\"\\nUpdated Dataset Shape:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "City\n",
      "Delhi                 2007\n",
      "Bengaluru             1925\n",
      "Lucknow               1907\n",
      "Hyderabad             1893\n",
      "Chennai               1892\n",
      "Patna                 1537\n",
      "Gurugram              1525\n",
      "Ahmedabad             1382\n",
      "Visakhapatnam         1237\n",
      "Amritsar              1177\n",
      "Jaipur                1102\n",
      "Thiruvananthapuram    1077\n",
      "Amaravati              896\n",
      "Jorapokhar             896\n",
      "Mumbai                 784\n",
      "Brajrajnagar           775\n",
      "Talcher                771\n",
      "Kolkata                759\n",
      "Guwahati               502\n",
      "Coimbatore             380\n",
      "Chandigarh             304\n",
      "Bhopal                 280\n",
      "Shillong               251\n",
      "Kochi                  162\n",
      "Ernakulam              158\n",
      "Aizawl                 113\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['City'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Updated Dataset Shape After Dropping Cities: (24044, 13)\n",
      "\n",
      "Remaining Cities:\n",
      "City\n",
      "Delhi                 2007\n",
      "Bengaluru             1925\n",
      "Lucknow               1907\n",
      "Hyderabad             1893\n",
      "Chennai               1892\n",
      "Patna                 1537\n",
      "Gurugram              1525\n",
      "Ahmedabad             1382\n",
      "Visakhapatnam         1237\n",
      "Amritsar              1177\n",
      "Jaipur                1102\n",
      "Thiruvananthapuram    1077\n",
      "Amaravati              896\n",
      "Jorapokhar             896\n",
      "Mumbai                 784\n",
      "Brajrajnagar           775\n",
      "Talcher                771\n",
      "Kolkata                759\n",
      "Guwahati               502\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "cities_to_drop = ['Coimbatore', 'Chandigarh', 'Bhopal', 'Shillong', 'Kochi', 'Ernakulam', 'Aizawl']\n",
    "\n",
    "df = df[~df['City'].isin(cities_to_drop)]\n",
    "\n",
    "print(\"\\nUpdated Dataset Shape After Dropping Cities:\", df.shape)\n",
    "\n",
    "print(\"\\nRemaining Cities:\")\n",
    "print(df['City'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate Rows: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Duplicate Rows:\", df.duplicated().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "numeric_df = df.select_dtypes(include=[np.number])\n",
    "\n",
    "corr_matrix = numeric_df.corr()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title(\"Feature Correlation Heatmap\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Updated Dataset Shape After Dropping NOx: (24044, 12)\n",
      "\n",
      "Remaining Columns: ['City', 'Date', 'PM2.5', 'PM10', 'NO', 'NO2', 'NH3', 'CO', 'SO2', 'O3', 'AQI', 'AQI_Bucket']\n"
     ]
    }
   ],
   "source": [
    "df.drop(columns=['NOx'], inplace=True)\n",
    "\n",
    "print(\"\\nUpdated Dataset Shape After Dropping NOx:\", df.shape)\n",
    "\n",
    "print(\"\\nRemaining Columns:\", df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total Outliers Detected: 1203 (5.00% of dataset)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "numeric_cols = ['PM2.5', 'PM10', 'NO', 'NO2', 'NH3', 'CO', 'SO2', 'O3', 'AQI']\n",
    "\n",
    "iso_forest = IsolationForest(contamination=0.05, random_state=42)  # Assuming 5% contamination (adjustable)\n",
    "\n",
    "df['Outlier'] = iso_forest.fit_predict(df[numeric_cols])\n",
    "\n",
    "outlier_count = (df['Outlier'] == -1).sum()\n",
    "total_count = df.shape[0]\n",
    "outlier_percentage = (outlier_count / total_count) * 100\n",
    "\n",
    "print(f\"\\nTotal Outliers Detected: {outlier_count} ({outlier_percentage:.2f}% of dataset)\")\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=df[numeric_cols])\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Boxplot of Features (Potential Outliers)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Outliers Capped. Updated Statistics:\n",
      "              PM2.5          PM10            NO           NO2           NH3  \\\n",
      "count  23309.000000  16755.000000  23721.000000  23723.000000  17379.000000   \n",
      "mean      68.724437    122.617810     17.418253     29.224013     23.652715   \n",
      "std       59.073637     87.456556     20.135329     22.889968     22.526597   \n",
      "min        8.730800     18.273200      0.930000      3.000000      1.447800   \n",
      "25%       30.370000     61.125000      5.900000     12.435000      9.130000   \n",
      "50%       50.600000     99.770000     10.090000     22.680000     16.250000   \n",
      "75%       83.300000    154.775000     20.380000     38.565000     30.775000   \n",
      "max      317.939200    451.033600    113.466000    120.642400    134.853000   \n",
      "\n",
      "                 CO           SO2            O3           AQI  \n",
      "count  23596.000000  23451.000000  23373.000000  23244.000000  \n",
      "mean       2.213990     14.538151     34.961162    168.643126  \n",
      "std        5.235839     16.557646     20.821542    123.873440  \n",
      "min        0.000000      1.310000      3.347200     38.000000  \n",
      "25%        0.590000      5.665000     19.580000     84.000000  \n",
      "50%        0.940000      9.230000     31.380000    122.000000  \n",
      "75%        1.510000     15.640000     46.060000    218.000000  \n",
      "max       36.351000    102.305000    107.055200    684.570000  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "numeric_cols = ['PM2.5', 'PM10', 'NO', 'NO2', 'NH3', 'CO', 'SO2', 'O3', 'AQI']\n",
    "\n",
    "lower_bound = df[numeric_cols].quantile(0.01)\n",
    "upper_bound = df[numeric_cols].quantile(0.99)\n",
    "\n",
    "df[numeric_cols] = df[numeric_cols].clip(lower=lower_bound, upper=upper_bound, axis=1)\n",
    "\n",
    "print(\"\\nOutliers Capped. Updated Statistics:\")\n",
    "print(df[numeric_cols].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Types Before Processing:\n",
      "City           object\n",
      "Date           object\n",
      "PM2.5         float64\n",
      "PM10          float64\n",
      "NO            float64\n",
      "NO2           float64\n",
      "NH3           float64\n",
      "CO            float64\n",
      "SO2           float64\n",
      "O3            float64\n",
      "AQI           float64\n",
      "AQI_Bucket     object\n",
      "Outlier         int64\n",
      "dtype: object\n",
      "\n",
      "Total Duplicate Rows: 0\n",
      "\n",
      "Negative Value Count Per Column:\n",
      "PM2.5    0\n",
      "PM10     0\n",
      "NO       0\n",
      "NO2      0\n",
      "NH3      0\n",
      "CO       0\n",
      "SO2      0\n",
      "O3       0\n",
      "AQI      0\n",
      "dtype: int64\n",
      "\n",
      "Final Dataset Info After Preprocessing Check:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 24044 entries, 27 to 29530\n",
      "Data columns (total 13 columns):\n",
      " #   Column      Non-Null Count  Dtype         \n",
      "---  ------      --------------  -----         \n",
      " 0   City        24044 non-null  object        \n",
      " 1   Date        24044 non-null  datetime64[ns]\n",
      " 2   PM2.5       23309 non-null  object        \n",
      " 3   PM10        16755 non-null  object        \n",
      " 4   NO          23721 non-null  object        \n",
      " 5   NO2         23723 non-null  object        \n",
      " 6   NH3         17379 non-null  object        \n",
      " 7   CO          23596 non-null  object        \n",
      " 8   SO2         23451 non-null  object        \n",
      " 9   O3          23373 non-null  object        \n",
      " 10  AQI         23244 non-null  object        \n",
      " 11  AQI_Bucket  23244 non-null  object        \n",
      " 12  Outlier     24044 non-null  int64         \n",
      "dtypes: datetime64[ns](1), int64(1), object(11)\n",
      "memory usage: 2.6+ MB\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5c/ttrql3v94vx3w3dr_82ptp280000gn/T/ipykernel_8823/515069750.py:22: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df[numeric_cols] = df[numeric_cols].applymap(lambda x: x if x >= 0 else pd.NA)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(\"\\nData Types Before Processing:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "\n",
    "duplicate_count = df.duplicated().sum()\n",
    "print(f\"\\nTotal Duplicate Rows: {duplicate_count}\")\n",
    "\n",
    "numeric_cols = ['PM2.5', 'PM10', 'NO', 'NO2', 'NH3', 'CO', 'SO2', 'O3', 'AQI']\n",
    "negative_values = (df[numeric_cols] < 0).sum()\n",
    "\n",
    "print(\"\\nNegative Value Count Per Column:\")\n",
    "print(negative_values)\n",
    "\n",
    "df[numeric_cols] = df[numeric_cols].applymap(lambda x: x if x >= 0 else pd.NA)\n",
    "\n",
    "print(\"\\nFinal Dataset Info After Preprocessing Check:\")\n",
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Updated Data Types:\n",
      "City                  object\n",
      "Date          datetime64[ns]\n",
      "PM2.5                float64\n",
      "PM10                 float64\n",
      "NO                   float64\n",
      "NO2                  float64\n",
      "NH3                  float64\n",
      "CO                   float64\n",
      "SO2                  float64\n",
      "O3                   float64\n",
      "AQI                  float64\n",
      "AQI_Bucket            object\n",
      "Outlier                int64\n",
      "dtype: object\n",
      "\n",
      "Non-Numeric Value Check (Should be 0):\n",
      "PM2.5    0\n",
      "PM10     0\n",
      "NO       0\n",
      "NO2      0\n",
      "NH3      0\n",
      "CO       0\n",
      "SO2      0\n",
      "O3       0\n",
      "AQI      0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5c/ttrql3v94vx3w3dr_82ptp280000gn/T/ipykernel_8823/388464787.py:11: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  print(df[numeric_cols].applymap(lambda x: isinstance(x, str)).sum())\n"
     ]
    }
   ],
   "source": [
    "numeric_cols = ['PM2.5', 'PM10', 'NO', 'NO2', 'NH3', 'CO', 'SO2', 'O3', 'AQI']\n",
    "df[numeric_cols] = df[numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "print(\"\\nUpdated Data Types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(\"\\nNon-Numeric Value Check (Should be 0):\")\n",
    "print(df[numeric_cols].applymap(lambda x: isinstance(x, str)).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset After Setting Date as Index:\n",
      "             City   PM2.5      PM10     NO    NO2     NH3     CO   SO2     O3  \\\n",
      "Date                                                                            \n",
      "2015-01-01  Delhi  313.22  451.0336  69.16  36.39   33.85  15.20  9.25  41.68   \n",
      "2015-01-02  Delhi  186.18  269.5500  62.09  32.87   31.83   9.54  6.65  29.97   \n",
      "2015-01-03  Delhi   87.18  131.9000  25.73  30.31   69.55  10.61  2.65  19.71   \n",
      "2015-01-04  Delhi  151.84  241.8400  25.01  36.91  130.36  11.54  4.63  25.36   \n",
      "2015-01-05  Delhi  146.60  219.1300  14.01  34.92  122.88   9.20  3.33  23.20   \n",
      "\n",
      "              AQI AQI_Bucket  Outlier  \n",
      "Date                                   \n",
      "2015-01-01  472.0     Severe       -1  \n",
      "2015-01-02  454.0     Severe        1  \n",
      "2015-01-03  143.0   Moderate        1  \n",
      "2015-01-04  319.0  Very Poor        1  \n",
      "2015-01-05  325.0  Very Poor        1  \n"
     ]
    }
   ],
   "source": [
    "df.set_index('Date', inplace=True)\n",
    "df.sort_index(inplace=True)\n",
    "\n",
    "print(\"\\nDataset After Setting Date as Index:\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing Values After Imputation (Excluding AQI & AQI_Bucket):\n",
      "City            0\n",
      "PM2.5           0\n",
      "PM10            0\n",
      "NO              0\n",
      "NO2             0\n",
      "NH3             0\n",
      "CO              0\n",
      "SO2             0\n",
      "O3              0\n",
      "AQI           800\n",
      "AQI_Bucket    800\n",
      "Outlier         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer  # Enable it explicitly\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "iter_imputer = IterativeImputer(random_state=42)\n",
    "df[['PM2.5', 'PM10']] = iter_imputer.fit_transform(df[['PM2.5', 'PM10']])\n",
    "\n",
    "df[['NO', 'NO2', 'CO', 'SO2', 'O3']] = df[['NO', 'NO2', 'CO', 'SO2', 'O3']].interpolate(method='time')\n",
    "\n",
    "df[['NH3']] = iter_imputer.fit_transform(df[['NH3']])\n",
    "\n",
    "print(\"\\nMissing Values After Imputation (Excluding AQI & AQI_Bucket):\")\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing Values After AQI Imputation:\n",
      "City          0\n",
      "PM2.5         0\n",
      "PM10          0\n",
      "NO            0\n",
      "NO2           0\n",
      "NH3           0\n",
      "CO            0\n",
      "SO2           0\n",
      "O3            0\n",
      "AQI           0\n",
      "AQI_Bucket    0\n",
      "Outlier       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "aqi_breakpoints = {\n",
    "    'PM2.5': [(0, 30, 0, 50), (31, 60, 51, 100), (61, 90, 101, 200), (91, 120, 201, 300),\n",
    "              (121, 250, 301, 400), (251, 500, 401, 500)],\n",
    "    'PM10': [(0, 50, 0, 50), (51, 100, 51, 100), (101, 250, 101, 200), (251, 350, 201, 300),\n",
    "             (351, 430, 301, 400), (431, 600, 401, 500)],\n",
    "    'NO2': [(0, 40, 0, 50), (41, 80, 51, 100), (81, 180, 101, 200), (181, 280, 201, 300),\n",
    "            (281, 400, 301, 400), (401, 1000, 401, 500)],\n",
    "    'SO2': [(0, 40, 0, 50), (41, 80, 51, 100), (81, 380, 101, 200), (381, 800, 201, 300),\n",
    "            (801, 1600, 301, 400), (1601, 2000, 401, 500)],\n",
    "    'CO': [(0, 1, 0, 50), (1.1, 2, 51, 100), (2.1, 10, 101, 200), (10.1, 17, 201, 300),\n",
    "           (17.1, 34, 301, 400), (34.1, 50, 401, 500)],\n",
    "    'O3': [(0, 50, 0, 50), (51, 100, 51, 100), (101, 168, 101, 200), (169, 208, 201, 300),\n",
    "           (209, 748, 301, 400), (749, 1000, 401, 500)]\n",
    "}\n",
    "\n",
    "def calculate_aqi(concentration, breakpoints):\n",
    "    \"\"\"Compute AQI using breakpoints.\"\"\"\n",
    "    for C_low, C_high, I_low, I_high in breakpoints:\n",
    "        if C_low <= concentration <= C_high:\n",
    "            return ((I_high - I_low) / (C_high - C_low)) * (concentration - C_low) + I_low\n",
    "    return None  # If value is out of range\n",
    "\n",
    "def compute_aqi_for_row(row):\n",
    "    aqi_values = [\n",
    "        calculate_aqi(row[pollutant], breakpoints)\n",
    "        for pollutant, breakpoints in aqi_breakpoints.items()\n",
    "        if not np.isnan(row[pollutant])\n",
    "    ]\n",
    "    aqi_values = [val for val in aqi_values if val is not None]\n",
    "    return max(aqi_values) if aqi_values else np.nan  # Take the worst AQI (max value)\n",
    "\n",
    "df.loc[df['AQI'].isnull(), 'AQI'] = df[df['AQI'].isnull()].apply(compute_aqi_for_row, axis=1)\n",
    "\n",
    "bins = [0, 50, 100, 200, 300, 400, float('inf')]\n",
    "labels = ['Good', 'Satisfactory', 'Moderate', 'Poor', 'Very Poor', 'Severe']\n",
    "df['AQI_Bucket'] = pd.cut(df['AQI'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "print(\"\\nMissing Values After AQI Imputation:\")\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 Total Flagged Rows: 939 (3.91%) of dataset.\n",
      "\n",
      "Sample Flagged Rows:\n",
      "               AQI  Computed_AQI  AQI_Difference\n",
      "Date                                            \n",
      "2015-01-31  514.00    343.704734      170.295266\n",
      "2015-02-01  684.57    415.015660      269.554340\n",
      "2015-02-02  684.57    415.015660      269.554340\n",
      "2015-02-03  660.00    380.141420      279.858580\n",
      "2015-02-04  294.00    168.081034      125.918966\n",
      "2015-02-08  379.00    186.465823      192.534177\n",
      "2015-02-11  388.00    154.134177      233.865823\n",
      "2015-02-13  510.00    307.326627      202.673373\n",
      "2015-02-14  684.57    415.015660      269.554340\n",
      "2015-02-15  475.00    279.195652      195.804348\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "df[\"Computed_AQI\"] = df.apply(compute_aqi_for_row, axis=1)\n",
    "\n",
    "df[\"AQI_Difference\"] = abs(df[\"AQI\"] - df[\"Computed_AQI\"])\n",
    "\n",
    "mean_diff = df[\"AQI_Difference\"].mean()\n",
    "std_diff = df[\"AQI_Difference\"].std()\n",
    "\n",
    "threshold = mean_diff + 2 * std_diff\n",
    "\n",
    "df[\"Flagged_AQI_Anomaly\"] = (df[\"AQI_Difference\"] > threshold).astype(int)\n",
    "\n",
    "num_flagged = df[\"Flagged_AQI_Anomaly\"].sum()\n",
    "print(f\"🚨 Total Flagged Rows: {num_flagged} ({(num_flagged / len(df)) * 100:.2f}%) of dataset.\")\n",
    "\n",
    "print(\"\\nSample Flagged Rows:\")\n",
    "print(df[df[\"Flagged_AQI_Anomaly\"] == 1][[\"AQI\", \"Computed_AQI\", \"AQI_Difference\"]].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Rows Modified: 939\n",
      "\n",
      "Sample Updated Rows:\n",
      "            Reported_AQI  Computed_AQI         AQI\n",
      "Date                                              \n",
      "2015-01-31        514.00    343.704734  343.704734\n",
      "2015-02-01        684.57    415.015660  415.015660\n",
      "2015-02-02        684.57    415.015660  415.015660\n",
      "2015-02-03        660.00    380.141420  380.141420\n",
      "2015-02-04        294.00    168.081034  168.081034\n",
      "2015-02-08        379.00    186.465823  186.465823\n",
      "2015-02-11        388.00    154.134177  154.134177\n",
      "2015-02-13        510.00    307.326627  307.326627\n",
      "2015-02-14        684.57    415.015660  415.015660\n",
      "2015-02-15        475.00    279.195652  279.195652\n"
     ]
    }
   ],
   "source": [
    "df['Reported_AQI'] = df['AQI']  # Preserve the reported AQI for transparency\n",
    "\n",
    "df['AQI'] = df.apply(lambda row: row['Computed_AQI'] if row['Flagged_AQI_Anomaly'] == 1 else row['AQI'], axis=1)\n",
    "\n",
    "print(f\"Total Rows Modified: {df['Flagged_AQI_Anomaly'].sum()}\")\n",
    "print(\"\\nSample Updated Rows:\")\n",
    "print(df[df['Flagged_AQI_Anomaly'] == 1][['Reported_AQI', 'Computed_AQI', 'AQI']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Updated AQI Bucket Assigned Successfully!\n",
      "           Reported_AQI_Bucket AQI_Bucket\n",
      "Date                                     \n",
      "2015-01-01              Severe     Severe\n",
      "2015-01-02              Severe     Severe\n",
      "2015-01-03            Moderate   Moderate\n",
      "2015-01-04           Very Poor  Very Poor\n",
      "2015-01-05           Very Poor  Very Poor\n",
      "2015-01-06           Very Poor  Very Poor\n",
      "2015-01-07           Very Poor  Very Poor\n",
      "2015-01-08           Very Poor  Very Poor\n",
      "2015-01-09           Very Poor  Very Poor\n",
      "2015-01-10           Very Poor  Very Poor\n"
     ]
    }
   ],
   "source": [
    "df.rename(columns={'AQI_Bucket': 'Reported_AQI_Bucket'}, inplace=True)\n",
    "\n",
    "bins = [0, 50, 100, 200, 300, 400, float('inf')]\n",
    "labels = ['Good', 'Satisfactory', 'Moderate', 'Poor', 'Very Poor', 'Severe']\n",
    "\n",
    "df['AQI_Bucket'] = pd.cut(df['AQI'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "print(\"\\nUpdated AQI Bucket Assigned Successfully!\")\n",
    "print(df[['Reported_AQI_Bucket', 'AQI_Bucket']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Rows Where AQI Bucket Changed:\n",
      "           Reported_AQI_Bucket AQI_Bucket         AQI  Reported_AQI\n",
      "Date                                                               \n",
      "2015-01-31              Severe  Very Poor  343.704734         514.0\n",
      "2015-02-03              Severe  Very Poor  380.141420         660.0\n",
      "2015-02-04                Poor   Moderate  168.081034         294.0\n",
      "2015-02-08           Very Poor   Moderate  186.465823         379.0\n",
      "2015-02-11           Very Poor   Moderate  154.134177         388.0\n",
      "2015-02-13              Severe  Very Poor  307.326627         510.0\n",
      "2015-02-15              Severe       Poor  279.195652         475.0\n",
      "2015-02-16              Severe  Very Poor  317.109467         536.0\n",
      "2015-02-17              Severe       Poor  285.078261         479.0\n",
      "2015-02-18              Severe       Poor  289.095652         592.0\n"
     ]
    }
   ],
   "source": [
    "df_check = df[df['Reported_AQI_Bucket'] != df['AQI_Bucket']][['Reported_AQI_Bucket', 'AQI_Bucket', 'AQI', 'Reported_AQI']]\n",
    "print(\"\\n🔍 Rows Where AQI Bucket Changed:\")\n",
    "print(df_check.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>PM2.5</th>\n",
       "      <th>PM10</th>\n",
       "      <th>NO</th>\n",
       "      <th>NO2</th>\n",
       "      <th>NH3</th>\n",
       "      <th>CO</th>\n",
       "      <th>SO2</th>\n",
       "      <th>O3</th>\n",
       "      <th>AQI</th>\n",
       "      <th>Reported_AQI_Bucket</th>\n",
       "      <th>Outlier</th>\n",
       "      <th>Computed_AQI</th>\n",
       "      <th>AQI_Difference</th>\n",
       "      <th>Flagged_AQI_Anomaly</th>\n",
       "      <th>Reported_AQI</th>\n",
       "      <th>AQI_Bucket</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-01</th>\n",
       "      <td>Delhi</td>\n",
       "      <td>313.22</td>\n",
       "      <td>451.0336</td>\n",
       "      <td>69.16</td>\n",
       "      <td>36.39</td>\n",
       "      <td>33.85</td>\n",
       "      <td>15.20</td>\n",
       "      <td>9.25</td>\n",
       "      <td>41.68</td>\n",
       "      <td>472.0</td>\n",
       "      <td>Severe</td>\n",
       "      <td>-1</td>\n",
       "      <td>425.738072</td>\n",
       "      <td>46.261928</td>\n",
       "      <td>0</td>\n",
       "      <td>472.0</td>\n",
       "      <td>Severe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-02</th>\n",
       "      <td>Delhi</td>\n",
       "      <td>186.18</td>\n",
       "      <td>269.5500</td>\n",
       "      <td>62.09</td>\n",
       "      <td>32.87</td>\n",
       "      <td>31.83</td>\n",
       "      <td>9.54</td>\n",
       "      <td>6.65</td>\n",
       "      <td>29.97</td>\n",
       "      <td>454.0</td>\n",
       "      <td>Severe</td>\n",
       "      <td>1</td>\n",
       "      <td>351.021860</td>\n",
       "      <td>102.978140</td>\n",
       "      <td>0</td>\n",
       "      <td>454.0</td>\n",
       "      <td>Severe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-03</th>\n",
       "      <td>Delhi</td>\n",
       "      <td>87.18</td>\n",
       "      <td>131.9000</td>\n",
       "      <td>25.73</td>\n",
       "      <td>30.31</td>\n",
       "      <td>69.55</td>\n",
       "      <td>10.61</td>\n",
       "      <td>2.65</td>\n",
       "      <td>19.71</td>\n",
       "      <td>143.0</td>\n",
       "      <td>Moderate</td>\n",
       "      <td>1</td>\n",
       "      <td>208.317391</td>\n",
       "      <td>65.317391</td>\n",
       "      <td>0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>Moderate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-04</th>\n",
       "      <td>Delhi</td>\n",
       "      <td>151.84</td>\n",
       "      <td>241.8400</td>\n",
       "      <td>25.01</td>\n",
       "      <td>36.91</td>\n",
       "      <td>130.36</td>\n",
       "      <td>11.54</td>\n",
       "      <td>4.63</td>\n",
       "      <td>25.36</td>\n",
       "      <td>319.0</td>\n",
       "      <td>Very Poor</td>\n",
       "      <td>1</td>\n",
       "      <td>324.667907</td>\n",
       "      <td>5.667907</td>\n",
       "      <td>0</td>\n",
       "      <td>319.0</td>\n",
       "      <td>Very Poor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-05</th>\n",
       "      <td>Delhi</td>\n",
       "      <td>146.60</td>\n",
       "      <td>219.1300</td>\n",
       "      <td>14.01</td>\n",
       "      <td>34.92</td>\n",
       "      <td>122.88</td>\n",
       "      <td>9.20</td>\n",
       "      <td>3.33</td>\n",
       "      <td>23.20</td>\n",
       "      <td>325.0</td>\n",
       "      <td>Very Poor</td>\n",
       "      <td>1</td>\n",
       "      <td>320.646512</td>\n",
       "      <td>4.353488</td>\n",
       "      <td>0</td>\n",
       "      <td>325.0</td>\n",
       "      <td>Very Poor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             City   PM2.5      PM10     NO    NO2     NH3     CO   SO2     O3  \\\n",
       "Date                                                                            \n",
       "2015-01-01  Delhi  313.22  451.0336  69.16  36.39   33.85  15.20  9.25  41.68   \n",
       "2015-01-02  Delhi  186.18  269.5500  62.09  32.87   31.83   9.54  6.65  29.97   \n",
       "2015-01-03  Delhi   87.18  131.9000  25.73  30.31   69.55  10.61  2.65  19.71   \n",
       "2015-01-04  Delhi  151.84  241.8400  25.01  36.91  130.36  11.54  4.63  25.36   \n",
       "2015-01-05  Delhi  146.60  219.1300  14.01  34.92  122.88   9.20  3.33  23.20   \n",
       "\n",
       "              AQI Reported_AQI_Bucket  Outlier  Computed_AQI  AQI_Difference  \\\n",
       "Date                                                                           \n",
       "2015-01-01  472.0              Severe       -1    425.738072       46.261928   \n",
       "2015-01-02  454.0              Severe        1    351.021860      102.978140   \n",
       "2015-01-03  143.0            Moderate        1    208.317391       65.317391   \n",
       "2015-01-04  319.0           Very Poor        1    324.667907        5.667907   \n",
       "2015-01-05  325.0           Very Poor        1    320.646512        4.353488   \n",
       "\n",
       "            Flagged_AQI_Anomaly  Reported_AQI AQI_Bucket  \n",
       "Date                                                      \n",
       "2015-01-01                    0         472.0     Severe  \n",
       "2015-01-02                    0         454.0     Severe  \n",
       "2015-01-03                    0         143.0   Moderate  \n",
       "2015-01-04                    0         319.0  Very Poor  \n",
       "2015-01-05                    0         325.0  Very Poor  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Columns After Dropping Outliers & Anomaly Flags:\n",
      " Index(['City', 'PM2.5', 'PM10', 'NO', 'NO2', 'NH3', 'CO', 'SO2', 'O3', 'AQI',\n",
      "       'Reported_AQI_Bucket', 'Computed_AQI', 'AQI_Difference', 'Reported_AQI',\n",
      "       'AQI_Bucket'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df.drop(columns=['Outlier', 'Flagged_AQI_Anomaly'], inplace=True)\n",
    "\n",
    "print(\"Updated Columns After Dropping Outliers & Anomaly Flags:\\n\", df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Columns After Dropping Outliers & Anomaly Flags:\n",
      " Index(['City', 'PM2.5', 'PM10', 'NO', 'NO2', 'NH3', 'CO', 'SO2', 'O3',\n",
      "       'Computed_AQI', 'Reported_AQI', 'AQI_Bucket'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df.drop(columns=['AQI', 'AQI_Difference', 'Reported_AQI_Bucket'], inplace=True)\n",
    "print(\"Updated Columns After Dropping Outliers & Anomaly Flags:\\n\", df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Year  Month  Day  DayOfWeek\n",
      "Date                                   \n",
      "2015-01-01  2015      1    1          3\n",
      "2015-01-02  2015      1    2          4\n",
      "2015-01-03  2015      1    3          5\n",
      "2015-01-04  2015      1    4          6\n",
      "2015-01-05  2015      1    5          0\n"
     ]
    }
   ],
   "source": [
    "df['Year'] = df.index.year\n",
    "df['Month'] = df.index.month\n",
    "df['Day'] = df.index.day\n",
    "df['DayOfWeek'] = df.index.dayofweek\n",
    "\n",
    "print(df[['Year', 'Month', 'Day', 'DayOfWeek']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             AQI_lag_1  PM2.5_lag_1  PM10_lag_1  PM2.5_rolling_mean_3  \\\n",
      "Date                                                                    \n",
      "2015-01-01         NaN          NaN         NaN            313.220000   \n",
      "2015-01-02  425.738072       313.22    451.0336            249.700000   \n",
      "2015-01-03  351.021860       186.18    269.5500            195.526667   \n",
      "2015-01-04  208.317391        87.18    131.9000            141.733333   \n",
      "2015-01-05  324.667907       151.84    241.8400            128.540000   \n",
      "2015-01-06  320.646512       146.60    219.1300            149.340000   \n",
      "2015-01-07  322.933488       149.58    252.1000            171.350000   \n",
      "2015-01-08  375.342093       217.87    376.5100            199.116667   \n",
      "2015-01-09  384.574419       229.90    360.9500            216.476667   \n",
      "2015-01-10  362.901860       201.66    397.4300            217.526667   \n",
      "\n",
      "            PM10_rolling_mean_3  \n",
      "Date                             \n",
      "2015-01-01           451.033600  \n",
      "2015-01-02           360.291800  \n",
      "2015-01-03           284.161200  \n",
      "2015-01-04           214.430000  \n",
      "2015-01-05           197.623333  \n",
      "2015-01-06           237.690000  \n",
      "2015-01-07           282.580000  \n",
      "2015-01-08           329.853333  \n",
      "2015-01-09           378.296667  \n",
      "2015-01-10           373.373333  \n"
     ]
    }
   ],
   "source": [
    "df['AQI_lag_1'] = df['Computed_AQI'].shift(1)\n",
    "df['PM2.5_lag_1'] = df['PM2.5'].shift(1)\n",
    "df['PM10_lag_1'] = df['PM10'].shift(1)\n",
    "\n",
    "df['PM2.5_rolling_mean_3'] = df['PM2.5'].rolling(window=3, min_periods=1).mean()\n",
    "df['PM10_rolling_mean_3'] = df['PM10'].rolling(window=3, min_periods=1).mean()\n",
    "\n",
    "print(df[['AQI_lag_1', 'PM2.5_lag_1', 'PM10_lag_1', 'PM2.5_rolling_mean_3', 'PM10_rolling_mean_3']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             City  City_Encoded AQI_Bucket  AQI_Bucket_Encoded\n",
      "Date                                                          \n",
      "2015-01-01  Delhi             6     Severe                   5\n",
      "2015-01-02  Delhi             6     Severe                   5\n",
      "2015-01-03  Delhi             6   Moderate                   2\n",
      "2015-01-04  Delhi             6  Very Poor                   4\n",
      "2015-01-05  Delhi             6  Very Poor                   4\n",
      "2015-01-06  Delhi             6  Very Poor                   4\n",
      "2015-01-07  Delhi             6  Very Poor                   4\n",
      "2015-01-08  Delhi             6  Very Poor                   4\n",
      "2015-01-09  Delhi             6  Very Poor                   4\n",
      "2015-01-10  Delhi             6  Very Poor                   4\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "city_encoder = LabelEncoder()\n",
    "df['City_Encoded'] = city_encoder.fit_transform(df['City'])\n",
    "\n",
    "aqi_order = ['Good', 'Satisfactory', 'Moderate', 'Poor', 'Very Poor', 'Severe']\n",
    "aqi_cat_type = CategoricalDtype(categories=aqi_order, ordered=True)\n",
    "\n",
    "df['AQI_Bucket'] = df['AQI_Bucket'].astype(aqi_cat_type)\n",
    "df['AQI_Bucket_Encoded'] = df['AQI_Bucket'].cat.codes\n",
    "\n",
    "print(df[['City', 'City_Encoded', 'AQI_Bucket', 'AQI_Bucket_Encoded']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Season\n",
      "Date              \n",
      "2015-01-01  Winter\n",
      "2015-01-02  Winter\n",
      "2015-01-03  Winter\n",
      "2015-01-04  Winter\n",
      "2015-01-05  Winter\n",
      "2015-01-06  Winter\n",
      "2015-01-07  Winter\n",
      "2015-01-08  Winter\n",
      "2015-01-09  Winter\n",
      "2015-01-10  Winter\n"
     ]
    }
   ],
   "source": [
    "def assign_season(month):\n",
    "    if month in [3, 4, 5, 6]:\n",
    "        return 'Pre-monsoon'\n",
    "    elif month in [7, 8, 9]:\n",
    "        return 'Monsoon'\n",
    "    elif month in [10, 11]:\n",
    "        return 'Post-monsoon'\n",
    "    else:\n",
    "        return 'Winter'\n",
    "\n",
    "df['Season'] = df.index.month.map(assign_season)\n",
    "\n",
    "print(df[['Season']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             PM2.5      PM10     NO    NO2     NH3     CO   SO2     O3  \\\n",
      "Date                                                                     \n",
      "2015-01-01  313.22  451.0336  69.16  36.39   33.85  15.20  9.25  41.68   \n",
      "2015-01-02  186.18  269.5500  62.09  32.87   31.83   9.54  6.65  29.97   \n",
      "2015-01-03   87.18  131.9000  25.73  30.31   69.55  10.61  2.65  19.71   \n",
      "2015-01-04  151.84  241.8400  25.01  36.91  130.36  11.54  4.63  25.36   \n",
      "2015-01-05  146.60  219.1300  14.01  34.92  122.88   9.20  3.33  23.20   \n",
      "\n",
      "            Computed_AQI  Reported_AQI  ... City_Lucknow  City_Mumbai  \\\n",
      "Date                                    ...                             \n",
      "2015-01-01    425.738072         472.0  ...          0.0          0.0   \n",
      "2015-01-02    351.021860         454.0  ...          0.0          0.0   \n",
      "2015-01-03    208.317391         143.0  ...          0.0          0.0   \n",
      "2015-01-04    324.667907         319.0  ...          0.0          0.0   \n",
      "2015-01-05    320.646512         325.0  ...          0.0          0.0   \n",
      "\n",
      "            City_Patna  City_Talcher  City_Thiruvananthapuram  \\\n",
      "Date                                                            \n",
      "2015-01-01         0.0           0.0                      0.0   \n",
      "2015-01-02         0.0           0.0                      0.0   \n",
      "2015-01-03         0.0           0.0                      0.0   \n",
      "2015-01-04         0.0           0.0                      0.0   \n",
      "2015-01-05         0.0           0.0                      0.0   \n",
      "\n",
      "            City_Visakhapatnam  Season_Monsoon  Season_Post-monsoon  \\\n",
      "Date                                                                  \n",
      "2015-01-01                 0.0             0.0                  0.0   \n",
      "2015-01-02                 0.0             0.0                  0.0   \n",
      "2015-01-03                 0.0             0.0                  0.0   \n",
      "2015-01-04                 0.0             0.0                  0.0   \n",
      "2015-01-05                 0.0             0.0                  0.0   \n",
      "\n",
      "            Season_Pre-monsoon  Season_Winter  \n",
      "Date                                           \n",
      "2015-01-01                 0.0            1.0  \n",
      "2015-01-02                 0.0            1.0  \n",
      "2015-01-03                 0.0            1.0  \n",
      "2015-01-04                 0.0            1.0  \n",
      "2015-01-05                 0.0            1.0  \n",
      "\n",
      "[5 rows x 44 columns]\n",
      "\n",
      "Season Encoding Distribution:\n",
      "Season_Monsoon         5425.0\n",
      "Season_Post-monsoon    3780.0\n",
      "Season_Pre-monsoon     8892.0\n",
      "Season_Winter          5947.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe_city = OneHotEncoder(drop='first', sparse_output=False)  \n",
    "ohe_season = OneHotEncoder(drop=None, sparse_output=False)   \n",
    "\n",
    "city_encoded = ohe_city.fit_transform(df[['City']])\n",
    "city_encoded_df = pd.DataFrame(city_encoded, index=df.index, columns=ohe_city.get_feature_names_out(['City']))\n",
    "\n",
    "season_encoded = ohe_season.fit_transform(df[['Season']])\n",
    "season_encoded_df = pd.DataFrame(season_encoded, index=df.index, columns=ohe_season.get_feature_names_out(['Season']))\n",
    "\n",
    "df = pd.concat([df, city_encoded_df, season_encoded_df], axis=1)\n",
    "df.drop(columns=['City', 'Season'], inplace=True) \n",
    "\n",
    "print(df.head())\n",
    "print(\"\\nSeason Encoding Distribution:\")\n",
    "print(season_encoded_df.sum())  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Shape: (19235, 36), Test Set Shape: (4809, 36)\n",
      "Scaled Feature Summary:\n",
      "              PM2.5          PM10            NO           NO2           NH3  \\\n",
      "count  19235.000000  19235.000000  19235.000000  19235.000000  19235.000000   \n",
      "mean       0.341555      0.290771      0.503466      0.249288     -0.000275   \n",
      "std        1.113023      0.988881      1.388975      0.877673      1.478345   \n",
      "min       -0.931544     -0.927794     -0.632854     -0.755667     -1.715990   \n",
      "25%       -0.382644     -0.374110     -0.289424     -0.394161     -0.936840   \n",
      "50%        0.000445      0.002844      0.000000      0.000768      0.000000   \n",
      "75%        0.618260      0.624657      0.701056      0.604879      0.065478   \n",
      "max        5.031009      4.168624      7.159148      3.763826      8.593531   \n",
      "\n",
      "                 CO           SO2            O3     AQI_lag_1   PM2.5_lag_1  \\\n",
      "count  19235.000000  19235.000000  19235.000000  19234.000000  19234.000000   \n",
      "mean       1.383397      0.535640      0.134584      0.335990      0.342647   \n",
      "std        5.680244      1.664567      0.787992      0.859838      1.115431   \n",
      "min       -1.021739     -0.794390     -1.056846     -0.758963     -0.931578   \n",
      "25%       -0.369565     -0.358628     -0.444444     -0.277686     -0.383451   \n",
      "50%        0.000000     -0.001002     -0.003031      0.000482     -0.000376   \n",
      "75%        0.630435      0.641623      0.550535      0.735799      0.621226   \n",
      "max       38.490217      9.322815      2.872604      2.690593      5.030754   \n",
      "\n",
      "       ...  City_Hyderabad   City_Jaipur  City_Jorapokhar  City_Kolkata  \\\n",
      "count  ...    19235.000000  19235.000000     19235.000000  19235.000000   \n",
      "mean   ...        0.078139      0.046634         0.037692      0.032285   \n",
      "std    ...        0.268397      0.210859         0.190455      0.176760   \n",
      "min    ...        0.000000      0.000000         0.000000      0.000000   \n",
      "25%    ...        0.000000      0.000000         0.000000      0.000000   \n",
      "50%    ...        0.000000      0.000000         0.000000      0.000000   \n",
      "75%    ...        0.000000      0.000000         0.000000      0.000000   \n",
      "max    ...        1.000000      1.000000         1.000000      1.000000   \n",
      "\n",
      "       City_Lucknow   City_Mumbai    City_Patna  City_Talcher  \\\n",
      "count  19235.000000  19235.000000  19235.000000  19235.000000   \n",
      "mean       0.080530      0.032909      0.063842      0.032441   \n",
      "std        0.272119      0.178402      0.244478      0.177172   \n",
      "min        0.000000      0.000000      0.000000      0.000000   \n",
      "25%        0.000000      0.000000      0.000000      0.000000   \n",
      "50%        0.000000      0.000000      0.000000      0.000000   \n",
      "75%        0.000000      0.000000      0.000000      0.000000   \n",
      "max        1.000000      1.000000      1.000000      1.000000   \n",
      "\n",
      "       City_Thiruvananthapuram  City_Visakhapatnam  \n",
      "count             19235.000000        19235.000000  \n",
      "mean                  0.044138            0.049649  \n",
      "std                   0.205408            0.217225  \n",
      "min                   0.000000            0.000000  \n",
      "25%                   0.000000            0.000000  \n",
      "50%                   0.000000            0.000000  \n",
      "75%                   0.000000            0.000000  \n",
      "max                   1.000000            1.000000  \n",
      "\n",
      "[8 rows x 36 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "feature_columns = ['PM2.5', 'PM10', 'NO', 'NO2', 'NH3', 'CO', 'SO2', 'O3',\n",
    "                   'AQI_lag_1', 'PM2.5_lag_1', 'PM10_lag_1',\n",
    "                   'PM2.5_rolling_mean_3', 'PM10_rolling_mean_3',\n",
    "                   'Season_Monsoon', 'Season_Post-monsoon', 'Season_Pre-monsoon', 'Season_Winter']\n",
    "\n",
    "feature_columns += [col for col in df.columns if col.startswith('City_')]\n",
    "\n",
    "X = df[feature_columns] \n",
    "y = df['Computed_AQI']  \n",
    "\n",
    "scaler = RobustScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training Set Shape: {X_train.shape}, Test Set Shape: {X_test.shape}\")\n",
    "print(f\"Scaled Feature Summary:\\n{pd.DataFrame(X_train, columns=feature_columns).describe()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking for Null Values in the Dataset:\n",
      "PM2.5                      0\n",
      "PM10                       0\n",
      "NO                         0\n",
      "NO2                        0\n",
      "NH3                        0\n",
      "CO                         0\n",
      "SO2                        0\n",
      "O3                         0\n",
      "Computed_AQI               0\n",
      "Reported_AQI               0\n",
      "AQI_Bucket                 0\n",
      "Year                       0\n",
      "Month                      0\n",
      "Day                        0\n",
      "DayOfWeek                  0\n",
      "AQI_lag_1                  1\n",
      "PM2.5_lag_1                1\n",
      "PM10_lag_1                 1\n",
      "PM2.5_rolling_mean_3       0\n",
      "PM10_rolling_mean_3        0\n",
      "City_Encoded               0\n",
      "AQI_Bucket_Encoded         0\n",
      "City_Amaravati             0\n",
      "City_Amritsar              0\n",
      "City_Bengaluru             0\n",
      "City_Brajrajnagar          0\n",
      "City_Chennai               0\n",
      "City_Delhi                 0\n",
      "City_Gurugram              0\n",
      "City_Guwahati              0\n",
      "City_Hyderabad             0\n",
      "City_Jaipur                0\n",
      "City_Jorapokhar            0\n",
      "City_Kolkata               0\n",
      "City_Lucknow               0\n",
      "City_Mumbai                0\n",
      "City_Patna                 0\n",
      "City_Talcher               0\n",
      "City_Thiruvananthapuram    0\n",
      "City_Visakhapatnam         0\n",
      "Season_Monsoon             0\n",
      "Season_Post-monsoon        0\n",
      "Season_Pre-monsoon         0\n",
      "Season_Winter              0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nChecking for Null Values in the Dataset:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking for Null Values in the Dataset:\n",
      "PM2.5                      0\n",
      "PM10                       0\n",
      "NO                         0\n",
      "NO2                        0\n",
      "NH3                        0\n",
      "CO                         0\n",
      "SO2                        0\n",
      "O3                         0\n",
      "Computed_AQI               0\n",
      "Reported_AQI               0\n",
      "AQI_Bucket                 0\n",
      "Year                       0\n",
      "Month                      0\n",
      "Day                        0\n",
      "DayOfWeek                  0\n",
      "AQI_lag_1                  0\n",
      "PM2.5_lag_1                0\n",
      "PM10_lag_1                 0\n",
      "PM2.5_rolling_mean_3       0\n",
      "PM10_rolling_mean_3        0\n",
      "City_Encoded               0\n",
      "AQI_Bucket_Encoded         0\n",
      "City_Amaravati             0\n",
      "City_Amritsar              0\n",
      "City_Bengaluru             0\n",
      "City_Brajrajnagar          0\n",
      "City_Chennai               0\n",
      "City_Delhi                 0\n",
      "City_Gurugram              0\n",
      "City_Guwahati              0\n",
      "City_Hyderabad             0\n",
      "City_Jaipur                0\n",
      "City_Jorapokhar            0\n",
      "City_Kolkata               0\n",
      "City_Lucknow               0\n",
      "City_Mumbai                0\n",
      "City_Patna                 0\n",
      "City_Talcher               0\n",
      "City_Thiruvananthapuram    0\n",
      "City_Visakhapatnam         0\n",
      "Season_Monsoon             0\n",
      "Season_Post-monsoon        0\n",
      "Season_Pre-monsoon         0\n",
      "Season_Winter              0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df.dropna(subset=['AQI_lag_1', 'PM2.5_lag_1', 'PM10_lag_1'], inplace=True)\n",
    "print(\"\\nChecking for Null Values in the Dataset:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nLinearRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# 1️⃣ Initialize & Train Linear Regression Model\u001b[39;00m\n\u001b[1;32m      6\u001b[0m lr_model \u001b[38;5;241m=\u001b[39m LinearRegression()\n\u001b[0;32m----> 7\u001b[0m \u001b[43mlr_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 2️⃣ Make Predictions\u001b[39;00m\n\u001b[1;32m     10\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m lr_model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_new_env/lib/python3.9/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_new_env/lib/python3.9/site-packages/sklearn/linear_model/_base.py:601\u001b[0m, in \u001b[0;36mLinearRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    597\u001b[0m n_jobs_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs\n\u001b[1;32m    599\u001b[0m accept_sparse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositive \u001b[38;5;28;01melse\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoo\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 601\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_numeric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    611\u001b[0m has_sw \u001b[38;5;241m=\u001b[39m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    612\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_sw:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_new_env/lib/python3.9/site-packages/sklearn/utils/validation.py:2961\u001b[0m, in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2959\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[1;32m   2960\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2961\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2962\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m   2964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_new_env/lib/python3.9/site-packages/sklearn/utils/validation.py:1370\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1365\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1366\u001b[0m     )\n\u001b[1;32m   1368\u001b[0m ensure_all_finite \u001b[38;5;241m=\u001b[39m _deprecate_force_all_finite(force_all_finite, ensure_all_finite)\n\u001b[0;32m-> 1370\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1371\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1372\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1374\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1375\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1376\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1377\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_writeable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1378\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1380\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1382\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1384\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1385\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1387\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[1;32m   1389\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_new_env/lib/python3.9/site-packages/sklearn/utils/validation.py:1107\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1102\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1103\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m   1104\u001b[0m     )\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[0;32m-> 1107\u001b[0m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1112\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[1;32m   1115\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[1;32m   1116\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_new_env/lib/python3.9/site-packages/sklearn/utils/validation.py:120\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_new_env/lib/python3.9/site-packages/sklearn/utils/validation.py:169\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    155\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    168\u001b[0m     )\n\u001b[0;32m--> 169\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input X contains NaN.\nLinearRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lr_model.predict(X_test)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\" Linear Regression Model - RMSE: {rmse:.2f}\")\n",
    "print(f\" Linear Regression Model - R² Score: {r2:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking for Null Values in X_train:\n",
      "0     0\n",
      "1     0\n",
      "2     0\n",
      "3     0\n",
      "4     0\n",
      "5     0\n",
      "6     0\n",
      "7     0\n",
      "8     1\n",
      "9     1\n",
      "10    1\n",
      "11    0\n",
      "12    0\n",
      "13    0\n",
      "14    0\n",
      "15    0\n",
      "16    0\n",
      "17    0\n",
      "18    0\n",
      "19    0\n",
      "20    0\n",
      "21    0\n",
      "22    0\n",
      "23    0\n",
      "24    0\n",
      "25    0\n",
      "26    0\n",
      "27    0\n",
      "28    0\n",
      "29    0\n",
      "30    0\n",
      "31    0\n",
      "32    0\n",
      "33    0\n",
      "34    0\n",
      "35    0\n",
      "dtype: int64\n",
      "\n",
      "Checking for Null Values in y_train:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nChecking for Null Values in X_train:\")\n",
    "print(pd.DataFrame(X_train).isnull().sum())\n",
    "\n",
    "print(\"\\nChecking for Null Values in y_train:\")\n",
    "print(pd.Series(y_train).isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking for Null Values in X_train:\n",
      "PM2.5                      0\n",
      "PM10                       0\n",
      "NO                         0\n",
      "NO2                        0\n",
      "NH3                        0\n",
      "CO                         0\n",
      "SO2                        0\n",
      "O3                         0\n",
      "AQI_lag_1                  0\n",
      "PM2.5_lag_1                0\n",
      "PM10_lag_1                 0\n",
      "PM2.5_rolling_mean_3       0\n",
      "PM10_rolling_mean_3        0\n",
      "Season_Monsoon             0\n",
      "Season_Post-monsoon        0\n",
      "Season_Pre-monsoon         0\n",
      "Season_Winter              0\n",
      "City_Encoded               0\n",
      "City_Amaravati             0\n",
      "City_Amritsar              0\n",
      "City_Bengaluru             0\n",
      "City_Brajrajnagar          0\n",
      "City_Chennai               0\n",
      "City_Delhi                 0\n",
      "City_Gurugram              0\n",
      "City_Guwahati              0\n",
      "City_Hyderabad             0\n",
      "City_Jaipur                0\n",
      "City_Jorapokhar            0\n",
      "City_Kolkata               0\n",
      "City_Lucknow               0\n",
      "City_Mumbai                0\n",
      "City_Patna                 0\n",
      "City_Talcher               0\n",
      "City_Thiruvananthapuram    0\n",
      "City_Visakhapatnam         0\n",
      "dtype: int64\n",
      "\n",
      "Checking for Null Values in y_train:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "X_train_df = pd.DataFrame(X_train, columns=feature_columns)\n",
    "y_train_series = pd.Series(y_train)\n",
    "\n",
    "X_train_clean = X_train_df.dropna()\n",
    "y_train_clean = y_train_series.iloc[X_train_clean.index]  # Keep y_train aligned with the cleaned X_train\n",
    "\n",
    "print(\"\\nChecking for Null Values in X_train:\")\n",
    "print(X_train_clean.isnull().sum())\n",
    "\n",
    "print(\"\\nChecking for Null Values in y_train:\")\n",
    "print(y_train_clean.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Linear Regression Model - RMSE: 31.05\n",
      "🔹 Linear Regression Model - R² Score: 0.92\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train_clean, y_train_clean)\n",
    "\n",
    "X_test_df = pd.DataFrame(X_test, columns=feature_columns)\n",
    "\n",
    "y_pred = lr_model.predict(X_test_df)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\" Linear Regression Model - RMSE: {rmse:.2f}\")\n",
    "print(f\" Linear Regression Model - R² Score: {r2:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Random Forest Model - RMSE: 5.28\n",
      "🔹 Random Forest Model - R² Score: 1.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train_clean, y_train_clean)\n",
    "\n",
    "y_pred_rf = rf_model.predict(X_test_df)\n",
    "\n",
    "rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "\n",
    "print(f\" Random Forest Model - RMSE: {rmse_rf:.2f}\")\n",
    "print(f\" Random Forest Model - R² Score: {r2_rf:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Decision Tree Model - RMSE: 6.98\n",
      "🔹 Decision Tree Model - R² Score: 1.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "dt_model = DecisionTreeRegressor(random_state=42)\n",
    "dt_model.fit(X_train_clean, y_train_clean)\n",
    "\n",
    "y_pred_dt = dt_model.predict(X_test_df)\n",
    "\n",
    "rmse_dt = np.sqrt(mean_squared_error(y_test, y_pred_dt))\n",
    "r2_dt = r2_score(y_test, y_pred_dt)\n",
    "\n",
    "print(f\" Decision Tree Model - RMSE: {rmse_dt:.2f}\")\n",
    "print(f\" Decision Tree Model - R² Score: {r2_dt:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 KNN Model - RMSE: 25.30\n",
      "🔹 KNN Model - R² Score: 0.94\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "knn_model = KNeighborsRegressor(n_neighbors=10)  # You can experiment with different values for n_neighbors\n",
    "knn_model.fit(X_train_clean, y_train_clean)\n",
    "\n",
    "y_pred_knn = knn_model.predict(X_test_df)\n",
    "\n",
    "rmse_knn = np.sqrt(mean_squared_error(y_test, y_pred_knn))\n",
    "r2_knn = r2_score(y_test, y_pred_knn)\n",
    "\n",
    "print(f\" KNN Model - RMSE: {rmse_knn:.2f}\")\n",
    "print(f\" KNN Model - R² Score: {r2_knn:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 SVR Model - RMSE: 24.22\n",
      "🔹 SVR Model - R² Score: 0.95\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "svr_model = SVR(kernel='rbf')  # You can experiment with different kernels like 'linear', 'poly', etc.\n",
    "svr_model.fit(X_train_clean, y_train_clean)\n",
    "\n",
    "y_pred_svr = svr_model.predict(X_test_df)\n",
    "\n",
    "rmse_svr = np.sqrt(mean_squared_error(y_test, y_pred_svr))\n",
    "r2_svr = r2_score(y_test, y_pred_svr)\n",
    "\n",
    "print(f\" SVR Model - RMSE: {rmse_svr:.2f}\")\n",
    "print(f\" SVR Model - R² Score: {r2_svr:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Ridge Regression Model - RMSE: 31.05\n",
      "🔹 Ridge Regression Model - R² Score: 0.92\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge_model = Ridge(alpha=1)  # You can experiment with different values for alpha\n",
    "ridge_model.fit(X_train_clean, y_train_clean)\n",
    "\n",
    "y_pred_ridge = ridge_model.predict(X_test_df)\n",
    "\n",
    "rmse_ridge = np.sqrt(mean_squared_error(y_test, y_pred_ridge))\n",
    "r2_ridge = r2_score(y_test, y_pred_ridge)\n",
    "\n",
    "print(f\" Ridge Regression Model - RMSE: {rmse_ridge:.2f}\")\n",
    "print(f\" Ridge Regression Model - R² Score: {r2_ridge:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Lasso Regression Model - RMSE: 31.57\n",
      "🔹 Lasso Regression Model - R² Score: 0.91\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso_model = Lasso(alpha=1)  # You can experiment with different values for alpha\n",
    "lasso_model.fit(X_train_clean, y_train_clean)\n",
    "\n",
    "y_pred_lasso = lasso_model.predict(X_test_df)\n",
    "\n",
    "rmse_lasso = np.sqrt(mean_squared_error(y_test, y_pred_lasso))\n",
    "r2_lasso = r2_score(y_test, y_pred_lasso)\n",
    "\n",
    "print(f\" Lasso Regression Model - RMSE: {rmse_lasso:.2f}\")\n",
    "print(f\" Lasso Regression Model - R² Score: {r2_lasso:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 XGBoost Model - RMSE: 8.13\n",
      "🔹 XGBoost Model - R² Score: 0.99\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xgboost_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)\n",
    "xgboost_model.fit(X_train_clean, y_train_clean)\n",
    "\n",
    "y_pred_xgboost = xgboost_model.predict(X_test_df)\n",
    "\n",
    "rmse_xgboost = np.sqrt(mean_squared_error(y_test, y_pred_xgboost))\n",
    "r2_xgboost = r2_score(y_test, y_pred_xgboost)\n",
    "\n",
    "print(f\" XGBoost Model - RMSE: {rmse_xgboost:.2f}\")\n",
    "print(f\" XGBoost Model - R² Score: {r2_xgboost:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 XGBoost Model - RMSE: 8.08\n",
      "🔹 XGBoost Model - R² Score: 0.99\n"
     ]
    }
   ],
   "source": [
    "xgboost_model = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    min_child_weight=1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    gamma=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "xgboost_model.fit(X_train_clean, y_train_clean)\n",
    "y_pred_xgboost = xgboost_model.predict(X_test_df)\n",
    "\n",
    "rmse_xgboost = np.sqrt(mean_squared_error(y_test, y_pred_xgboost))\n",
    "r2_xgboost = r2_score(y_test, y_pred_xgboost)\n",
    "\n",
    "print(f\" XGBoost Model - RMSE: {rmse_xgboost:.2f}\")\n",
    "print(f\" XGBoost Model - R² Score: {r2_xgboost:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 XGBoost Model - RMSE: 8.04\n",
      "🔹 XGBoost Model - R² Score: 0.99\n"
     ]
    }
   ],
   "source": [
    "xgboost_model = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    n_estimators=1500,        \n",
    "    learning_rate=0.01,       \n",
    "    max_depth=10,             \n",
    "    min_child_weight=10,      \n",
    "    subsample=0.8,           \n",
    "    colsample_bytree=0.8,     \n",
    "    gamma=0.3,               \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgboost_model.fit(X_train_clean, y_train_clean)\n",
    "\n",
    "y_pred_xgboost = xgboost_model.predict(X_test_df)\n",
    "\n",
    "rmse_xgboost = np.sqrt(mean_squared_error(y_test, y_pred_xgboost))\n",
    "r2_xgboost = r2_score(y_test, y_pred_xgboost)\n",
    "\n",
    "print(f\" XGBoost Model - RMSE: {rmse_xgboost:.2f}\")\n",
    "print(f\" XGBoost Model - R² Score: {r2_xgboost:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/my_new_env/lib/python3.9/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 809us/step - loss: 21705.1836 - mae: 116.5432 - val_loss: 1197.0515 - val_mae: 27.3272\n",
      "Epoch 2/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 679us/step - loss: 1471.1649 - mae: 29.5426 - val_loss: 365.0338 - val_mae: 12.6968\n",
      "Epoch 3/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 673us/step - loss: 890.2397 - mae: 21.8200 - val_loss: 318.1917 - val_mae: 11.5522\n",
      "Epoch 4/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 674us/step - loss: 779.1415 - mae: 20.4199 - val_loss: 306.1909 - val_mae: 11.6129\n",
      "Epoch 5/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 752us/step - loss: 692.0445 - mae: 19.4651 - val_loss: 296.0805 - val_mae: 11.6649\n",
      "Epoch 6/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 677us/step - loss: 666.5343 - mae: 19.0797 - val_loss: 271.0205 - val_mae: 10.5261\n",
      "Epoch 7/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 682us/step - loss: 614.1395 - mae: 18.3794 - val_loss: 247.6304 - val_mae: 10.0281\n",
      "Epoch 8/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 669us/step - loss: 574.9974 - mae: 17.7066 - val_loss: 262.2355 - val_mae: 10.8172\n",
      "Epoch 9/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 691us/step - loss: 572.7595 - mae: 17.5925 - val_loss: 243.0087 - val_mae: 10.1296\n",
      "Epoch 10/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 705us/step - loss: 552.3681 - mae: 17.2330 - val_loss: 216.5977 - val_mae: 9.1087\n",
      "Epoch 11/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 676us/step - loss: 541.7339 - mae: 16.9387 - val_loss: 225.9543 - val_mae: 9.3351\n",
      "Epoch 12/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 502.8107 - mae: 16.5096 - val_loss: 203.8648 - val_mae: 8.7357\n",
      "Epoch 13/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 760us/step - loss: 498.4395 - mae: 16.1971 - val_loss: 203.4222 - val_mae: 8.9112\n",
      "Epoch 14/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 672us/step - loss: 499.2094 - mae: 16.4655 - val_loss: 214.3548 - val_mae: 9.0231\n",
      "Epoch 15/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 479.8585 - mae: 16.0721 - val_loss: 180.9363 - val_mae: 8.2211\n",
      "Epoch 16/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 674us/step - loss: 480.2573 - mae: 15.8694 - val_loss: 197.0293 - val_mae: 8.6500\n",
      "Epoch 17/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 674us/step - loss: 476.2711 - mae: 15.6351 - val_loss: 182.6738 - val_mae: 8.1987\n",
      "Epoch 18/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 664us/step - loss: 462.8264 - mae: 15.8011 - val_loss: 190.5846 - val_mae: 8.8782\n",
      "Epoch 19/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 679us/step - loss: 454.0576 - mae: 15.4701 - val_loss: 182.0939 - val_mae: 8.2736\n",
      "Epoch 20/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 667us/step - loss: 435.8831 - mae: 15.0385 - val_loss: 165.6417 - val_mae: 7.3883\n",
      "Epoch 21/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 668us/step - loss: 400.8625 - mae: 14.3988 - val_loss: 165.1322 - val_mae: 7.4727\n",
      "Epoch 22/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 694us/step - loss: 401.8608 - mae: 14.6319 - val_loss: 169.1513 - val_mae: 7.7498\n",
      "Epoch 23/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 661us/step - loss: 405.3910 - mae: 14.6604 - val_loss: 183.2878 - val_mae: 8.1557\n",
      "Epoch 24/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 401.5248 - mae: 14.4802 - val_loss: 173.9895 - val_mae: 7.7587\n",
      "Epoch 25/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 690us/step - loss: 393.3773 - mae: 14.3117 - val_loss: 169.1484 - val_mae: 7.6383\n",
      "Epoch 26/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 677us/step - loss: 386.5708 - mae: 14.2668 - val_loss: 179.5331 - val_mae: 8.5155\n",
      "Epoch 27/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 666us/step - loss: 391.2765 - mae: 14.2983 - val_loss: 185.2737 - val_mae: 7.3350\n",
      "Epoch 28/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 673us/step - loss: 390.6518 - mae: 14.0177 - val_loss: 159.0164 - val_mae: 7.3350\n",
      "Epoch 29/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 387.3182 - mae: 13.8790 - val_loss: 172.8275 - val_mae: 7.8299\n",
      "Epoch 30/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 386.5254 - mae: 14.0590 - val_loss: 151.9655 - val_mae: 6.9948\n",
      "Epoch 31/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 662us/step - loss: 382.0478 - mae: 13.8550 - val_loss: 165.6996 - val_mae: 7.2372\n",
      "Epoch 32/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 387.8434 - mae: 13.8398 - val_loss: 167.4209 - val_mae: 7.2405\n",
      "Epoch 33/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 725us/step - loss: 362.2855 - mae: 13.5807 - val_loss: 155.6971 - val_mae: 7.3083\n",
      "Epoch 34/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 364.5276 - mae: 13.5734 - val_loss: 158.0034 - val_mae: 7.4039\n",
      "Epoch 35/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 647us/step - loss: 360.2610 - mae: 13.4715 - val_loss: 153.1753 - val_mae: 6.6657\n",
      "Epoch 36/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 685us/step - loss: 335.1738 - mae: 13.0971 - val_loss: 156.0126 - val_mae: 7.2258\n",
      "Epoch 37/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 688us/step - loss: 378.3197 - mae: 13.8053 - val_loss: 159.7250 - val_mae: 7.0455\n",
      "Epoch 38/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 670us/step - loss: 338.0865 - mae: 13.0887 - val_loss: 161.8510 - val_mae: 7.1260\n",
      "Epoch 39/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 676us/step - loss: 362.0040 - mae: 13.3712 - val_loss: 144.6986 - val_mae: 6.8261\n",
      "Epoch 40/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 341.2963 - mae: 12.8627 - val_loss: 152.3337 - val_mae: 7.0165\n",
      "Epoch 41/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 647us/step - loss: 320.0030 - mae: 12.9013 - val_loss: 152.9499 - val_mae: 6.7749\n",
      "Epoch 42/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 675us/step - loss: 315.1497 - mae: 12.7902 - val_loss: 157.5747 - val_mae: 7.3293\n",
      "Epoch 43/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 649us/step - loss: 340.1101 - mae: 12.9292 - val_loss: 156.6596 - val_mae: 6.9645\n",
      "Epoch 44/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 314.0300 - mae: 12.5991 - val_loss: 153.1128 - val_mae: 7.4410\n",
      "Epoch 45/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 646us/step - loss: 324.6805 - mae: 12.8443 - val_loss: 147.2652 - val_mae: 6.6964\n",
      "Epoch 46/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 629us/step - loss: 310.8594 - mae: 12.4613 - val_loss: 154.8350 - val_mae: 7.6856\n",
      "Epoch 47/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 629us/step - loss: 299.6203 - mae: 12.2137 - val_loss: 163.1212 - val_mae: 7.3083\n",
      "Epoch 48/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 629us/step - loss: 309.7196 - mae: 12.3582 - val_loss: 155.8644 - val_mae: 7.0000\n",
      "Epoch 49/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 303.1095 - mae: 12.2240 - val_loss: 142.8977 - val_mae: 6.5442\n",
      "Epoch 50/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 771us/step - loss: 305.6083 - mae: 12.3474 - val_loss: 152.3479 - val_mae: 6.8791\n",
      "Epoch 51/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 666us/step - loss: 320.2811 - mae: 12.4523 - val_loss: 159.5348 - val_mae: 7.2955\n",
      "Epoch 52/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 642us/step - loss: 291.6526 - mae: 11.9914 - val_loss: 164.6785 - val_mae: 7.7568\n",
      "Epoch 53/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 692us/step - loss: 297.2729 - mae: 12.2083 - val_loss: 161.4763 - val_mae: 7.7039\n",
      "Epoch 54/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 767us/step - loss: 283.2570 - mae: 11.8470 - val_loss: 146.1855 - val_mae: 6.4458\n",
      "Epoch 55/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 676us/step - loss: 311.6267 - mae: 12.2097 - val_loss: 155.6786 - val_mae: 7.0130\n",
      "Epoch 56/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 676us/step - loss: 299.6101 - mae: 12.1401 - val_loss: 147.3347 - val_mae: 6.4318\n",
      "Epoch 57/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 671us/step - loss: 284.1177 - mae: 11.7992 - val_loss: 158.6991 - val_mae: 6.7665\n",
      "Epoch 58/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 644us/step - loss: 287.0333 - mae: 11.8452 - val_loss: 145.3002 - val_mae: 6.8508\n",
      "Epoch 59/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 673us/step - loss: 289.2393 - mae: 11.9039 - val_loss: 158.7543 - val_mae: 6.6902\n",
      "Epoch 60/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 282.3478 - mae: 11.6834 - val_loss: 169.5130 - val_mae: 7.3060\n",
      "Epoch 61/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 748us/step - loss: 277.2794 - mae: 11.7571 - val_loss: 156.0889 - val_mae: 6.9236\n",
      "Epoch 62/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 684us/step - loss: 279.7935 - mae: 11.7365 - val_loss: 151.0738 - val_mae: 7.0269\n",
      "Epoch 63/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 679us/step - loss: 286.2352 - mae: 11.8562 - val_loss: 139.6669 - val_mae: 5.9524\n",
      "Epoch 64/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 704us/step - loss: 278.8206 - mae: 11.5933 - val_loss: 145.7579 - val_mae: 6.4999\n",
      "Epoch 65/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 260.1779 - mae: 11.3160 - val_loss: 162.7773 - val_mae: 7.9561\n",
      "Epoch 66/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 673us/step - loss: 269.4555 - mae: 11.4641 - val_loss: 155.7854 - val_mae: 7.0514\n",
      "Epoch 67/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 669us/step - loss: 266.4196 - mae: 11.4014 - val_loss: 148.9550 - val_mae: 6.9934\n",
      "Epoch 68/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 644us/step - loss: 265.1863 - mae: 11.2737 - val_loss: 149.0350 - val_mae: 6.1100\n",
      "Epoch 69/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 735us/step - loss: 252.6973 - mae: 11.1563 - val_loss: 142.0854 - val_mae: 6.0803\n",
      "Epoch 70/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step - loss: 272.4841 - mae: 11.4381 - val_loss: 150.9241 - val_mae: 6.8848\n",
      "Epoch 71/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 642us/step - loss: 263.9006 - mae: 11.3161 - val_loss: 148.4248 - val_mae: 6.7735\n",
      "Epoch 72/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 646us/step - loss: 236.2310 - mae: 10.9308 - val_loss: 145.7086 - val_mae: 6.2840\n",
      "Epoch 73/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 657us/step - loss: 264.5966 - mae: 11.1274 - val_loss: 191.9301 - val_mae: 8.8288\n",
      "Epoch 74/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 642us/step - loss: 269.4928 - mae: 11.4679 - val_loss: 152.9713 - val_mae: 7.0749\n",
      "Epoch 75/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 648us/step - loss: 260.2460 - mae: 11.1953 - val_loss: 168.4318 - val_mae: 7.4914\n",
      "Epoch 76/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 633us/step - loss: 247.9396 - mae: 10.9535 - val_loss: 142.7839 - val_mae: 6.6135\n",
      "Epoch 77/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 653us/step - loss: 243.8066 - mae: 10.9316 - val_loss: 142.3295 - val_mae: 6.1889\n",
      "Epoch 78/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 635us/step - loss: 261.9420 - mae: 11.1302 - val_loss: 157.3096 - val_mae: 7.2159\n",
      "Epoch 79/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 642us/step - loss: 234.3121 - mae: 10.6394 - val_loss: 151.7875 - val_mae: 6.6873\n",
      "Epoch 80/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 673us/step - loss: 235.6007 - mae: 10.7440 - val_loss: 172.1085 - val_mae: 8.2782\n",
      "Epoch 81/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 634us/step - loss: 251.0060 - mae: 10.9306 - val_loss: 155.5831 - val_mae: 7.1882\n",
      "Epoch 82/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 639us/step - loss: 242.7527 - mae: 10.7499 - val_loss: 151.1538 - val_mae: 6.5221\n",
      "Epoch 83/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 753us/step - loss: 251.7517 - mae: 10.9609 - val_loss: 162.6411 - val_mae: 7.3714\n",
      "Epoch 84/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 658us/step - loss: 250.0661 - mae: 10.8376 - val_loss: 153.5560 - val_mae: 6.6889\n",
      "Epoch 85/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 636us/step - loss: 258.2314 - mae: 10.7959 - val_loss: 156.5985 - val_mae: 6.4616\n",
      "Epoch 86/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 628us/step - loss: 226.5871 - mae: 10.4792 - val_loss: 177.7994 - val_mae: 7.5307\n",
      "Epoch 87/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 669us/step - loss: 229.3699 - mae: 10.4763 - val_loss: 165.6736 - val_mae: 7.8526\n",
      "Epoch 88/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 663us/step - loss: 240.9905 - mae: 10.6498 - val_loss: 164.9559 - val_mae: 7.5376\n",
      "Epoch 89/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 654us/step - loss: 240.9450 - mae: 10.6055 - val_loss: 154.3443 - val_mae: 6.6611\n",
      "Epoch 90/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 646us/step - loss: 237.7384 - mae: 10.5843 - val_loss: 151.0284 - val_mae: 6.7261\n",
      "Epoch 91/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 632us/step - loss: 240.1992 - mae: 10.6355 - val_loss: 147.1991 - val_mae: 6.5759\n",
      "Epoch 92/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 634us/step - loss: 232.9381 - mae: 10.4412 - val_loss: 154.6748 - val_mae: 6.9457\n",
      "Epoch 93/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 640us/step - loss: 232.2333 - mae: 10.4246 - val_loss: 147.6613 - val_mae: 6.3592\n",
      "Epoch 94/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 726us/step - loss: 220.2463 - mae: 10.1593 - val_loss: 160.7103 - val_mae: 6.9211\n",
      "Epoch 95/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 660us/step - loss: 219.9485 - mae: 10.4514 - val_loss: 170.1865 - val_mae: 7.7069\n",
      "Epoch 96/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 666us/step - loss: 221.6832 - mae: 10.3008 - val_loss: 167.0792 - val_mae: 7.7698\n",
      "Epoch 97/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 655us/step - loss: 213.3525 - mae: 10.1287 - val_loss: 165.0596 - val_mae: 6.7575\n",
      "Epoch 98/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 633us/step - loss: 213.0430 - mae: 10.1825 - val_loss: 179.6351 - val_mae: 8.1845\n",
      "Epoch 99/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 667us/step - loss: 229.1076 - mae: 10.2568 - val_loss: 157.1540 - val_mae: 6.7022\n",
      "Epoch 100/100\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 674us/step - loss: 218.2976 - mae: 10.2468 - val_loss: 153.5078 - val_mae: 6.5811\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 441us/step\n",
      "🔹 Neural Network Model - RMSE: 12.39\n",
      "🔹 Neural Network Model - R² Score: 0.99\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_clean)\n",
    "X_test_scaled = scaler.transform(X_test_df)\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),  # First Hidden Layer\n",
    "    BatchNormalization(),  \n",
    "    Dropout(0.2),  \n",
    "\n",
    "    Dense(64, activation='relu'),  \n",
    "    BatchNormalization(),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    Dense(32, activation='relu'),  \n",
    "    Dense(1, activation='linear')  \n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),  \n",
    "    loss='mse',  \n",
    "    metrics=['mae']  \n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train_clean,\n",
    "    validation_data=(X_test_scaled, y_test),\n",
    "    epochs=100,  \n",
    "    batch_size=32,  \n",
    "    verbose=1  \n",
    ")\n",
    "\n",
    "y_pred_nn = model.predict(X_test_scaled)\n",
    "\n",
    "rmse_nn = np.sqrt(mean_squared_error(y_test, y_pred_nn))\n",
    "r2_nn = r2_score(y_test, y_pred_nn)\n",
    "\n",
    "print(f\" Neural Network Model - RMSE: {rmse_nn:.2f}\")\n",
    "print(f\" Neural Network Model - R² Score: {r2_nn:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/my_new_env/lib/python3.9/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 33289.3047 - mae: 151.0238 - val_loss: 20117.0879 - val_mae: 124.4271\n",
      "Epoch 2/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 16410.5234 - mae: 112.1916 - val_loss: 5909.6826 - val_mae: 68.4942\n",
      "Epoch 3/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 5716.4912 - mae: 65.9402 - val_loss: 3312.1541 - val_mae: 47.2449\n",
      "Epoch 4/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 3614.9458 - mae: 50.0293 - val_loss: 2021.5516 - val_mae: 34.6551\n",
      "Epoch 5/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 2625.9587 - mae: 41.2067 - val_loss: 1390.1738 - val_mae: 27.4923\n",
      "Epoch 6/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 2017.4149 - mae: 35.5008 - val_loss: 1064.8949 - val_mae: 23.0533\n",
      "Epoch 7/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 1693.9498 - mae: 32.3488 - val_loss: 770.0713 - val_mae: 19.3663\n",
      "Epoch 8/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 1336.0741 - mae: 28.4329 - val_loss: 599.5192 - val_mae: 17.1497\n",
      "Epoch 9/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1202.9130 - mae: 26.6893 - val_loss: 515.8157 - val_mae: 15.7639\n",
      "Epoch 10/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 1118.8966 - mae: 25.4160 - val_loss: 430.1814 - val_mae: 14.3755\n",
      "Epoch 11/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1022.3965 - mae: 23.9498 - val_loss: 366.5843 - val_mae: 13.1158\n",
      "Epoch 12/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 910.3708 - mae: 22.5799 - val_loss: 321.1176 - val_mae: 11.8967\n",
      "Epoch 13/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 873.2174 - mae: 22.0070 - val_loss: 296.3591 - val_mae: 11.0845\n",
      "Epoch 14/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 848.9026 - mae: 21.5351 - val_loss: 319.3367 - val_mae: 11.9193\n",
      "Epoch 15/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 757.6323 - mae: 20.4711 - val_loss: 279.5917 - val_mae: 10.8265\n",
      "Epoch 16/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 755.7036 - mae: 20.4229 - val_loss: 264.0986 - val_mae: 10.4047\n",
      "Epoch 17/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 709.2253 - mae: 19.5571 - val_loss: 242.0472 - val_mae: 9.7344\n",
      "Epoch 18/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 710.8849 - mae: 19.8069 - val_loss: 242.2021 - val_mae: 9.6496\n",
      "Epoch 19/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 667.4310 - mae: 19.0959 - val_loss: 228.3503 - val_mae: 9.4246\n",
      "Epoch 20/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 668.8696 - mae: 18.8101 - val_loss: 229.4039 - val_mae: 9.5296\n",
      "Epoch 21/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 607.3077 - mae: 18.1484 - val_loss: 223.1509 - val_mae: 9.4412\n",
      "Epoch 22/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 606.5134 - mae: 18.1758 - val_loss: 226.3478 - val_mae: 9.0828\n",
      "Epoch 23/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 605.5414 - mae: 17.8560 - val_loss: 211.9506 - val_mae: 8.8655\n",
      "Epoch 24/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 575.3595 - mae: 17.6092 - val_loss: 202.7431 - val_mae: 8.5363\n",
      "Epoch 25/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 551.1542 - mae: 17.0175 - val_loss: 209.0384 - val_mae: 9.0939\n",
      "Epoch 26/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 543.5341 - mae: 17.0099 - val_loss: 202.4969 - val_mae: 8.7597\n",
      "Epoch 27/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 498.3114 - mae: 16.3377 - val_loss: 192.4594 - val_mae: 8.3572\n",
      "Epoch 28/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 489.8753 - mae: 16.1355 - val_loss: 194.3953 - val_mae: 8.4055\n",
      "Epoch 29/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 504.3362 - mae: 16.5091 - val_loss: 203.4252 - val_mae: 9.0010\n",
      "Epoch 30/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 476.8023 - mae: 15.8017 - val_loss: 186.6426 - val_mae: 8.1249\n",
      "Epoch 31/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 447.9958 - mae: 15.4307 - val_loss: 181.1078 - val_mae: 7.9888\n",
      "Epoch 32/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 476.1056 - mae: 15.8891 - val_loss: 177.2424 - val_mae: 8.0112\n",
      "Epoch 33/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 434.2758 - mae: 15.1753 - val_loss: 181.0046 - val_mae: 7.9960\n",
      "Epoch 34/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 427.0627 - mae: 15.1145 - val_loss: 182.5981 - val_mae: 7.8551\n",
      "Epoch 35/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 432.7205 - mae: 15.0584 - val_loss: 168.6222 - val_mae: 7.6520\n",
      "Epoch 36/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 421.5952 - mae: 14.9607 - val_loss: 177.3211 - val_mae: 7.9562\n",
      "Epoch 37/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 393.9502 - mae: 14.4837 - val_loss: 158.2089 - val_mae: 7.0832\n",
      "Epoch 38/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 418.2007 - mae: 14.8399 - val_loss: 167.0891 - val_mae: 7.2623\n",
      "Epoch 39/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 404.8062 - mae: 14.6741 - val_loss: 163.8786 - val_mae: 7.4762\n",
      "Epoch 40/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 398.5585 - mae: 14.7377 - val_loss: 163.0514 - val_mae: 7.3261\n",
      "Epoch 41/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 410.7319 - mae: 14.5981 - val_loss: 165.0062 - val_mae: 7.6277\n",
      "Epoch 42/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 386.0760 - mae: 14.4724 - val_loss: 153.7382 - val_mae: 7.2236\n",
      "Epoch 43/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 399.9136 - mae: 14.4656 - val_loss: 169.8399 - val_mae: 7.3702\n",
      "Epoch 44/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 398.5444 - mae: 14.3872 - val_loss: 159.7702 - val_mae: 7.1289\n",
      "Epoch 45/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 375.8732 - mae: 14.0610 - val_loss: 159.2938 - val_mae: 7.0222\n",
      "Epoch 46/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 381.0134 - mae: 14.0687 - val_loss: 158.6948 - val_mae: 6.9900\n",
      "Epoch 47/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 366.7165 - mae: 13.8957 - val_loss: 146.8775 - val_mae: 6.5228\n",
      "Epoch 48/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 352.6334 - mae: 13.7246 - val_loss: 156.0715 - val_mae: 6.7760\n",
      "Epoch 49/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 395.5543 - mae: 14.4624 - val_loss: 145.6146 - val_mae: 6.5318\n",
      "Epoch 50/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 392.9515 - mae: 14.3807 - val_loss: 145.4649 - val_mae: 6.5800\n",
      "Epoch 51/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 375.2849 - mae: 14.0645 - val_loss: 140.0597 - val_mae: 6.3995\n",
      "Epoch 52/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 394.7007 - mae: 14.4354 - val_loss: 151.5676 - val_mae: 6.6649\n",
      "Epoch 53/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 378.1669 - mae: 13.9548 - val_loss: 143.9272 - val_mae: 6.2327\n",
      "Epoch 54/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 373.5789 - mae: 14.0360 - val_loss: 144.5201 - val_mae: 6.6235\n",
      "Epoch 55/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 380.3987 - mae: 14.2038 - val_loss: 142.3328 - val_mae: 6.1995\n",
      "Epoch 56/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 349.4840 - mae: 13.5753 - val_loss: 137.7355 - val_mae: 6.0356\n",
      "Epoch 57/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 349.8545 - mae: 13.6444 - val_loss: 147.0552 - val_mae: 6.5016\n",
      "Epoch 58/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 353.6379 - mae: 13.5837 - val_loss: 145.4967 - val_mae: 6.7539\n",
      "Epoch 59/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 346.8242 - mae: 13.4034 - val_loss: 142.6587 - val_mae: 6.1876\n",
      "Epoch 60/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 337.5506 - mae: 13.3788 - val_loss: 134.5949 - val_mae: 6.0494\n",
      "Epoch 61/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 353.5830 - mae: 13.6742 - val_loss: 134.7782 - val_mae: 6.1637\n",
      "Epoch 62/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 330.7615 - mae: 13.3323 - val_loss: 141.0724 - val_mae: 6.2468\n",
      "Epoch 63/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 362.7852 - mae: 13.6414 - val_loss: 144.7534 - val_mae: 6.3051\n",
      "Epoch 64/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 335.8448 - mae: 13.3294 - val_loss: 139.7510 - val_mae: 6.5566\n",
      "Epoch 65/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 343.6176 - mae: 13.2928 - val_loss: 135.7310 - val_mae: 6.1683\n",
      "Epoch 66/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 336.3797 - mae: 13.3691 - val_loss: 141.3812 - val_mae: 6.4864\n",
      "Epoch 67/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 323.4575 - mae: 13.1466 - val_loss: 137.8685 - val_mae: 6.3501\n",
      "Epoch 68/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 336.7384 - mae: 13.4253 - val_loss: 141.1097 - val_mae: 5.9989\n",
      "Epoch 69/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 338.1046 - mae: 13.2656 - val_loss: 141.0040 - val_mae: 6.3615\n",
      "Epoch 70/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 333.2258 - mae: 13.1834 - val_loss: 149.5552 - val_mae: 6.4426\n",
      "Epoch 71/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 355.0747 - mae: 13.5201 - val_loss: 137.5586 - val_mae: 5.8458\n",
      "Epoch 72/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 330.1433 - mae: 13.2725 - val_loss: 143.5795 - val_mae: 6.1892\n",
      "Epoch 73/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 331.9958 - mae: 13.2786 - val_loss: 134.9144 - val_mae: 5.6678\n",
      "Epoch 74/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 332.2864 - mae: 13.0284 - val_loss: 142.7468 - val_mae: 6.2922\n",
      "Epoch 75/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 341.5058 - mae: 13.2676 - val_loss: 135.5091 - val_mae: 6.0601\n",
      "Epoch 76/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 327.5300 - mae: 12.9176 - val_loss: 137.5147 - val_mae: 6.2674\n",
      "Epoch 77/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 320.3105 - mae: 13.0112 - val_loss: 131.0847 - val_mae: 6.0926\n",
      "Epoch 78/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 309.0563 - mae: 12.6598 - val_loss: 135.9415 - val_mae: 5.8809\n",
      "Epoch 79/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 307.3871 - mae: 12.7349 - val_loss: 133.1074 - val_mae: 5.8786\n",
      "Epoch 80/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 326.1211 - mae: 13.1072 - val_loss: 130.0729 - val_mae: 5.4569\n",
      "Epoch 81/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 310.0054 - mae: 12.9066 - val_loss: 132.2808 - val_mae: 5.5199\n",
      "Epoch 82/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 305.1870 - mae: 12.7783 - val_loss: 138.7427 - val_mae: 6.0825\n",
      "Epoch 83/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 334.4377 - mae: 13.1567 - val_loss: 134.0679 - val_mae: 5.8190\n",
      "Epoch 84/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 319.1007 - mae: 12.9168 - val_loss: 136.2099 - val_mae: 5.6521\n",
      "Epoch 85/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 326.1400 - mae: 12.9603 - val_loss: 133.7522 - val_mae: 5.8572\n",
      "Epoch 86/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 316.3405 - mae: 12.7857 - val_loss: 130.9296 - val_mae: 5.7625\n",
      "Epoch 87/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 316.9560 - mae: 12.8545 - val_loss: 127.6650 - val_mae: 5.2633\n",
      "Epoch 88/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 330.7635 - mae: 13.1242 - val_loss: 135.0401 - val_mae: 5.7944\n",
      "Epoch 89/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 308.9039 - mae: 12.7575 - val_loss: 133.0778 - val_mae: 6.0038\n",
      "Epoch 90/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 328.8048 - mae: 13.0187 - val_loss: 134.4906 - val_mae: 5.8414\n",
      "Epoch 91/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 311.5139 - mae: 12.7318 - val_loss: 131.6955 - val_mae: 5.8105\n",
      "Epoch 92/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 324.6608 - mae: 12.9095 - val_loss: 133.0946 - val_mae: 5.5755\n",
      "Epoch 93/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 312.6717 - mae: 12.8082 - val_loss: 131.6210 - val_mae: 5.3708\n",
      "Epoch 94/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 317.2302 - mae: 12.7814 - val_loss: 130.4377 - val_mae: 5.5214\n",
      "Epoch 95/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 311.4343 - mae: 12.4522 - val_loss: 135.9502 - val_mae: 5.6751\n",
      "Epoch 96/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 313.9661 - mae: 12.7369 - val_loss: 134.4140 - val_mae: 5.7398\n",
      "Epoch 97/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 292.0432 - mae: 12.3579 - val_loss: 134.5624 - val_mae: 5.9092\n",
      "Epoch 98/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 294.6865 - mae: 12.3976 - val_loss: 129.5843 - val_mae: 5.3349\n",
      "Epoch 99/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 303.9238 - mae: 12.5175 - val_loss: 128.0333 - val_mae: 5.5669\n",
      "Epoch 100/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 317.0797 - mae: 12.6825 - val_loss: 132.0192 - val_mae: 5.4132\n",
      "Epoch 101/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 304.0669 - mae: 12.5829 - val_loss: 127.9391 - val_mae: 5.6981\n",
      "Epoch 102/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 288.9468 - mae: 12.3361 - val_loss: 125.9179 - val_mae: 5.4465\n",
      "Epoch 103/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 305.1561 - mae: 12.3800 - val_loss: 129.7572 - val_mae: 5.7631\n",
      "Epoch 104/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 313.1324 - mae: 12.8042 - val_loss: 129.7436 - val_mae: 5.7264\n",
      "Epoch 105/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 314.5208 - mae: 12.6345 - val_loss: 132.1914 - val_mae: 5.9468\n",
      "Epoch 106/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 308.2432 - mae: 12.6096 - val_loss: 128.0259 - val_mae: 5.2895\n",
      "Epoch 107/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 296.7373 - mae: 12.3049 - val_loss: 127.1374 - val_mae: 5.4183\n",
      "Epoch 108/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 316.2174 - mae: 12.6331 - val_loss: 129.4901 - val_mae: 5.5863\n",
      "Epoch 109/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 302.1975 - mae: 12.3762 - val_loss: 126.8024 - val_mae: 5.3803\n",
      "Epoch 110/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 300.2772 - mae: 12.4356 - val_loss: 132.8785 - val_mae: 5.8349\n",
      "Epoch 111/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 305.1615 - mae: 12.3352 - val_loss: 129.6422 - val_mae: 5.7257\n",
      "Epoch 112/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 293.1496 - mae: 12.2343 - val_loss: 133.4530 - val_mae: 5.8470\n",
      "Epoch 113/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 273.9848 - mae: 12.0278 - val_loss: 124.9066 - val_mae: 5.4699\n",
      "Epoch 114/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 291.1111 - mae: 12.2307 - val_loss: 136.9079 - val_mae: 5.6849\n",
      "Epoch 115/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 297.3593 - mae: 12.1896 - val_loss: 126.2643 - val_mae: 5.4213\n",
      "Epoch 116/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 282.4618 - mae: 12.1937 - val_loss: 128.4077 - val_mae: 5.4180\n",
      "Epoch 117/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 280.5413 - mae: 12.0157 - val_loss: 127.0406 - val_mae: 5.4310\n",
      "Epoch 118/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 287.1934 - mae: 12.1676 - val_loss: 137.4271 - val_mae: 5.9668\n",
      "Epoch 119/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 276.3445 - mae: 12.0169 - val_loss: 123.2147 - val_mae: 5.2773\n",
      "Epoch 120/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 290.0035 - mae: 12.0929 - val_loss: 127.6493 - val_mae: 5.5156\n",
      "Epoch 121/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 304.8347 - mae: 12.2214 - val_loss: 129.9384 - val_mae: 5.6593\n",
      "Epoch 122/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 292.0925 - mae: 12.2037 - val_loss: 129.9056 - val_mae: 5.7500\n",
      "Epoch 123/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 295.3993 - mae: 12.2069 - val_loss: 126.3350 - val_mae: 5.4365\n",
      "Epoch 124/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 283.0727 - mae: 12.1680 - val_loss: 125.6927 - val_mae: 5.2855\n",
      "Epoch 125/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 286.1942 - mae: 12.0261 - val_loss: 129.4927 - val_mae: 5.4982\n",
      "Epoch 126/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 274.1836 - mae: 11.9731 - val_loss: 125.6452 - val_mae: 5.3836\n",
      "Epoch 127/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 289.3090 - mae: 12.0542 - val_loss: 126.5312 - val_mae: 5.5170\n",
      "Epoch 128/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 312.1136 - mae: 12.2756 - val_loss: 131.0422 - val_mae: 5.4106\n",
      "Epoch 129/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 316.0182 - mae: 12.5717 - val_loss: 129.8180 - val_mae: 5.3521\n",
      "Epoch 130/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 274.1264 - mae: 11.8094 - val_loss: 124.1262 - val_mae: 5.0353\n",
      "Epoch 131/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 291.7623 - mae: 12.1775 - val_loss: 124.5086 - val_mae: 5.2031\n",
      "Epoch 132/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 287.0774 - mae: 11.8530 - val_loss: 126.6333 - val_mae: 5.6141\n",
      "Epoch 133/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 293.5969 - mae: 12.2089 - val_loss: 130.8412 - val_mae: 5.2398\n",
      "Epoch 134/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 276.8095 - mae: 11.8695 - val_loss: 127.9828 - val_mae: 5.7864\n",
      "Epoch 135/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 271.9256 - mae: 11.8088 - val_loss: 125.0710 - val_mae: 5.1547\n",
      "Epoch 136/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 275.0114 - mae: 11.9560 - val_loss: 124.6108 - val_mae: 5.4250\n",
      "Epoch 137/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 286.9822 - mae: 11.8778 - val_loss: 126.3885 - val_mae: 5.5950\n",
      "Epoch 138/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 265.5974 - mae: 11.8069 - val_loss: 119.9426 - val_mae: 5.0355\n",
      "Epoch 139/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 292.7196 - mae: 12.2460 - val_loss: 125.7162 - val_mae: 5.6214\n",
      "Epoch 140/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 270.4190 - mae: 11.8307 - val_loss: 135.5919 - val_mae: 6.1315\n",
      "Epoch 141/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 279.4822 - mae: 12.0412 - val_loss: 124.4582 - val_mae: 5.4504\n",
      "Epoch 142/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 271.9390 - mae: 11.6510 - val_loss: 120.2376 - val_mae: 5.1260\n",
      "Epoch 143/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 271.1177 - mae: 11.7425 - val_loss: 122.4024 - val_mae: 5.0875\n",
      "Epoch 144/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 268.9728 - mae: 11.7446 - val_loss: 123.2064 - val_mae: 5.2902\n",
      "Epoch 145/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 277.5310 - mae: 11.7892 - val_loss: 126.1500 - val_mae: 5.7263\n",
      "Epoch 146/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 269.3821 - mae: 11.7700 - val_loss: 121.8127 - val_mae: 5.3377\n",
      "Epoch 147/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 268.6556 - mae: 11.7136 - val_loss: 118.5802 - val_mae: 5.0620\n",
      "Epoch 148/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 263.1586 - mae: 11.4375 - val_loss: 124.8687 - val_mae: 5.1699\n",
      "Epoch 149/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 286.9114 - mae: 11.9197 - val_loss: 120.0956 - val_mae: 5.1805\n",
      "Epoch 150/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 271.0739 - mae: 11.6343 - val_loss: 123.0845 - val_mae: 5.4454\n",
      "Epoch 151/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 272.4749 - mae: 11.8312 - val_loss: 125.8201 - val_mae: 5.2388\n",
      "Epoch 152/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 278.2940 - mae: 11.6862 - val_loss: 122.8380 - val_mae: 5.0455\n",
      "Epoch 153/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 275.8693 - mae: 11.7392 - val_loss: 125.8377 - val_mae: 5.3354\n",
      "Epoch 154/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 291.3521 - mae: 11.9170 - val_loss: 123.6034 - val_mae: 5.3091\n",
      "Epoch 155/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 262.6227 - mae: 11.5762 - val_loss: 123.4320 - val_mae: 4.9821\n",
      "Epoch 156/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 274.1136 - mae: 11.8192 - val_loss: 120.0776 - val_mae: 4.8735\n",
      "Epoch 157/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 262.0596 - mae: 11.4809 - val_loss: 127.2526 - val_mae: 5.6458\n",
      "Epoch 158/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 264.1805 - mae: 11.5966 - val_loss: 123.7478 - val_mae: 5.3398\n",
      "Epoch 159/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 261.6165 - mae: 11.5651 - val_loss: 125.3484 - val_mae: 5.2445\n",
      "Epoch 160/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 264.8584 - mae: 11.5692 - val_loss: 128.1176 - val_mae: 5.7285\n",
      "Epoch 161/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 260.9037 - mae: 11.4559 - val_loss: 121.5954 - val_mae: 5.0642\n",
      "Epoch 162/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 285.8241 - mae: 11.7766 - val_loss: 127.9306 - val_mae: 5.7291\n",
      "Epoch 163/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 260.1214 - mae: 11.4494 - val_loss: 129.0783 - val_mae: 5.4913\n",
      "Epoch 164/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 255.2411 - mae: 11.3782 - val_loss: 122.3027 - val_mae: 4.9237\n",
      "Epoch 165/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 270.3045 - mae: 11.6850 - val_loss: 124.3521 - val_mae: 5.1923\n",
      "Epoch 166/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 254.2963 - mae: 11.3897 - val_loss: 123.1327 - val_mae: 5.6116\n",
      "Epoch 167/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 251.2825 - mae: 11.3428 - val_loss: 118.6674 - val_mae: 4.7972\n",
      "Epoch 168/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 259.0159 - mae: 11.4519 - val_loss: 118.6172 - val_mae: 5.1835\n",
      "Epoch 169/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 244.9789 - mae: 11.3239 - val_loss: 135.0039 - val_mae: 6.2143\n",
      "Epoch 170/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 262.6050 - mae: 11.5145 - val_loss: 127.8947 - val_mae: 5.7669\n",
      "Epoch 171/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 254.3720 - mae: 11.4012 - val_loss: 120.4299 - val_mae: 5.1329\n",
      "Epoch 172/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 251.1597 - mae: 11.3471 - val_loss: 122.3178 - val_mae: 5.0607\n",
      "Epoch 173/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 254.8493 - mae: 11.3547 - val_loss: 123.2659 - val_mae: 5.0968\n",
      "Epoch 174/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 284.8811 - mae: 11.5372 - val_loss: 119.3563 - val_mae: 5.3250\n",
      "Epoch 175/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 277.5886 - mae: 11.8179 - val_loss: 124.1822 - val_mae: 5.4530\n",
      "Epoch 176/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 263.4816 - mae: 11.4141 - val_loss: 119.4492 - val_mae: 5.0922\n",
      "Epoch 177/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 271.4553 - mae: 11.4726 - val_loss: 116.7297 - val_mae: 5.0099\n",
      "Epoch 178/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 263.6775 - mae: 11.3833 - val_loss: 122.2525 - val_mae: 4.9670\n",
      "Epoch 179/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 258.4435 - mae: 11.4908 - val_loss: 124.5211 - val_mae: 5.5297\n",
      "Epoch 180/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 250.6903 - mae: 11.2447 - val_loss: 121.2096 - val_mae: 5.2708\n",
      "Epoch 181/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 268.6470 - mae: 11.5703 - val_loss: 126.8683 - val_mae: 5.6564\n",
      "Epoch 182/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 251.7696 - mae: 11.1801 - val_loss: 118.3149 - val_mae: 4.8871\n",
      "Epoch 183/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 253.1977 - mae: 11.3208 - val_loss: 122.9567 - val_mae: 4.9595\n",
      "Epoch 184/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 252.8279 - mae: 11.3883 - val_loss: 120.0097 - val_mae: 4.9273\n",
      "Epoch 185/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 265.0215 - mae: 11.4545 - val_loss: 125.5278 - val_mae: 5.4911\n",
      "Epoch 186/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 251.6980 - mae: 11.3583 - val_loss: 121.7885 - val_mae: 4.9249\n",
      "Epoch 187/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 262.8847 - mae: 11.4584 - val_loss: 118.0621 - val_mae: 4.8296\n",
      "Epoch 188/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 264.3904 - mae: 11.4433 - val_loss: 123.3351 - val_mae: 5.3301\n",
      "Epoch 189/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 265.2850 - mae: 11.3230 - val_loss: 121.6134 - val_mae: 4.8991\n",
      "Epoch 190/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 240.9031 - mae: 11.0466 - val_loss: 123.9161 - val_mae: 5.2426\n",
      "Epoch 191/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 241.7944 - mae: 11.1864 - val_loss: 118.6716 - val_mae: 4.9890\n",
      "Epoch 192/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 251.3183 - mae: 11.2899 - val_loss: 123.7237 - val_mae: 5.2951\n",
      "Epoch 193/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 270.2622 - mae: 11.3448 - val_loss: 121.4819 - val_mae: 5.1636\n",
      "Epoch 194/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 247.4336 - mae: 11.2416 - val_loss: 118.4268 - val_mae: 4.9530\n",
      "Epoch 195/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 255.5282 - mae: 11.3415 - val_loss: 119.9705 - val_mae: 5.1025\n",
      "Epoch 196/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 249.0152 - mae: 11.0183 - val_loss: 123.3759 - val_mae: 5.4913\n",
      "Epoch 197/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 253.0398 - mae: 11.3195 - val_loss: 123.1480 - val_mae: 5.3592\n",
      "Epoch 198/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 261.8106 - mae: 11.4052 - val_loss: 116.8709 - val_mae: 4.7505\n",
      "Epoch 199/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 247.2485 - mae: 11.2901 - val_loss: 122.1532 - val_mae: 5.6475\n",
      "Epoch 200/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 252.7776 - mae: 11.1742 - val_loss: 120.1693 - val_mae: 4.9748\n",
      "Epoch 201/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 265.0126 - mae: 11.3728 - val_loss: 122.1242 - val_mae: 5.2308\n",
      "Epoch 202/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 253.6338 - mae: 11.2694 - val_loss: 114.2234 - val_mae: 4.6237\n",
      "Epoch 203/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 229.0331 - mae: 10.8773 - val_loss: 120.6543 - val_mae: 5.0843\n",
      "Epoch 204/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 236.8623 - mae: 11.0236 - val_loss: 118.3319 - val_mae: 4.9193\n",
      "Epoch 205/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 256.9231 - mae: 11.0437 - val_loss: 116.5912 - val_mae: 4.5792\n",
      "Epoch 206/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 262.4716 - mae: 11.3060 - val_loss: 115.9987 - val_mae: 4.8181\n",
      "Epoch 207/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 265.1923 - mae: 11.0030 - val_loss: 123.2862 - val_mae: 5.5808\n",
      "Epoch 208/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 247.7566 - mae: 11.1508 - val_loss: 121.4184 - val_mae: 5.1871\n",
      "Epoch 209/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 244.3481 - mae: 11.0248 - val_loss: 117.2372 - val_mae: 5.2356\n",
      "Epoch 210/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 240.2999 - mae: 11.0053 - val_loss: 122.0991 - val_mae: 5.3401\n",
      "Epoch 211/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 247.6872 - mae: 11.0947 - val_loss: 116.2501 - val_mae: 4.7426\n",
      "Epoch 212/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 248.3985 - mae: 11.1338 - val_loss: 121.7043 - val_mae: 5.3096\n",
      "Epoch 213/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 239.9267 - mae: 10.8403 - val_loss: 117.2048 - val_mae: 4.9488\n",
      "Epoch 214/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 252.9456 - mae: 11.0240 - val_loss: 117.3645 - val_mae: 4.8782\n",
      "Epoch 215/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 239.7558 - mae: 10.9295 - val_loss: 119.4692 - val_mae: 4.9965\n",
      "Epoch 216/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 253.6314 - mae: 11.2587 - val_loss: 119.0528 - val_mae: 4.8739\n",
      "Epoch 217/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 255.9030 - mae: 11.1112 - val_loss: 118.4963 - val_mae: 4.6982\n",
      "Epoch 218/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 244.2829 - mae: 10.9812 - val_loss: 119.4106 - val_mae: 5.1009\n",
      "Epoch 219/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 252.0923 - mae: 11.1353 - val_loss: 121.7677 - val_mae: 5.4782\n",
      "Epoch 220/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 239.9927 - mae: 10.7945 - val_loss: 119.7680 - val_mae: 4.9192\n",
      "Epoch 221/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 240.4161 - mae: 10.9796 - val_loss: 123.0722 - val_mae: 5.3467\n",
      "Epoch 222/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 230.2496 - mae: 10.7890 - val_loss: 119.2984 - val_mae: 5.2171\n",
      "Epoch 223/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 237.3594 - mae: 10.8087 - val_loss: 120.7537 - val_mae: 5.2540\n",
      "Epoch 224/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 239.9393 - mae: 10.6317 - val_loss: 117.1931 - val_mae: 4.8130\n",
      "Epoch 225/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 244.6816 - mae: 10.9252 - val_loss: 125.6571 - val_mae: 5.7327\n",
      "Epoch 226/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 236.2580 - mae: 10.7887 - val_loss: 117.1509 - val_mae: 4.7208\n",
      "Epoch 227/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 250.4283 - mae: 11.1480 - val_loss: 123.9533 - val_mae: 4.8760\n",
      "Epoch 228/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 228.9244 - mae: 10.7723 - val_loss: 117.5510 - val_mae: 4.7729\n",
      "Epoch 229/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 252.3602 - mae: 10.9371 - val_loss: 122.1971 - val_mae: 5.2797\n",
      "Epoch 230/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 239.2440 - mae: 10.9130 - val_loss: 115.6952 - val_mae: 4.8072\n",
      "Epoch 231/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 235.3879 - mae: 10.7905 - val_loss: 117.7534 - val_mae: 4.7550\n",
      "Epoch 232/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 234.6656 - mae: 10.8421 - val_loss: 121.0667 - val_mae: 5.1885\n",
      "Epoch 233/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 236.3444 - mae: 10.9261 - val_loss: 122.0996 - val_mae: 5.1356\n",
      "Epoch 234/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 236.7965 - mae: 10.8413 - val_loss: 116.1625 - val_mae: 4.7357\n",
      "Epoch 235/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 242.2156 - mae: 10.8775 - val_loss: 120.0200 - val_mae: 5.0309\n",
      "Epoch 236/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 240.4622 - mae: 10.7863 - val_loss: 119.2581 - val_mae: 5.1824\n",
      "Epoch 237/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 232.1784 - mae: 10.5382 - val_loss: 115.9168 - val_mae: 4.6678\n",
      "Epoch 238/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 234.0325 - mae: 10.6831 - val_loss: 122.4891 - val_mae: 4.9325\n",
      "Epoch 239/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 231.3116 - mae: 10.5819 - val_loss: 121.4083 - val_mae: 5.0514\n",
      "Epoch 240/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 242.6935 - mae: 10.6989 - val_loss: 120.1702 - val_mae: 5.1548\n",
      "Epoch 241/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 247.9644 - mae: 10.9539 - val_loss: 125.3298 - val_mae: 5.1180\n",
      "Epoch 242/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 235.7195 - mae: 10.7431 - val_loss: 122.8633 - val_mae: 5.2168\n",
      "Epoch 243/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 229.7571 - mae: 10.6559 - val_loss: 116.7230 - val_mae: 4.8519\n",
      "Epoch 244/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 246.1704 - mae: 10.9240 - val_loss: 120.2234 - val_mae: 4.9919\n",
      "Epoch 245/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 226.8449 - mae: 10.7212 - val_loss: 118.4097 - val_mae: 5.2782\n",
      "Epoch 246/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 237.4826 - mae: 10.7821 - val_loss: 114.3942 - val_mae: 4.4105\n",
      "Epoch 247/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 230.8404 - mae: 10.6235 - val_loss: 120.0676 - val_mae: 5.0721\n",
      "Epoch 248/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 237.5007 - mae: 10.6216 - val_loss: 122.4851 - val_mae: 5.2153\n",
      "Epoch 249/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 232.7081 - mae: 10.5535 - val_loss: 118.8374 - val_mae: 4.8283\n",
      "Epoch 250/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 224.8658 - mae: 10.5796 - val_loss: 119.0247 - val_mae: 5.0592\n",
      "Epoch 251/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 243.0063 - mae: 10.6753 - val_loss: 120.1229 - val_mae: 5.2380\n",
      "Epoch 252/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 218.4158 - mae: 10.3683 - val_loss: 121.6154 - val_mae: 5.1432\n",
      "Epoch 253/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 233.9232 - mae: 10.7340 - val_loss: 129.8370 - val_mae: 5.8673\n",
      "Epoch 254/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 234.6336 - mae: 10.6504 - val_loss: 119.3317 - val_mae: 4.9714\n",
      "Epoch 255/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 241.1581 - mae: 10.6756 - val_loss: 117.3109 - val_mae: 5.0322\n",
      "Epoch 256/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 225.0124 - mae: 10.7266 - val_loss: 114.8670 - val_mae: 4.5901\n",
      "Epoch 257/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 224.8021 - mae: 10.4951 - val_loss: 117.2042 - val_mae: 4.9918\n",
      "Epoch 258/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 230.5468 - mae: 10.5541 - val_loss: 118.5658 - val_mae: 4.9757\n",
      "Epoch 259/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 245.3679 - mae: 10.8510 - val_loss: 118.0217 - val_mae: 4.9105\n",
      "Epoch 260/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 225.3391 - mae: 10.4611 - val_loss: 116.5719 - val_mae: 4.5265\n",
      "Epoch 261/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 223.3266 - mae: 10.4410 - val_loss: 116.9013 - val_mae: 5.2019\n",
      "Epoch 262/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 228.3956 - mae: 10.4824 - val_loss: 119.6225 - val_mae: 5.0574\n",
      "Epoch 263/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 215.6095 - mae: 10.3707 - val_loss: 117.1246 - val_mae: 5.0413\n",
      "Epoch 264/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 230.9858 - mae: 10.4619 - val_loss: 116.9610 - val_mae: 4.8345\n",
      "Epoch 265/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 229.4407 - mae: 10.5060 - val_loss: 121.3399 - val_mae: 4.9442\n",
      "Epoch 266/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 224.5725 - mae: 10.3745 - val_loss: 117.6877 - val_mae: 4.6775\n",
      "Epoch 267/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 227.1078 - mae: 10.5562 - val_loss: 122.6350 - val_mae: 5.1181\n",
      "Epoch 268/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 228.0285 - mae: 10.5432 - val_loss: 119.9928 - val_mae: 5.1705\n",
      "Epoch 269/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 231.1643 - mae: 10.5639 - val_loss: 121.4380 - val_mae: 5.3802\n",
      "Epoch 270/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 229.5822 - mae: 10.5717 - val_loss: 115.3824 - val_mae: 4.6868\n",
      "Epoch 271/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 228.4603 - mae: 10.4826 - val_loss: 114.5908 - val_mae: 4.8871\n",
      "Epoch 272/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 244.1770 - mae: 10.5300 - val_loss: 117.6225 - val_mae: 4.7543\n",
      "Epoch 273/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 227.9718 - mae: 10.3488 - val_loss: 117.9460 - val_mae: 4.7427\n",
      "Epoch 274/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 237.5141 - mae: 10.5580 - val_loss: 114.1914 - val_mae: 4.4724\n",
      "Epoch 275/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 218.2303 - mae: 10.3041 - val_loss: 113.9465 - val_mae: 4.7537\n",
      "Epoch 276/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 234.2002 - mae: 10.4295 - val_loss: 121.2352 - val_mae: 5.0676\n",
      "Epoch 277/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 227.8678 - mae: 10.3212 - val_loss: 126.2790 - val_mae: 5.5752\n",
      "Epoch 278/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 213.2175 - mae: 10.1043 - val_loss: 120.1523 - val_mae: 4.7599\n",
      "Epoch 279/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 215.9426 - mae: 10.2981 - val_loss: 117.1389 - val_mae: 4.9659\n",
      "Epoch 280/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 234.1078 - mae: 10.4399 - val_loss: 121.1458 - val_mae: 5.3977\n",
      "Epoch 281/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 215.7926 - mae: 10.1665 - val_loss: 119.1112 - val_mae: 5.1402\n",
      "Epoch 282/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 220.2262 - mae: 10.3254 - val_loss: 117.8810 - val_mae: 4.9679\n",
      "Epoch 283/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 221.1815 - mae: 10.3060 - val_loss: 117.2892 - val_mae: 4.9010\n",
      "Epoch 284/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 211.0232 - mae: 10.0488 - val_loss: 121.0298 - val_mae: 5.5275\n",
      "Epoch 285/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 232.8611 - mae: 10.3799 - val_loss: 125.5491 - val_mae: 5.4442\n",
      "Epoch 286/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 222.4707 - mae: 10.3492 - val_loss: 115.7135 - val_mae: 4.9362\n",
      "Epoch 287/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 218.7410 - mae: 10.3201 - val_loss: 111.4990 - val_mae: 4.5178\n",
      "Epoch 288/300\n",
      "\u001b[1m205/602\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 215.1334 - mae: 10.4211"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39moptimizer, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmae\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# 4️⃣ Train for More Epochs\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_clean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Increased Epochs\u001b[39;49;00m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m  \u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# 5️⃣ Make Predictions\u001b[39;00m\n\u001b[1;32m     46\u001b[0m y_pred_nn \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test_scaled)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_new_env/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_new_env/lib/python3.9/site-packages/keras/src/backend/tensorflow/trainer.py:371\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[1;32m    370\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 371\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    372\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_new_env/lib/python3.9/site-packages/keras/src/backend/tensorflow/trainer.py:219\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfunction\u001b[39m(iterator):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    217\u001b[0m         iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[1;32m    218\u001b[0m     ):\n\u001b[0;32m--> 219\u001b[0m         opt_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[1;32m    221\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_new_env/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_new_env/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_new_env/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_new_env/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_new_env/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_new_env/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_new_env/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_new_env/lib/python3.9/site-packages/tensorflow/python/eager/context.py:1683\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1683\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1684\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1685\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1686\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1687\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1688\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1689\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1690\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1691\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1692\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1693\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1697\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1698\u001b[0m   )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/my_new_env/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_clean)\n",
    "X_test_scaled = scaler.transform(X_test_df)\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(512, activation='swish', kernel_regularizer=l1_l2(0.0001, 0.0001), input_shape=(X_train_scaled.shape[1],)),  \n",
    "    BatchNormalization(),\n",
    "    Dropout(0.2),  \n",
    "\n",
    "    Dense(256, activation='swish', kernel_regularizer=l1_l2(0.0001, 0.0001)),  \n",
    "    BatchNormalization(),\n",
    "    Dropout(0.2),  \n",
    "\n",
    "    Dense(128, activation='swish', kernel_regularizer=l1_l2(0.0001, 0.0001)),  \n",
    "    BatchNormalization(),\n",
    "    Dropout(0.2),  \n",
    "\n",
    "    Dense(64, activation='swish', kernel_regularizer=l1_l2(0.0001, 0.0001)),  \n",
    "    Dense(1, activation='linear') \n",
    "])\n",
    "\n",
    "optimizer = Adam(learning_rate=0.0001, clipnorm=0.5)  \n",
    "model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train_clean,\n",
    "    validation_data=(X_test_scaled, y_test),\n",
    "    epochs=300, \n",
    "    batch_size=32,\n",
    "    verbose=1  \n",
    ")\n",
    "\n",
    "y_pred_nn = model.predict(X_test_scaled)\n",
    "\n",
    "rmse_nn = np.sqrt(mean_squared_error(y_test, y_pred_nn))\n",
    "r2_nn = r2_score(y_test, y_pred_nn)\n",
    "\n",
    "print(f\" Further Improved Neural Network - RMSE: {rmse_nn:.2f}\")\n",
    "print(f\" Further Improved Neural Network - R² Score: {r2_nn:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Blended Model - RMSE: 9.32\n",
      "🔹 Blended Model - R² Score: 0.99\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.5  \n",
    "beta = 0.5  \n",
    "\n",
    "y_pred_blended = (alpha * y_pred_nn.flatten()) + (beta * y_pred_xgboost)\n",
    "\n",
    "rmse_blended = np.sqrt(mean_squared_error(y_test, y_pred_blended))\n",
    "r2_blended = r2_score(y_test, y_pred_blended)\n",
    "\n",
    "print(f\" Blended Model - RMSE: {rmse_blended:.2f}\")\n",
    "print(f\" Blended Model - R² Score: {r2_blended:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validated RMSE: 6.80\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "rmse_list = []\n",
    "\n",
    "for train_index, val_index in kf.split(X_train_clean):\n",
    "    \n",
    "    X_train_cv, X_val_cv = X_train_clean.iloc[train_index], X_train_clean.iloc[val_index]\n",
    "    y_train_cv, y_val_cv = y_train_clean.iloc[train_index], y_train_clean.iloc[val_index]\n",
    "\n",
    "    xgboost_model.fit(X_train_cv, y_train_cv)\n",
    "    y_pred = xgboost_model.predict(X_val_cv)\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(y_val_cv, y_pred))\n",
    "    rmse_list.append(rmse)\n",
    "\n",
    "print(f\"Cross-validated RMSE: {np.mean(rmse_list):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5c/ttrql3v94vx3w3dr_82ptp280000gn/T/ipykernel_89003/2299875268.py:10: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  y_train_cv, y_val_cv = y_train_clean[train_index], y_train_clean[val_index]\n",
      "/Users/me.pranesh/Desktop/new clean/.venv/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 721us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5c/ttrql3v94vx3w3dr_82ptp280000gn/T/ipykernel_89003/2299875268.py:10: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  y_train_cv, y_val_cv = y_train_clean[train_index], y_train_clean[val_index]\n",
      "/Users/me.pranesh/Desktop/new clean/.venv/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 723us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5c/ttrql3v94vx3w3dr_82ptp280000gn/T/ipykernel_89003/2299875268.py:10: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  y_train_cv, y_val_cv = y_train_clean[train_index], y_train_clean[val_index]\n",
      "/Users/me.pranesh/Desktop/new clean/.venv/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 744us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5c/ttrql3v94vx3w3dr_82ptp280000gn/T/ipykernel_89003/2299875268.py:10: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  y_train_cv, y_val_cv = y_train_clean[train_index], y_train_clean[val_index]\n",
      "/Users/me.pranesh/Desktop/new clean/.venv/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5c/ttrql3v94vx3w3dr_82ptp280000gn/T/ipykernel_89003/2299875268.py:10: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  y_train_cv, y_val_cv = y_train_clean[train_index], y_train_clean[val_index]\n",
      "/Users/me.pranesh/Desktop/new clean/.venv/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m121/121\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 710us/step\n",
      "Cross-validated RMSE: 9.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "rmse_list = []\n",
    "\n",
    "for train_index, val_index in kf.split(X_train_scaled):\n",
    "    X_train_cv, X_val_cv = X_train_scaled[train_index], X_train_scaled[val_index]\n",
    "    y_train_cv, y_val_cv = y_train_clean[train_index], y_train_clean[val_index]\n",
    "\n",
    "    model = Sequential([\n",
    "        Dense(512, activation='swish', input_shape=(X_train_cv.shape[1],)),\n",
    "        Dense(256, activation='swish'),\n",
    "        Dense(128, activation='swish'),\n",
    "        Dense(64, activation='swish'),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "    model.fit(X_train_cv, y_train_cv, epochs=50, batch_size=32, verbose=0)\n",
    "    y_pred = model.predict(X_val_cv)\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(y_val_cv, y_pred))\n",
    "    rmse_list.append(rmse)\n",
    "\n",
    "print(f\"Cross-validated RMSE: {np.mean(rmse_list):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble RMSE (Bagging): 9.28\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "import numpy as np\n",
    "\n",
    "n_models = 5  \n",
    "rmse_list = []\n",
    "\n",
    "for i in range(n_models):\n",
    "    X_train_resampled, y_train_resampled = resample(X_train_clean, y_train_clean, n_samples=len(X_train_clean), random_state=42)\n",
    "    \n",
    "    xgboost_model.fit(X_train_resampled, y_train_resampled)\n",
    "    y_pred = xgboost_model.predict(X_test_df)\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    rmse_list.append(rmse)\n",
    "\n",
    "print(f\"Ensemble RMSE (Bagging): {np.mean(rmse_list):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 777us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/me.pranesh/Desktop/new clean/.venv/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 808us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/me.pranesh/Desktop/new clean/.venv/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 851us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/me.pranesh/Desktop/new clean/.venv/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 780us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/me.pranesh/Desktop/new clean/.venv/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 752us/step\n",
      "Ensemble RMSE (Manual Bagging, NN): 11.26\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "import numpy as np\n",
    "\n",
    "class NNModel(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, model=None):\n",
    "        self.model = model or self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = Sequential([\n",
    "            Dense(512, activation='swish', kernel_regularizer=l1_l2(0.0001, 0.0001), input_shape=(X_train_scaled.shape[1],)),  \n",
    "            BatchNormalization(),\n",
    "            Dropout(0.2),\n",
    "            Dense(256, activation='swish', kernel_regularizer=l1_l2(0.0001, 0.0001)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.2),\n",
    "            Dense(128, activation='swish', kernel_regularizer=l1_l2(0.0001, 0.0001)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.2),\n",
    "            Dense(64, activation='swish', kernel_regularizer=l1_l2(0.0001, 0.0001)),\n",
    "            Dense(1, activation='linear')\n",
    "        ])\n",
    "        optimizer = Adam(learning_rate=0.0001, clipnorm=0.5)  \n",
    "        model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "        return model\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.model.fit(X, y, epochs=100, batch_size=32, verbose=0)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X).flatten()\n",
    "\n",
    "n_estimators = 5\n",
    "predictions = np.zeros((X_test_scaled.shape[0], n_estimators))\n",
    "\n",
    "for i in range(n_estimators):\n",
    "    nn_bagger = NNModel()\n",
    "    nn_bagger.fit(X_train_scaled, y_train_clean)\n",
    "    \n",
    "    predictions[:, i] = nn_bagger.predict(X_test_scaled)\n",
    "\n",
    "y_pred_bagging = np.mean(predictions, axis=1)\n",
    "\n",
    "rmse_bagging = np.sqrt(mean_squared_error(y_test, y_pred_bagging))\n",
    "print(f\"Ensemble RMSE (Manual Bagging, NN): {rmse_bagging:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/me.pranesh/Desktop/new clean/.venv/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 20491.5449 - mae: 114.2335 - val_loss: 1466.7815 - val_mae: 29.7800 - learning_rate: 5.0000e-04\n",
      "Epoch 2/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 924us/step - loss: 1746.2476 - mae: 32.6594 - val_loss: 663.7928 - val_mae: 18.2728 - learning_rate: 5.0000e-04\n",
      "Epoch 3/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 922us/step - loss: 988.0432 - mae: 23.7592 - val_loss: 468.3336 - val_mae: 14.8683 - learning_rate: 5.0000e-04\n",
      "Epoch 4/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 912us/step - loss: 792.4912 - mae: 20.8582 - val_loss: 449.6706 - val_mae: 14.6139 - learning_rate: 5.0000e-04\n",
      "Epoch 5/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 912us/step - loss: 768.1743 - mae: 20.5064 - val_loss: 413.8792 - val_mae: 13.6403 - learning_rate: 5.0000e-04\n",
      "Epoch 6/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 913us/step - loss: 670.8211 - mae: 19.1335 - val_loss: 358.0653 - val_mae: 12.7479 - learning_rate: 5.0000e-04\n",
      "Epoch 7/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 912us/step - loss: 611.8300 - mae: 18.3104 - val_loss: 414.4469 - val_mae: 13.9208 - learning_rate: 5.0000e-04\n",
      "Epoch 8/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 916us/step - loss: 632.8385 - mae: 18.4962 - val_loss: 312.9984 - val_mae: 11.5462 - learning_rate: 5.0000e-04\n",
      "Epoch 9/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 957us/step - loss: 561.0142 - mae: 17.4944 - val_loss: 356.9307 - val_mae: 12.5024 - learning_rate: 5.0000e-04\n",
      "Epoch 10/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 542.1661 - mae: 17.1710 - val_loss: 321.0232 - val_mae: 11.6401 - learning_rate: 5.0000e-04\n",
      "Epoch 11/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 530.6381 - mae: 16.8172 - val_loss: 333.2752 - val_mae: 12.1845 - learning_rate: 5.0000e-04\n",
      "Epoch 12/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 477.9899 - mae: 16.1435 - val_loss: 282.0550 - val_mae: 10.8408 - learning_rate: 5.0000e-04\n",
      "Epoch 13/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 927us/step - loss: 496.9827 - mae: 16.0362 - val_loss: 287.9650 - val_mae: 10.8672 - learning_rate: 5.0000e-04\n",
      "Epoch 14/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 923us/step - loss: 470.7805 - mae: 15.9303 - val_loss: 305.9046 - val_mae: 11.2622 - learning_rate: 5.0000e-04\n",
      "Epoch 15/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 868us/step - loss: 467.1620 - mae: 15.7628 - val_loss: 272.8608 - val_mae: 10.2045 - learning_rate: 5.0000e-04\n",
      "Epoch 16/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 855us/step - loss: 467.7796 - mae: 15.8678 - val_loss: 279.6178 - val_mae: 10.6483 - learning_rate: 5.0000e-04\n",
      "Epoch 17/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 874us/step - loss: 445.0441 - mae: 15.4722 - val_loss: 293.7483 - val_mae: 10.6943 - learning_rate: 5.0000e-04\n",
      "Epoch 18/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 876us/step - loss: 429.0125 - mae: 15.1329 - val_loss: 333.4357 - val_mae: 11.6755 - learning_rate: 5.0000e-04\n",
      "Epoch 19/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 913us/step - loss: 430.1300 - mae: 15.0854 - val_loss: 270.7473 - val_mae: 10.3286 - learning_rate: 5.0000e-04\n",
      "Epoch 20/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 874us/step - loss: 420.6790 - mae: 14.8408 - val_loss: 321.0952 - val_mae: 11.2755 - learning_rate: 5.0000e-04\n",
      "Epoch 21/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 872us/step - loss: 411.0776 - mae: 14.5547 - val_loss: 302.8941 - val_mae: 10.7212 - learning_rate: 5.0000e-04\n",
      "Epoch 22/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 877us/step - loss: 400.0524 - mae: 14.5767 - val_loss: 349.5050 - val_mae: 11.6242 - learning_rate: 5.0000e-04\n",
      "Epoch 23/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 955us/step - loss: 388.0864 - mae: 14.3477 - val_loss: 336.2164 - val_mae: 11.2725 - learning_rate: 5.0000e-04\n",
      "Epoch 24/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 868us/step - loss: 383.9392 - mae: 14.1034 - val_loss: 268.9351 - val_mae: 10.4346 - learning_rate: 5.0000e-04\n",
      "Epoch 25/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 878us/step - loss: 387.7769 - mae: 14.3865 - val_loss: 284.8148 - val_mae: 10.3843 - learning_rate: 5.0000e-04\n",
      "Epoch 26/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 905us/step - loss: 376.2397 - mae: 14.0111 - val_loss: 259.1081 - val_mae: 9.8370 - learning_rate: 5.0000e-04\n",
      "Epoch 27/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 992us/step - loss: 373.3757 - mae: 14.0511 - val_loss: 286.0661 - val_mae: 9.7868 - learning_rate: 5.0000e-04\n",
      "Epoch 28/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 924us/step - loss: 369.0653 - mae: 13.7791 - val_loss: 287.7702 - val_mae: 9.9243 - learning_rate: 5.0000e-04\n",
      "Epoch 29/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 968us/step - loss: 370.2244 - mae: 13.8663 - val_loss: 255.2915 - val_mae: 9.7481 - learning_rate: 5.0000e-04\n",
      "Epoch 30/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 987us/step - loss: 350.4816 - mae: 13.4753 - val_loss: 258.7466 - val_mae: 9.4095 - learning_rate: 5.0000e-04\n",
      "Epoch 31/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 962us/step - loss: 341.3198 - mae: 13.4547 - val_loss: 247.0206 - val_mae: 9.0451 - learning_rate: 5.0000e-04\n",
      "Epoch 32/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 967us/step - loss: 347.7489 - mae: 13.3676 - val_loss: 320.0943 - val_mae: 11.4257 - learning_rate: 5.0000e-04\n",
      "Epoch 33/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 352.2179 - mae: 13.3882 - val_loss: 252.8315 - val_mae: 9.0391 - learning_rate: 5.0000e-04\n",
      "Epoch 34/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 976us/step - loss: 331.2400 - mae: 13.1449 - val_loss: 238.4897 - val_mae: 8.8034 - learning_rate: 5.0000e-04\n",
      "Epoch 35/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 922us/step - loss: 337.7294 - mae: 13.2202 - val_loss: 296.7239 - val_mae: 10.1373 - learning_rate: 5.0000e-04\n",
      "Epoch 36/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 919us/step - loss: 329.8658 - mae: 13.0012 - val_loss: 231.5860 - val_mae: 8.8853 - learning_rate: 5.0000e-04\n",
      "Epoch 37/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 869us/step - loss: 323.7776 - mae: 12.9177 - val_loss: 280.2252 - val_mae: 9.7413 - learning_rate: 5.0000e-04\n",
      "Epoch 38/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 887us/step - loss: 310.1219 - mae: 12.7657 - val_loss: 257.8500 - val_mae: 9.0296 - learning_rate: 5.0000e-04\n",
      "Epoch 39/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 956us/step - loss: 314.0084 - mae: 12.7852 - val_loss: 262.9708 - val_mae: 9.4216 - learning_rate: 5.0000e-04\n",
      "Epoch 40/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 913us/step - loss: 319.8252 - mae: 12.5458 - val_loss: 263.7830 - val_mae: 9.6928 - learning_rate: 5.0000e-04\n",
      "Epoch 41/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 862us/step - loss: 318.2897 - mae: 12.7338 - val_loss: 256.9973 - val_mae: 9.0830 - learning_rate: 5.0000e-04\n",
      "Epoch 42/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 843us/step - loss: 314.9369 - mae: 12.4597 - val_loss: 264.1319 - val_mae: 9.2910 - learning_rate: 2.5000e-04\n",
      "Epoch 43/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 905us/step - loss: 319.1198 - mae: 12.7846 - val_loss: 227.7003 - val_mae: 8.5306 - learning_rate: 2.5000e-04\n",
      "Epoch 44/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 851us/step - loss: 289.7700 - mae: 12.1711 - val_loss: 256.6801 - val_mae: 9.1663 - learning_rate: 2.5000e-04\n",
      "Epoch 45/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 864us/step - loss: 304.7215 - mae: 12.4389 - val_loss: 246.1734 - val_mae: 8.6444 - learning_rate: 2.5000e-04\n",
      "Epoch 46/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 862us/step - loss: 321.5825 - mae: 12.5113 - val_loss: 249.9652 - val_mae: 9.0008 - learning_rate: 2.5000e-04\n",
      "Epoch 47/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 851us/step - loss: 306.4005 - mae: 12.3316 - val_loss: 242.2121 - val_mae: 8.5193 - learning_rate: 2.5000e-04\n",
      "Epoch 48/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 858us/step - loss: 287.7571 - mae: 12.0076 - val_loss: 257.6568 - val_mae: 9.4577 - learning_rate: 2.5000e-04\n",
      "Epoch 49/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 862us/step - loss: 299.5934 - mae: 12.1769 - val_loss: 239.6087 - val_mae: 8.7440 - learning_rate: 1.2500e-04\n",
      "Epoch 50/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 850us/step - loss: 310.3875 - mae: 12.1390 - val_loss: 258.6694 - val_mae: 8.9820 - learning_rate: 1.2500e-04\n",
      "Epoch 51/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 856us/step - loss: 286.2107 - mae: 11.8992 - val_loss: 279.5315 - val_mae: 9.5183 - learning_rate: 1.2500e-04\n",
      "Epoch 52/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 901us/step - loss: 278.1951 - mae: 11.8649 - val_loss: 248.8995 - val_mae: 8.8473 - learning_rate: 1.2500e-04\n",
      "Epoch 53/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 867us/step - loss: 281.8228 - mae: 11.9437 - val_loss: 245.7836 - val_mae: 8.9279 - learning_rate: 1.2500e-04\n",
      "Epoch 54/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 858us/step - loss: 289.0635 - mae: 11.8776 - val_loss: 254.7337 - val_mae: 8.6938 - learning_rate: 6.2500e-05\n",
      "Epoch 55/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 864us/step - loss: 271.6045 - mae: 11.9004 - val_loss: 237.4643 - val_mae: 8.3875 - learning_rate: 6.2500e-05\n",
      "Epoch 56/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 927us/step - loss: 269.0797 - mae: 11.5465 - val_loss: 263.3276 - val_mae: 9.1669 - learning_rate: 6.2500e-05\n",
      "Epoch 57/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 865us/step - loss: 280.7805 - mae: 12.0187 - val_loss: 272.8951 - val_mae: 9.4569 - learning_rate: 6.2500e-05\n",
      "Epoch 58/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 851us/step - loss: 279.8846 - mae: 11.8241 - val_loss: 263.9301 - val_mae: 8.9861 - learning_rate: 6.2500e-05\n",
      "Epoch 59/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 852us/step - loss: 274.8439 - mae: 11.7782 - val_loss: 243.5949 - val_mae: 9.0642 - learning_rate: 3.1250e-05\n",
      "Epoch 60/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 866us/step - loss: 272.1166 - mae: 11.6444 - val_loss: 248.0312 - val_mae: 8.4130 - learning_rate: 3.1250e-05\n",
      "Epoch 61/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 864us/step - loss: 295.5996 - mae: 11.9755 - val_loss: 243.7379 - val_mae: 8.6737 - learning_rate: 3.1250e-05\n",
      "Epoch 62/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 860us/step - loss: 272.7340 - mae: 11.5181 - val_loss: 261.6211 - val_mae: 8.9729 - learning_rate: 3.1250e-05\n",
      "Epoch 63/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 860us/step - loss: 273.9442 - mae: 11.7815 - val_loss: 250.7578 - val_mae: 9.1716 - learning_rate: 3.1250e-05\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 567us/step\n",
      "🔹 Improved Neural Network - RMSE: 14.76\n",
      "🔹 Improved Neural Network - R² Score: 0.98\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(256, activation='swish', kernel_regularizer=l2(0.01), input_shape=(X_train_scaled.shape[1],)),\n",
    "    Dropout(0.3),\n",
    "    BatchNormalization(),\n",
    "    Dense(128, activation='swish', kernel_regularizer=l2(0.01)),\n",
    "    Dropout(0.3),\n",
    "    BatchNormalization(),\n",
    "    Dense(64, activation='swish', kernel_regularizer=l2(0.01)),\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "optimizer = Adam(learning_rate=0.0005)\n",
    "model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train_clean,\n",
    "    validation_data=(X_test_scaled, y_test),\n",
    "    epochs=300,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping, lr_scheduler]\n",
    ")\n",
    "\n",
    "y_pred_nn = model.predict(X_test_scaled)\n",
    "rmse_nn = np.sqrt(mean_squared_error(y_test, y_pred_nn))\n",
    "r2_nn = r2_score(y_test, y_pred_nn)\n",
    "\n",
    "print(f\" Improved Neural Network - RMSE: {rmse_nn:.2f}\")\n",
    "print(f\" Improved Neural Network - R² Score: {r2_nn:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/me.pranesh/Desktop/new clean/.venv/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 24469.2012 - mae: 121.7985 - val_loss: 2289.3538 - val_mae: 35.4600\n",
      "Epoch 2/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1880.7703 - mae: 31.7921 - val_loss: 1105.6096 - val_mae: 24.1012\n",
      "Epoch 3/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1079.3130 - mae: 23.8057 - val_loss: 740.3726 - val_mae: 18.8117\n",
      "Epoch 4/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 782.0952 - mae: 19.9808 - val_loss: 551.5518 - val_mae: 15.9471\n",
      "Epoch 5/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 606.2410 - mae: 17.4574 - val_loss: 456.6242 - val_mae: 14.1549\n",
      "Epoch 6/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 567.1971 - mae: 16.5787 - val_loss: 392.5216 - val_mae: 13.0538\n",
      "Epoch 7/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 488.4397 - mae: 15.5351 - val_loss: 373.6022 - val_mae: 12.6077\n",
      "Epoch 8/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 439.5323 - mae: 14.8399 - val_loss: 333.4615 - val_mae: 11.9719\n",
      "Epoch 9/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 428.9856 - mae: 14.5364 - val_loss: 314.1639 - val_mae: 11.4754\n",
      "Epoch 10/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 389.0058 - mae: 14.0842 - val_loss: 306.7083 - val_mae: 11.3727\n",
      "Epoch 11/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 383.3381 - mae: 13.8558 - val_loss: 291.0686 - val_mae: 11.0809\n",
      "Epoch 12/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 370.4683 - mae: 13.6511 - val_loss: 281.5260 - val_mae: 10.7550\n",
      "Epoch 13/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 364.5359 - mae: 13.5241 - val_loss: 280.7433 - val_mae: 10.9696\n",
      "Epoch 14/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 363.5003 - mae: 13.2924 - val_loss: 269.0883 - val_mae: 10.5284\n",
      "Epoch 15/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 333.1293 - mae: 12.8691 - val_loss: 265.5156 - val_mae: 10.5200\n",
      "Epoch 16/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 345.7602 - mae: 13.0164 - val_loss: 258.2930 - val_mae: 10.2966\n",
      "Epoch 17/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 331.8110 - mae: 12.8087 - val_loss: 251.6451 - val_mae: 10.1760\n",
      "Epoch 18/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 321.7760 - mae: 12.6553 - val_loss: 249.0965 - val_mae: 10.1103\n",
      "Epoch 19/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 322.0640 - mae: 12.5529 - val_loss: 245.0401 - val_mae: 9.9865\n",
      "Epoch 20/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 307.5393 - mae: 12.4021 - val_loss: 234.0416 - val_mae: 9.6124\n",
      "Epoch 21/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 296.4780 - mae: 12.1358 - val_loss: 231.0066 - val_mae: 9.5616\n",
      "Epoch 22/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 280.0120 - mae: 11.8609 - val_loss: 223.0425 - val_mae: 9.3368\n",
      "Epoch 23/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 282.0125 - mae: 11.8046 - val_loss: 222.9771 - val_mae: 9.3418\n",
      "Epoch 24/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 278.6581 - mae: 11.7801 - val_loss: 215.5223 - val_mae: 9.1154\n",
      "Epoch 25/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 272.5273 - mae: 11.6424 - val_loss: 209.4188 - val_mae: 8.9190\n",
      "Epoch 26/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 278.6375 - mae: 11.6499 - val_loss: 205.2538 - val_mae: 8.7808\n",
      "Epoch 27/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 263.3457 - mae: 11.2380 - val_loss: 199.2625 - val_mae: 8.6680\n",
      "Epoch 28/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 261.8609 - mae: 11.1814 - val_loss: 200.7107 - val_mae: 8.7829\n",
      "Epoch 29/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 267.8404 - mae: 11.1309 - val_loss: 191.6437 - val_mae: 8.3973\n",
      "Epoch 30/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 244.6198 - mae: 10.7848 - val_loss: 184.4444 - val_mae: 8.1783\n",
      "Epoch 31/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 243.3739 - mae: 10.7422 - val_loss: 180.2220 - val_mae: 7.9849\n",
      "Epoch 32/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 247.6526 - mae: 10.6935 - val_loss: 174.6626 - val_mae: 7.9087\n",
      "Epoch 33/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 237.7333 - mae: 10.5394 - val_loss: 173.2764 - val_mae: 7.7068\n",
      "Epoch 34/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 223.6762 - mae: 10.2322 - val_loss: 172.2465 - val_mae: 7.9306\n",
      "Epoch 35/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 238.6428 - mae: 10.3944 - val_loss: 166.4485 - val_mae: 7.4543\n",
      "Epoch 36/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 235.4557 - mae: 10.4343 - val_loss: 167.0432 - val_mae: 7.6903\n",
      "Epoch 37/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 215.3011 - mae: 9.8371 - val_loss: 155.6964 - val_mae: 7.1999\n",
      "Epoch 38/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 218.3376 - mae: 9.8703 - val_loss: 157.1800 - val_mae: 7.0921\n",
      "Epoch 39/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 210.7819 - mae: 9.6773 - val_loss: 150.5409 - val_mae: 6.8340\n",
      "Epoch 40/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 205.4601 - mae: 9.6568 - val_loss: 148.2901 - val_mae: 6.8435\n",
      "Epoch 41/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 192.5597 - mae: 9.3503 - val_loss: 148.4717 - val_mae: 6.9460\n",
      "Epoch 42/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 208.3491 - mae: 9.4746 - val_loss: 147.5839 - val_mae: 6.6743\n",
      "Epoch 43/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 201.4138 - mae: 9.3188 - val_loss: 144.0357 - val_mae: 6.6796\n",
      "Epoch 44/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 198.1499 - mae: 9.3146 - val_loss: 139.8511 - val_mae: 6.5451\n",
      "Epoch 45/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 184.9712 - mae: 9.1286 - val_loss: 141.0886 - val_mae: 6.4214\n",
      "Epoch 46/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 195.3228 - mae: 9.1946 - val_loss: 138.6664 - val_mae: 6.3597\n",
      "Epoch 47/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 193.6504 - mae: 9.1268 - val_loss: 145.4037 - val_mae: 6.7261\n",
      "Epoch 48/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 184.1927 - mae: 8.9915 - val_loss: 136.5006 - val_mae: 6.3223\n",
      "Epoch 49/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 193.2948 - mae: 8.9128 - val_loss: 144.6340 - val_mae: 6.7185\n",
      "Epoch 50/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 178.8155 - mae: 8.8779 - val_loss: 132.6035 - val_mae: 6.0991\n",
      "Epoch 51/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 183.6824 - mae: 8.8260 - val_loss: 131.6688 - val_mae: 6.0924\n",
      "Epoch 52/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 177.5792 - mae: 8.7820 - val_loss: 138.0567 - val_mae: 6.2944\n",
      "Epoch 53/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 177.2052 - mae: 8.7373 - val_loss: 134.5465 - val_mae: 6.2548\n",
      "Epoch 54/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 178.5948 - mae: 8.6519 - val_loss: 129.1214 - val_mae: 5.9645\n",
      "Epoch 55/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 172.9560 - mae: 8.4456 - val_loss: 131.7003 - val_mae: 5.9368\n",
      "Epoch 56/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 158.5101 - mae: 8.2977 - val_loss: 129.3568 - val_mae: 5.8265\n",
      "Epoch 57/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 171.3545 - mae: 8.4395 - val_loss: 130.2912 - val_mae: 6.1092\n",
      "Epoch 58/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 161.2508 - mae: 8.3049 - val_loss: 131.1552 - val_mae: 5.9552\n",
      "Epoch 59/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 162.1197 - mae: 8.2421 - val_loss: 128.4330 - val_mae: 5.9082\n",
      "Epoch 60/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 166.9536 - mae: 8.3186 - val_loss: 132.9288 - val_mae: 6.0531\n",
      "Epoch 61/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 161.7174 - mae: 8.1812 - val_loss: 130.4177 - val_mae: 5.7777\n",
      "Epoch 62/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 152.6086 - mae: 7.9923 - val_loss: 124.7668 - val_mae: 5.7417\n",
      "Epoch 63/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 163.1487 - mae: 8.1239 - val_loss: 126.0721 - val_mae: 5.6930\n",
      "Epoch 64/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 155.9569 - mae: 8.0615 - val_loss: 128.4493 - val_mae: 6.1691\n",
      "Epoch 65/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 159.4396 - mae: 8.0899 - val_loss: 125.6625 - val_mae: 5.5340\n",
      "Epoch 66/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 158.8749 - mae: 7.9238 - val_loss: 125.3307 - val_mae: 5.5958\n",
      "Epoch 67/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 144.8865 - mae: 7.8053 - val_loss: 120.7415 - val_mae: 5.4794\n",
      "Epoch 68/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 152.5911 - mae: 7.9005 - val_loss: 125.7641 - val_mae: 5.4125\n",
      "Epoch 69/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 157.3744 - mae: 7.7548 - val_loss: 118.6871 - val_mae: 5.5630\n",
      "Epoch 70/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 145.6491 - mae: 7.6205 - val_loss: 124.5821 - val_mae: 5.6241\n",
      "Epoch 71/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 162.4462 - mae: 7.9512 - val_loss: 122.6036 - val_mae: 5.5637\n",
      "Epoch 72/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 154.3901 - mae: 7.7950 - val_loss: 119.0631 - val_mae: 5.2720\n",
      "Epoch 73/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 140.1345 - mae: 7.4950 - val_loss: 119.7244 - val_mae: 5.1200\n",
      "Epoch 74/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 136.5508 - mae: 7.4732 - val_loss: 131.1143 - val_mae: 6.1227\n",
      "Epoch 75/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 146.5238 - mae: 7.5996 - val_loss: 118.1980 - val_mae: 5.0801\n",
      "Epoch 76/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 147.8485 - mae: 7.5310 - val_loss: 117.5726 - val_mae: 5.0782\n",
      "Epoch 77/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 148.5238 - mae: 7.5633 - val_loss: 121.1935 - val_mae: 5.5132\n",
      "Epoch 78/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 156.7954 - mae: 7.5448 - val_loss: 118.2378 - val_mae: 5.2335\n",
      "Epoch 79/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 990us/step - loss: 145.6772 - mae: 7.5096 - val_loss: 115.8192 - val_mae: 5.0924\n",
      "Epoch 80/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 145.2226 - mae: 7.4016 - val_loss: 119.4069 - val_mae: 5.1697\n",
      "Epoch 81/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 143.8061 - mae: 7.3631 - val_loss: 118.7083 - val_mae: 5.2693\n",
      "Epoch 82/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 138.8300 - mae: 7.3113 - val_loss: 114.8958 - val_mae: 5.1356\n",
      "Epoch 83/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 150.0390 - mae: 7.3731 - val_loss: 114.8647 - val_mae: 5.2063\n",
      "Epoch 84/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 146.5413 - mae: 7.3981 - val_loss: 116.5488 - val_mae: 5.3062\n",
      "Epoch 85/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 152.7376 - mae: 7.3689 - val_loss: 117.6248 - val_mae: 5.0805\n",
      "Epoch 86/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 136.2876 - mae: 7.1792 - val_loss: 115.7927 - val_mae: 5.1433\n",
      "Epoch 87/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 132.7525 - mae: 7.1070 - val_loss: 115.9237 - val_mae: 5.3789\n",
      "Epoch 88/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 147.5528 - mae: 7.3187 - val_loss: 114.8159 - val_mae: 5.1001\n",
      "Epoch 89/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 134.0237 - mae: 7.1884 - val_loss: 115.5098 - val_mae: 5.1804\n",
      "Epoch 90/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 139.6401 - mae: 7.1960 - val_loss: 115.2792 - val_mae: 5.2026\n",
      "Epoch 91/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 135.0994 - mae: 7.0996 - val_loss: 114.5622 - val_mae: 4.8705\n",
      "Epoch 92/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 137.2301 - mae: 7.0351 - val_loss: 116.3984 - val_mae: 4.9746\n",
      "Epoch 93/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 150.2178 - mae: 7.1410 - val_loss: 114.0166 - val_mae: 4.8754\n",
      "Epoch 94/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 135.1505 - mae: 7.1054 - val_loss: 112.4174 - val_mae: 4.9332\n",
      "Epoch 95/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 137.3969 - mae: 7.0092 - val_loss: 111.3984 - val_mae: 4.9016\n",
      "Epoch 96/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 129.2640 - mae: 6.9525 - val_loss: 112.7746 - val_mae: 5.0155\n",
      "Epoch 97/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 130.4402 - mae: 6.8803 - val_loss: 111.3134 - val_mae: 4.8769\n",
      "Epoch 98/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 128.2798 - mae: 6.8912 - val_loss: 111.4521 - val_mae: 4.7767\n",
      "Epoch 99/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 125.7152 - mae: 6.9720 - val_loss: 124.2388 - val_mae: 5.6305\n",
      "Epoch 100/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 140.9341 - mae: 6.9446 - val_loss: 120.9976 - val_mae: 5.7676\n",
      "Epoch 101/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 122.9280 - mae: 6.8081 - val_loss: 115.7796 - val_mae: 4.8261\n",
      "Epoch 102/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 128.8368 - mae: 6.7882 - val_loss: 110.6156 - val_mae: 4.8168\n",
      "Epoch 103/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 122.5633 - mae: 6.7295 - val_loss: 110.3510 - val_mae: 4.8157\n",
      "Epoch 104/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 125.0270 - mae: 6.7132 - val_loss: 118.1148 - val_mae: 5.6017\n",
      "Epoch 105/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 140.9070 - mae: 6.8170 - val_loss: 111.3016 - val_mae: 4.7292\n",
      "Epoch 106/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 124.0633 - mae: 6.8130 - val_loss: 113.7515 - val_mae: 5.2166\n",
      "Epoch 107/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 123.2974 - mae: 6.6442 - val_loss: 110.3391 - val_mae: 4.7936\n",
      "Epoch 108/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 126.3629 - mae: 6.8319 - val_loss: 110.8411 - val_mae: 4.6868\n",
      "Epoch 109/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 120.9198 - mae: 6.6291 - val_loss: 113.0162 - val_mae: 4.8692\n",
      "Epoch 110/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 119.4308 - mae: 6.5324 - val_loss: 110.6130 - val_mae: 4.5756\n",
      "Epoch 111/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 119.9524 - mae: 6.5324 - val_loss: 110.7727 - val_mae: 4.6355\n",
      "Epoch 112/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 122.6723 - mae: 6.5765 - val_loss: 110.2794 - val_mae: 4.5733\n",
      "Epoch 113/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 120.2127 - mae: 6.5583 - val_loss: 110.4858 - val_mae: 4.9697\n",
      "Epoch 114/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 124.3838 - mae: 6.5872 - val_loss: 108.2394 - val_mae: 4.5726\n",
      "Epoch 115/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 120.8861 - mae: 6.4874 - val_loss: 107.4822 - val_mae: 4.7150\n",
      "Epoch 116/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 122.7921 - mae: 6.4963 - val_loss: 109.1855 - val_mae: 4.6749\n",
      "Epoch 117/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 131.4765 - mae: 6.5722 - val_loss: 109.7257 - val_mae: 4.8564\n",
      "Epoch 118/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 115.2041 - mae: 6.3991 - val_loss: 109.9897 - val_mae: 4.8303\n",
      "Epoch 119/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 116.9194 - mae: 6.4348 - val_loss: 106.7661 - val_mae: 4.5554\n",
      "Epoch 120/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 122.4435 - mae: 6.5041 - val_loss: 120.7729 - val_mae: 5.7022\n",
      "Epoch 121/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 133.6788 - mae: 6.6452 - val_loss: 106.8197 - val_mae: 4.5342\n",
      "Epoch 122/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 993us/step - loss: 116.4113 - mae: 6.4238 - val_loss: 107.9026 - val_mae: 4.5512\n",
      "Epoch 123/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 111.1474 - mae: 6.2811 - val_loss: 107.5739 - val_mae: 4.5667\n",
      "Epoch 124/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 115.7683 - mae: 6.2586 - val_loss: 106.5826 - val_mae: 4.4968\n",
      "Epoch 125/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 114.8890 - mae: 6.2852 - val_loss: 107.4447 - val_mae: 4.5668\n",
      "Epoch 126/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 130.4560 - mae: 6.4198 - val_loss: 112.7910 - val_mae: 4.5790\n",
      "Epoch 127/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 122.3301 - mae: 6.3027 - val_loss: 110.1318 - val_mae: 4.9679\n",
      "Epoch 128/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 114.3654 - mae: 6.2668 - val_loss: 110.9318 - val_mae: 4.9734\n",
      "Epoch 129/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 113.5705 - mae: 6.3172 - val_loss: 115.1653 - val_mae: 5.1576\n",
      "Epoch 130/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 112.1936 - mae: 6.2545 - val_loss: 114.9768 - val_mae: 5.2189\n",
      "Epoch 131/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 110.6686 - mae: 6.1483 - val_loss: 110.7768 - val_mae: 4.9982\n",
      "Epoch 132/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 104.4116 - mae: 6.1942 - val_loss: 110.2157 - val_mae: 4.7745\n",
      "Epoch 133/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 109.9710 - mae: 6.1215 - val_loss: 118.8864 - val_mae: 5.3301\n",
      "Epoch 134/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 109.5421 - mae: 6.0809 - val_loss: 110.8543 - val_mae: 4.5618\n",
      "Epoch 135/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 118.1853 - mae: 6.2323 - val_loss: 111.1043 - val_mae: 4.9039\n",
      "Epoch 136/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 97.0830 - mae: 5.9333 - val_loss: 111.8342 - val_mae: 5.0786\n",
      "Epoch 137/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 102.8643 - mae: 5.9855 - val_loss: 114.6140 - val_mae: 5.0895\n",
      "Epoch 138/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 104.9913 - mae: 5.9906 - val_loss: 138.5899 - val_mae: 6.3023\n",
      "Epoch 139/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 114.7407 - mae: 6.0937 - val_loss: 114.1514 - val_mae: 4.9166\n",
      "Epoch 140/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 102.3940 - mae: 5.9091 - val_loss: 114.5798 - val_mae: 5.0890\n",
      "Epoch 141/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 112.3118 - mae: 5.9693 - val_loss: 124.7371 - val_mae: 5.7113\n",
      "Epoch 142/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 104.0752 - mae: 5.9450 - val_loss: 125.7013 - val_mae: 5.5259\n",
      "Epoch 143/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 101.5065 - mae: 5.9334 - val_loss: 123.6450 - val_mae: 5.4209\n",
      "Epoch 144/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 104.6843 - mae: 5.8523 - val_loss: 120.2369 - val_mae: 5.2038\n",
      "Epoch 145/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 106.2364 - mae: 5.9383 - val_loss: 119.8279 - val_mae: 5.3973\n",
      "Epoch 146/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 990us/step - loss: 103.1959 - mae: 5.8474 - val_loss: 127.8185 - val_mae: 5.6017\n",
      "Epoch 147/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 114.1590 - mae: 5.9220 - val_loss: 120.2112 - val_mae: 5.2717\n",
      "Epoch 148/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 104.4148 - mae: 5.8854 - val_loss: 136.2052 - val_mae: 6.0287\n",
      "Epoch 149/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 108.1873 - mae: 5.8022 - val_loss: 126.2137 - val_mae: 5.3927\n",
      "Epoch 150/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 90.5180 - mae: 5.6851 - val_loss: 117.2676 - val_mae: 4.9770\n",
      "Epoch 151/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 103.3325 - mae: 5.7673 - val_loss: 148.7658 - val_mae: 6.5348\n",
      "Epoch 152/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 105.2751 - mae: 5.7049 - val_loss: 128.0954 - val_mae: 5.6261\n",
      "Epoch 153/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 101.8433 - mae: 5.8240 - val_loss: 142.1124 - val_mae: 6.1047\n",
      "Epoch 154/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 998us/step - loss: 97.7413 - mae: 5.7037 - val_loss: 132.5189 - val_mae: 5.8327\n",
      "Epoch 155/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 102.2370 - mae: 5.6934 - val_loss: 131.3664 - val_mae: 5.6394\n",
      "Epoch 156/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 91.9614 - mae: 5.6168 - val_loss: 177.2337 - val_mae: 7.3463\n",
      "Epoch 157/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 101.5227 - mae: 5.6742 - val_loss: 132.5397 - val_mae: 5.8829\n",
      "Epoch 158/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 85.9022 - mae: 5.4536 - val_loss: 141.4086 - val_mae: 6.1861\n",
      "Epoch 159/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 95.8116 - mae: 5.5656 - val_loss: 137.2904 - val_mae: 5.8329\n",
      "Epoch 160/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 93.0352 - mae: 5.5198 - val_loss: 148.2180 - val_mae: 6.3074\n",
      "Epoch 161/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 108.3375 - mae: 5.6544 - val_loss: 148.9316 - val_mae: 6.3066\n",
      "Epoch 162/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 94.2321 - mae: 5.5446 - val_loss: 136.8138 - val_mae: 5.7836\n",
      "Epoch 163/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 95.8631 - mae: 5.5096 - val_loss: 146.9583 - val_mae: 6.3075\n",
      "Epoch 164/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 91.7252 - mae: 5.5000 - val_loss: 158.3540 - val_mae: 6.4285\n",
      "Epoch 165/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 86.5308 - mae: 5.4249 - val_loss: 130.3927 - val_mae: 5.6520\n",
      "Epoch 166/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 102.5746 - mae: 5.4096 - val_loss: 139.5683 - val_mae: 6.1235\n",
      "Epoch 167/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 88.5462 - mae: 5.4031 - val_loss: 160.1983 - val_mae: 6.5711\n",
      "Epoch 168/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 86.8633 - mae: 5.3569 - val_loss: 159.9530 - val_mae: 6.6675\n",
      "Epoch 169/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 96.1252 - mae: 5.3843 - val_loss: 167.2706 - val_mae: 6.9114\n",
      "Epoch 170/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 83.2097 - mae: 5.2796 - val_loss: 160.9365 - val_mae: 6.6575\n",
      "Epoch 171/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 100.3285 - mae: 5.4895 - val_loss: 151.8984 - val_mae: 6.3617\n",
      "Epoch 172/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 86.3710 - mae: 5.2881 - val_loss: 164.9207 - val_mae: 6.8698\n",
      "Epoch 173/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 92.4792 - mae: 5.3860 - val_loss: 134.0300 - val_mae: 5.8503\n",
      "Epoch 174/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 83.3080 - mae: 5.1934 - val_loss: 192.1696 - val_mae: 7.5013\n",
      "Epoch 175/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 89.3459 - mae: 5.3432 - val_loss: 174.7364 - val_mae: 6.8735\n",
      "Epoch 176/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 86.0908 - mae: 5.2887 - val_loss: 147.9100 - val_mae: 6.4291\n",
      "Epoch 177/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 82.9980 - mae: 5.2505 - val_loss: 175.7370 - val_mae: 7.0600\n",
      "Epoch 178/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 91.0983 - mae: 5.2562 - val_loss: 152.5773 - val_mae: 6.4269\n",
      "Epoch 179/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 93.1640 - mae: 5.3422 - val_loss: 177.0819 - val_mae: 7.0804\n",
      "Epoch 180/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 89.1920 - mae: 5.2457 - val_loss: 163.1057 - val_mae: 6.6914\n",
      "Epoch 181/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 993us/step - loss: 94.5244 - mae: 5.2932 - val_loss: 164.8707 - val_mae: 6.8480\n",
      "Epoch 182/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 86.2155 - mae: 5.0920 - val_loss: 154.4617 - val_mae: 6.4273\n",
      "Epoch 183/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 94.4114 - mae: 5.2645 - val_loss: 178.9598 - val_mae: 7.0367\n",
      "Epoch 184/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 87.8433 - mae: 5.2048 - val_loss: 171.6613 - val_mae: 7.0092\n",
      "Epoch 185/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 989us/step - loss: 87.4571 - mae: 5.1758 - val_loss: 171.1259 - val_mae: 6.9005\n",
      "Epoch 186/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 82.7270 - mae: 5.0444 - val_loss: 177.6568 - val_mae: 6.8846\n",
      "Epoch 187/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 78.3132 - mae: 5.0101 - val_loss: 186.8656 - val_mae: 7.2316\n",
      "Epoch 188/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 87.7437 - mae: 5.1963 - val_loss: 193.3595 - val_mae: 7.3806\n",
      "Epoch 189/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 87.7160 - mae: 4.9857 - val_loss: 203.7119 - val_mae: 7.7999\n",
      "Epoch 190/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 85.6376 - mae: 5.0400 - val_loss: 159.5020 - val_mae: 6.6288\n",
      "Epoch 191/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 81.2027 - mae: 5.0445 - val_loss: 207.7507 - val_mae: 7.6398\n",
      "Epoch 192/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 81.8139 - mae: 5.0249 - val_loss: 160.6853 - val_mae: 6.5036\n",
      "Epoch 193/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 84.3769 - mae: 5.0512 - val_loss: 204.9091 - val_mae: 7.5691\n",
      "Epoch 194/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 83.8281 - mae: 4.9921 - val_loss: 198.7947 - val_mae: 7.4045\n",
      "Epoch 195/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 87.3731 - mae: 5.0404 - val_loss: 185.7427 - val_mae: 7.2778\n",
      "Epoch 196/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 75.2954 - mae: 4.9227 - val_loss: 194.3027 - val_mae: 7.5234\n",
      "Epoch 197/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 84.8234 - mae: 5.0565 - val_loss: 173.1421 - val_mae: 7.0453\n",
      "Epoch 198/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 78.4936 - mae: 4.9722 - val_loss: 216.5468 - val_mae: 7.8190\n",
      "Epoch 199/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 73.3684 - mae: 4.9040 - val_loss: 189.4921 - val_mae: 7.4576\n",
      "Epoch 200/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 72.3386 - mae: 4.8323 - val_loss: 157.7911 - val_mae: 6.4418\n",
      "Epoch 201/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 74.3798 - mae: 4.8617 - val_loss: 186.6567 - val_mae: 7.2779\n",
      "Epoch 202/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 89.8945 - mae: 5.0681 - val_loss: 193.1655 - val_mae: 7.4950\n",
      "Epoch 203/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 89.4307 - mae: 5.0053 - val_loss: 183.8622 - val_mae: 7.1751\n",
      "Epoch 204/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 77.8302 - mae: 4.8518 - val_loss: 182.8735 - val_mae: 7.0125\n",
      "Epoch 205/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 993us/step - loss: 75.3733 - mae: 4.8559 - val_loss: 171.5791 - val_mae: 7.0103\n",
      "Epoch 206/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 75.8267 - mae: 4.8603 - val_loss: 192.3802 - val_mae: 7.4616\n",
      "Epoch 207/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 76.7398 - mae: 4.8679 - val_loss: 215.3270 - val_mae: 8.1269\n",
      "Epoch 208/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 84.3383 - mae: 4.9682 - val_loss: 166.6472 - val_mae: 6.8029\n",
      "Epoch 209/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 88.7556 - mae: 4.9692 - val_loss: 149.7414 - val_mae: 6.2547\n",
      "Epoch 210/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 73.0157 - mae: 4.7718 - val_loss: 196.5342 - val_mae: 7.5410\n",
      "Epoch 211/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 83.1067 - mae: 4.9008 - val_loss: 197.6251 - val_mae: 7.3506\n",
      "Epoch 212/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 79.3764 - mae: 4.8146 - val_loss: 203.4001 - val_mae: 7.5490\n",
      "Epoch 213/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 76.9985 - mae: 4.7751 - val_loss: 184.4755 - val_mae: 7.2408\n",
      "Epoch 214/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 81.3197 - mae: 4.8499 - val_loss: 193.4607 - val_mae: 7.4921\n",
      "Epoch 215/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 79.9753 - mae: 4.8083 - val_loss: 180.4930 - val_mae: 7.1070\n",
      "Epoch 216/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 80.6906 - mae: 4.8214 - val_loss: 210.8522 - val_mae: 7.6913\n",
      "Epoch 217/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 76.7699 - mae: 4.7660 - val_loss: 188.7120 - val_mae: 7.3264\n",
      "Epoch 218/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 73.7804 - mae: 4.7310 - val_loss: 214.6841 - val_mae: 7.9877\n",
      "Epoch 219/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 79.2796 - mae: 4.8406 - val_loss: 175.4418 - val_mae: 7.0187\n",
      "Epoch 220/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 72.0975 - mae: 4.7699 - val_loss: 183.3350 - val_mae: 7.0971\n",
      "Epoch 221/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 77.0235 - mae: 4.7655 - val_loss: 215.7032 - val_mae: 7.9812\n",
      "Epoch 222/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 71.7468 - mae: 4.6723 - val_loss: 203.8950 - val_mae: 7.7923\n",
      "Epoch 223/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 75.7549 - mae: 4.8078 - val_loss: 236.3586 - val_mae: 8.3158\n",
      "Epoch 224/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 990us/step - loss: 79.1519 - mae: 4.8862 - val_loss: 168.1318 - val_mae: 7.0038\n",
      "Epoch 225/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 71.7443 - mae: 4.6780 - val_loss: 198.6477 - val_mae: 7.4908\n",
      "Epoch 226/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 71.8776 - mae: 4.6374 - val_loss: 165.7261 - val_mae: 6.6620\n",
      "Epoch 227/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 64.7718 - mae: 4.5112 - val_loss: 185.4133 - val_mae: 7.0718\n",
      "Epoch 228/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 73.4586 - mae: 4.8111 - val_loss: 173.4597 - val_mae: 7.0102\n",
      "Epoch 229/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 72.6144 - mae: 4.6503 - val_loss: 186.4760 - val_mae: 7.3986\n",
      "Epoch 230/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 70.8241 - mae: 4.6704 - val_loss: 188.3686 - val_mae: 7.2977\n",
      "Epoch 231/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 82.0722 - mae: 4.7753 - val_loss: 197.1592 - val_mae: 7.5539\n",
      "Epoch 232/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 74.4685 - mae: 4.6861 - val_loss: 177.3515 - val_mae: 6.9780\n",
      "Epoch 233/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 74.7533 - mae: 4.7184 - val_loss: 185.0122 - val_mae: 7.2903\n",
      "Epoch 234/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 81.4041 - mae: 4.7155 - val_loss: 175.0515 - val_mae: 7.0583\n",
      "Epoch 235/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 81.2294 - mae: 4.7155 - val_loss: 187.4098 - val_mae: 7.3628\n",
      "Epoch 236/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 77.4485 - mae: 4.6528 - val_loss: 195.9039 - val_mae: 7.4717\n",
      "Epoch 237/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 81.4801 - mae: 4.7434 - val_loss: 206.5609 - val_mae: 7.6533\n",
      "Epoch 238/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 86.7254 - mae: 4.8293 - val_loss: 185.0540 - val_mae: 7.1459\n",
      "Epoch 239/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 73.4762 - mae: 4.5661 - val_loss: 214.0976 - val_mae: 7.7830\n",
      "Epoch 240/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 73.2841 - mae: 4.6059 - val_loss: 181.7556 - val_mae: 6.9941\n",
      "Epoch 241/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 68.7488 - mae: 4.5658 - val_loss: 196.2397 - val_mae: 7.5925\n",
      "Epoch 242/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 81.0849 - mae: 4.6996 - val_loss: 164.9105 - val_mae: 6.6880\n",
      "Epoch 243/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 71.9459 - mae: 4.6314 - val_loss: 163.4345 - val_mae: 6.5436\n",
      "Epoch 244/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 69.2313 - mae: 4.5077 - val_loss: 168.3834 - val_mae: 6.7747\n",
      "Epoch 245/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 68.6028 - mae: 4.5430 - val_loss: 191.8846 - val_mae: 7.1657\n",
      "Epoch 246/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 75.7457 - mae: 4.6669 - val_loss: 197.2334 - val_mae: 7.3349\n",
      "Epoch 247/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 76.0094 - mae: 4.5255 - val_loss: 224.7753 - val_mae: 8.0433\n",
      "Epoch 248/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 76.7115 - mae: 4.6021 - val_loss: 219.9601 - val_mae: 7.8516\n",
      "Epoch 249/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 74.1301 - mae: 4.5649 - val_loss: 172.3871 - val_mae: 6.7630\n",
      "Epoch 250/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 68.2367 - mae: 4.5125 - val_loss: 210.3744 - val_mae: 7.7939\n",
      "Epoch 251/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 76.8271 - mae: 4.6049 - val_loss: 177.8060 - val_mae: 7.0388\n",
      "Epoch 252/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 72.6659 - mae: 4.6051 - val_loss: 217.9718 - val_mae: 8.0272\n",
      "Epoch 253/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 74.1322 - mae: 4.5260 - val_loss: 183.7726 - val_mae: 7.1494\n",
      "Epoch 254/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 69.9329 - mae: 4.4409 - val_loss: 210.9548 - val_mae: 7.8472\n",
      "Epoch 255/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 68.2194 - mae: 4.4470 - val_loss: 243.1694 - val_mae: 8.2091\n",
      "Epoch 256/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 72.5926 - mae: 4.5763 - val_loss: 208.4207 - val_mae: 7.7845\n",
      "Epoch 257/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 995us/step - loss: 72.6509 - mae: 4.5291 - val_loss: 187.6637 - val_mae: 7.2267\n",
      "Epoch 258/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 79.1100 - mae: 4.5272 - val_loss: 202.9983 - val_mae: 7.4937\n",
      "Epoch 259/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 71.6598 - mae: 4.5294 - val_loss: 195.0473 - val_mae: 7.3280\n",
      "Epoch 260/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 69.8443 - mae: 4.5038 - val_loss: 239.5539 - val_mae: 8.2091\n",
      "Epoch 261/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 84.8871 - mae: 4.5831 - val_loss: 193.5242 - val_mae: 7.3528\n",
      "Epoch 262/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 68.2367 - mae: 4.4334 - val_loss: 170.3286 - val_mae: 6.7349\n",
      "Epoch 263/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 68.8444 - mae: 4.4779 - val_loss: 174.9566 - val_mae: 6.8981\n",
      "Epoch 264/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 69.3610 - mae: 4.4725 - val_loss: 232.7449 - val_mae: 7.9591\n",
      "Epoch 265/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 65.9864 - mae: 4.4287 - val_loss: 202.6710 - val_mae: 7.6346\n",
      "Epoch 266/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 68.3025 - mae: 4.4582 - val_loss: 205.7008 - val_mae: 7.6551\n",
      "Epoch 267/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 66.0641 - mae: 4.4072 - val_loss: 187.7689 - val_mae: 7.2737\n",
      "Epoch 268/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 68.0318 - mae: 4.5022 - val_loss: 219.7295 - val_mae: 7.8104\n",
      "Epoch 269/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 65.9700 - mae: 4.3171 - val_loss: 183.9246 - val_mae: 7.1110\n",
      "Epoch 270/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 76.9033 - mae: 4.5610 - val_loss: 153.4436 - val_mae: 6.2201\n",
      "Epoch 271/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 66.5288 - mae: 4.3810 - val_loss: 210.5014 - val_mae: 7.6576\n",
      "Epoch 272/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 64.6282 - mae: 4.3884 - val_loss: 205.0655 - val_mae: 7.6672\n",
      "Epoch 273/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 74.4467 - mae: 4.4399 - val_loss: 189.0762 - val_mae: 7.2664\n",
      "Epoch 274/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 75.9078 - mae: 4.4863 - val_loss: 186.3405 - val_mae: 7.2161\n",
      "Epoch 275/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 71.3532 - mae: 4.4278 - val_loss: 193.5361 - val_mae: 7.2175\n",
      "Epoch 276/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 70.6344 - mae: 4.4330 - val_loss: 178.5636 - val_mae: 7.0624\n",
      "Epoch 277/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 64.4085 - mae: 4.3906 - val_loss: 238.2726 - val_mae: 8.2460\n",
      "Epoch 278/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 73.5509 - mae: 4.4860 - val_loss: 228.7706 - val_mae: 8.1708\n",
      "Epoch 279/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 74.3433 - mae: 4.4910 - val_loss: 206.1562 - val_mae: 7.6380\n",
      "Epoch 280/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 66.1167 - mae: 4.3404 - val_loss: 191.3629 - val_mae: 7.0743\n",
      "Epoch 281/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 77.3013 - mae: 4.5003 - val_loss: 218.4316 - val_mae: 7.7643\n",
      "Epoch 282/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 68.5301 - mae: 4.4239 - val_loss: 168.3327 - val_mae: 6.8448\n",
      "Epoch 283/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 65.5804 - mae: 4.4358 - val_loss: 212.8045 - val_mae: 7.9225\n",
      "Epoch 284/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 71.5644 - mae: 4.4375 - val_loss: 145.3496 - val_mae: 6.1277\n",
      "Epoch 285/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 75.4892 - mae: 4.4705 - val_loss: 185.7418 - val_mae: 7.2125\n",
      "Epoch 286/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 997us/step - loss: 72.1338 - mae: 4.4992 - val_loss: 214.7878 - val_mae: 7.7637\n",
      "Epoch 287/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 66.6969 - mae: 4.2502 - val_loss: 170.6884 - val_mae: 6.7718\n",
      "Epoch 288/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 994us/step - loss: 62.9151 - mae: 4.3552 - val_loss: 209.6421 - val_mae: 7.8666\n",
      "Epoch 289/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 64.3996 - mae: 4.3746 - val_loss: 170.0630 - val_mae: 6.7881\n",
      "Epoch 290/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 991us/step - loss: 68.8185 - mae: 4.3842 - val_loss: 230.1684 - val_mae: 8.1440\n",
      "Epoch 291/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 69.1682 - mae: 4.3418 - val_loss: 183.7318 - val_mae: 7.1550\n",
      "Epoch 292/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 71.6263 - mae: 4.4126 - val_loss: 217.7697 - val_mae: 7.8933\n",
      "Epoch 293/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 63.7177 - mae: 4.2541 - val_loss: 205.5918 - val_mae: 7.6445\n",
      "Epoch 294/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 990us/step - loss: 68.7750 - mae: 4.3459 - val_loss: 173.6596 - val_mae: 6.8087\n",
      "Epoch 295/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 998us/step - loss: 72.8387 - mae: 4.4797 - val_loss: 185.0178 - val_mae: 7.0087\n",
      "Epoch 296/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 72.1336 - mae: 4.4091 - val_loss: 227.8525 - val_mae: 8.1464\n",
      "Epoch 297/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 63.5422 - mae: 4.3276 - val_loss: 211.3341 - val_mae: 7.6747\n",
      "Epoch 298/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 75.4571 - mae: 4.4828 - val_loss: 232.5329 - val_mae: 8.1266\n",
      "Epoch 299/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 72.4211 - mae: 4.4240 - val_loss: 200.1081 - val_mae: 7.3151\n",
      "Epoch 300/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 73.7755 - mae: 4.3523 - val_loss: 214.3074 - val_mae: 7.7921\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 791us/step\n",
      "🔹 LSTM Model - RMSE: 14.64\n",
      "🔹 LSTM Model - R² Score: 0.98\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_clean)\n",
    "X_test_scaled = scaler.transform(X_test_df)\n",
    "\n",
    "X_train_scaled = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\n",
    "X_test_scaled = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(128, activation='relu', input_shape=(X_train_scaled.shape[1], X_train_scaled.shape[2]), return_sequences=False),\n",
    "    Dropout(0.2),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "optimizer = Adam(learning_rate=0.0005)\n",
    "model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "\n",
    "history = model.fit(X_train_scaled, y_train_clean, validation_data=(X_test_scaled, y_test), epochs=300, batch_size=32)\n",
    "\n",
    "y_pred_lstm = model.predict(X_test_scaled)\n",
    "rmse_lstm = np.sqrt(mean_squared_error(y_test, y_pred_lstm))\n",
    "r2_lstm = r2_score(y_test, y_pred_lstm)\n",
    "\n",
    "print(f\"🔹 LSTM Model - RMSE: {rmse_lstm:.2f}\")\n",
    "print(f\"🔹 LSTM Model - R² Score: {r2_lstm:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/me.pranesh/Desktop/new clean/.venv/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 35605.6367 - mae: 154.9352 - val_loss: 30100.5449 - val_mae: 143.8360\n",
      "Epoch 2/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 24562.7090 - mae: 130.0021 - val_loss: 8518.3418 - val_mae: 73.7259\n",
      "Epoch 3/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 7202.0942 - mae: 63.4710 - val_loss: 3079.8918 - val_mae: 40.8121\n",
      "Epoch 4/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 3223.3958 - mae: 40.8034 - val_loss: 1964.9391 - val_mae: 32.2525\n",
      "Epoch 5/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 2226.3303 - mae: 33.8223 - val_loss: 1540.9437 - val_mae: 28.0181\n",
      "Epoch 6/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 1953.4335 - mae: 31.6678 - val_loss: 1304.1010 - val_mae: 25.3268\n",
      "Epoch 7/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 1672.0916 - mae: 29.1193 - val_loss: 1103.7465 - val_mae: 22.9739\n",
      "Epoch 8/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 1576.5612 - mae: 27.7763 - val_loss: 970.4927 - val_mae: 21.0310\n",
      "Epoch 9/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 1424.9729 - mae: 26.2751 - val_loss: 850.0167 - val_mae: 19.5282\n",
      "Epoch 10/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 1336.2715 - mae: 25.5285 - val_loss: 766.2093 - val_mae: 18.3949\n",
      "Epoch 11/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 1241.0973 - mae: 24.6360 - val_loss: 685.0834 - val_mae: 17.3074\n",
      "Epoch 12/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 1164.1272 - mae: 23.6899 - val_loss: 621.1241 - val_mae: 16.5097\n",
      "Epoch 13/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 1111.9371 - mae: 23.3150 - val_loss: 573.3987 - val_mae: 15.7887\n",
      "Epoch 14/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 1099.8328 - mae: 23.0937 - val_loss: 543.0513 - val_mae: 15.2403\n",
      "Epoch 15/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 1024.4219 - mae: 22.1862 - val_loss: 505.6805 - val_mae: 14.6779\n",
      "Epoch 16/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 1012.1251 - mae: 22.2899 - val_loss: 476.9814 - val_mae: 14.2980\n",
      "Epoch 17/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 999.1474 - mae: 21.9847 - val_loss: 456.3253 - val_mae: 13.9603\n",
      "Epoch 18/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 981.1104 - mae: 21.8743 - val_loss: 437.4893 - val_mae: 13.7592\n",
      "Epoch 19/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 947.0034 - mae: 21.4956 - val_loss: 419.1065 - val_mae: 13.3873\n",
      "Epoch 20/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 863.1583 - mae: 20.9000 - val_loss: 412.3636 - val_mae: 13.2179\n",
      "Epoch 21/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 870.1515 - mae: 20.4774 - val_loss: 397.0350 - val_mae: 12.9734\n",
      "Epoch 22/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 876.9233 - mae: 20.7074 - val_loss: 389.6978 - val_mae: 12.8482\n",
      "Epoch 23/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 838.0259 - mae: 20.2941 - val_loss: 370.7051 - val_mae: 12.5362\n",
      "Epoch 24/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 848.3645 - mae: 20.3869 - val_loss: 362.8796 - val_mae: 12.4105\n",
      "Epoch 25/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 837.3922 - mae: 20.2706 - val_loss: 357.7207 - val_mae: 12.3270\n",
      "Epoch 26/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 820.2001 - mae: 20.0726 - val_loss: 366.8794 - val_mae: 12.3932\n",
      "Epoch 27/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 826.2977 - mae: 19.9423 - val_loss: 343.0093 - val_mae: 12.0308\n",
      "Epoch 28/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 813.8066 - mae: 19.8144 - val_loss: 341.7014 - val_mae: 11.9528\n",
      "Epoch 29/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 803.3109 - mae: 19.5742 - val_loss: 335.3849 - val_mae: 11.8607\n",
      "Epoch 30/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 767.4858 - mae: 19.4220 - val_loss: 337.8513 - val_mae: 11.9044\n",
      "Epoch 31/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 752.4733 - mae: 19.1696 - val_loss: 326.3586 - val_mae: 11.7759\n",
      "Epoch 32/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 764.0464 - mae: 19.4308 - val_loss: 323.4492 - val_mae: 11.6508\n",
      "Epoch 33/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 718.1316 - mae: 18.7749 - val_loss: 326.2545 - val_mae: 11.6644\n",
      "Epoch 34/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 730.4191 - mae: 18.9576 - val_loss: 321.2737 - val_mae: 11.6342\n",
      "Epoch 35/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 733.8481 - mae: 18.9424 - val_loss: 311.4860 - val_mae: 11.4308\n",
      "Epoch 36/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 718.8293 - mae: 18.8598 - val_loss: 307.8685 - val_mae: 11.3556\n",
      "Epoch 37/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 741.2410 - mae: 18.9598 - val_loss: 308.4312 - val_mae: 11.2519\n",
      "Epoch 38/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 696.4658 - mae: 18.4976 - val_loss: 301.5405 - val_mae: 11.1560\n",
      "Epoch 39/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 721.5398 - mae: 18.7240 - val_loss: 305.0750 - val_mae: 11.3006\n",
      "Epoch 40/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 694.2330 - mae: 18.3607 - val_loss: 298.2411 - val_mae: 11.1187\n",
      "Epoch 41/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 710.7433 - mae: 18.6876 - val_loss: 293.9197 - val_mae: 11.0113\n",
      "Epoch 42/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 668.3976 - mae: 18.1928 - val_loss: 295.8051 - val_mae: 11.0254\n",
      "Epoch 43/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 694.8209 - mae: 18.2290 - val_loss: 289.9464 - val_mae: 10.9105\n",
      "Epoch 44/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 667.2759 - mae: 18.0157 - val_loss: 293.8390 - val_mae: 11.0224\n",
      "Epoch 45/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 682.4400 - mae: 18.2235 - val_loss: 295.0744 - val_mae: 11.0335\n",
      "Epoch 46/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 675.2870 - mae: 18.1656 - val_loss: 289.1937 - val_mae: 10.9076\n",
      "Epoch 47/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 662.8092 - mae: 17.8577 - val_loss: 282.3387 - val_mae: 10.7166\n",
      "Epoch 48/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 657.2087 - mae: 17.7680 - val_loss: 274.9842 - val_mae: 10.6255\n",
      "Epoch 49/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 647.4645 - mae: 17.7965 - val_loss: 282.9304 - val_mae: 10.7657\n",
      "Epoch 50/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 680.7362 - mae: 17.9821 - val_loss: 274.7739 - val_mae: 10.5245\n",
      "Epoch 51/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 652.4422 - mae: 17.6790 - val_loss: 273.6700 - val_mae: 10.5342\n",
      "Epoch 52/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 654.4548 - mae: 17.7634 - val_loss: 272.3881 - val_mae: 10.4277\n",
      "Epoch 53/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 636.3199 - mae: 17.4501 - val_loss: 271.8626 - val_mae: 10.3782\n",
      "Epoch 54/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 637.2222 - mae: 17.3637 - val_loss: 269.6440 - val_mae: 10.3907\n",
      "Epoch 55/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 654.2332 - mae: 17.8467 - val_loss: 263.2816 - val_mae: 10.2243\n",
      "Epoch 56/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 672.5163 - mae: 17.9363 - val_loss: 258.6491 - val_mae: 10.1990\n",
      "Epoch 57/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 648.7466 - mae: 17.5672 - val_loss: 267.6773 - val_mae: 10.3582\n",
      "Epoch 58/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 622.5110 - mae: 17.3832 - val_loss: 266.6109 - val_mae: 10.2812\n",
      "Epoch 59/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 631.3192 - mae: 17.3994 - val_loss: 276.7462 - val_mae: 10.6350\n",
      "Epoch 60/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 637.1988 - mae: 17.4616 - val_loss: 268.4695 - val_mae: 10.4383\n",
      "Epoch 61/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 652.3751 - mae: 17.5125 - val_loss: 261.2087 - val_mae: 10.1386\n",
      "Epoch 62/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 640.8060 - mae: 17.4404 - val_loss: 264.8437 - val_mae: 10.2626\n",
      "Epoch 63/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 609.3929 - mae: 17.0971 - val_loss: 261.4807 - val_mae: 10.2261\n",
      "Epoch 64/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 612.2935 - mae: 17.0285 - val_loss: 256.6369 - val_mae: 10.0404\n",
      "Epoch 65/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 640.2341 - mae: 17.2206 - val_loss: 249.1730 - val_mae: 9.8274\n",
      "Epoch 66/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 632.9281 - mae: 17.1041 - val_loss: 248.0501 - val_mae: 9.7943\n",
      "Epoch 67/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 624.0897 - mae: 17.0269 - val_loss: 243.4872 - val_mae: 9.6586\n",
      "Epoch 68/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 607.5382 - mae: 16.9414 - val_loss: 250.2000 - val_mae: 9.9436\n",
      "Epoch 69/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 598.0035 - mae: 16.9495 - val_loss: 236.8812 - val_mae: 9.4877\n",
      "Epoch 70/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 601.4985 - mae: 16.7488 - val_loss: 242.5483 - val_mae: 9.6201\n",
      "Epoch 71/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 617.1840 - mae: 16.9452 - val_loss: 242.6035 - val_mae: 9.6272\n",
      "Epoch 72/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 591.9393 - mae: 16.6576 - val_loss: 236.1856 - val_mae: 9.5258\n",
      "Epoch 73/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 572.7379 - mae: 16.3714 - val_loss: 238.1726 - val_mae: 9.5281\n",
      "Epoch 74/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 600.6961 - mae: 16.7921 - val_loss: 238.5411 - val_mae: 9.5238\n",
      "Epoch 75/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 590.1205 - mae: 16.6098 - val_loss: 249.6871 - val_mae: 10.0314\n",
      "Epoch 76/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 574.2579 - mae: 16.5064 - val_loss: 226.2655 - val_mae: 9.1574\n",
      "Epoch 77/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 570.0447 - mae: 16.4596 - val_loss: 250.7964 - val_mae: 10.0622\n",
      "Epoch 78/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 592.5027 - mae: 16.5324 - val_loss: 228.7576 - val_mae: 9.2602\n",
      "Epoch 79/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 575.7693 - mae: 16.2762 - val_loss: 234.1471 - val_mae: 9.4076\n",
      "Epoch 80/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 558.6931 - mae: 16.2006 - val_loss: 231.5021 - val_mae: 9.4910\n",
      "Epoch 81/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 568.5664 - mae: 16.2115 - val_loss: 219.9741 - val_mae: 8.9998\n",
      "Epoch 82/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 561.2990 - mae: 16.1106 - val_loss: 240.4705 - val_mae: 9.8726\n",
      "Epoch 83/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 560.5289 - mae: 16.0939 - val_loss: 242.7181 - val_mae: 9.9145\n",
      "Epoch 84/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 571.8306 - mae: 16.0506 - val_loss: 210.0780 - val_mae: 8.6766\n",
      "Epoch 85/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 582.4393 - mae: 16.3595 - val_loss: 223.9737 - val_mae: 9.2871\n",
      "Epoch 86/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 544.7423 - mae: 15.9665 - val_loss: 236.2483 - val_mae: 9.6772\n",
      "Epoch 87/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 555.9142 - mae: 15.9783 - val_loss: 231.4543 - val_mae: 9.5797\n",
      "Epoch 88/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 556.0803 - mae: 16.1357 - val_loss: 205.0711 - val_mae: 8.5425\n",
      "Epoch 89/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 548.0031 - mae: 15.8816 - val_loss: 223.9938 - val_mae: 9.2960\n",
      "Epoch 90/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 564.6707 - mae: 16.0191 - val_loss: 227.9661 - val_mae: 9.4622\n",
      "Epoch 91/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 550.3540 - mae: 15.8704 - val_loss: 216.4733 - val_mae: 8.9398\n",
      "Epoch 92/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 566.3962 - mae: 15.9737 - val_loss: 223.1024 - val_mae: 9.3652\n",
      "Epoch 93/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 538.3704 - mae: 15.9027 - val_loss: 210.5739 - val_mae: 8.8716\n",
      "Epoch 94/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 554.5029 - mae: 15.8510 - val_loss: 220.3537 - val_mae: 9.1843\n",
      "Epoch 95/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 536.5081 - mae: 15.7187 - val_loss: 228.0204 - val_mae: 9.4963\n",
      "Epoch 96/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 558.2416 - mae: 15.9785 - val_loss: 223.1655 - val_mae: 9.4507\n",
      "Epoch 97/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 524.9377 - mae: 15.5789 - val_loss: 203.5260 - val_mae: 8.7485\n",
      "Epoch 98/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 552.7523 - mae: 15.8272 - val_loss: 222.7579 - val_mae: 9.4850\n",
      "Epoch 99/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 529.1514 - mae: 15.5092 - val_loss: 213.5905 - val_mae: 9.1315\n",
      "Epoch 100/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 555.1359 - mae: 15.7650 - val_loss: 213.7318 - val_mae: 9.2022\n",
      "Epoch 101/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 539.7585 - mae: 15.5607 - val_loss: 223.8869 - val_mae: 9.5692\n",
      "Epoch 102/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 547.0040 - mae: 15.6284 - val_loss: 225.8401 - val_mae: 9.6575\n",
      "Epoch 103/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 516.0160 - mae: 15.2965 - val_loss: 210.5512 - val_mae: 9.1426\n",
      "Epoch 104/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 496.2859 - mae: 14.9743 - val_loss: 206.8200 - val_mae: 8.9242\n",
      "Epoch 105/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 528.6577 - mae: 15.3574 - val_loss: 190.3127 - val_mae: 8.3176\n",
      "Epoch 106/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 502.3590 - mae: 15.0723 - val_loss: 219.9169 - val_mae: 9.3971\n",
      "Epoch 107/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 532.6486 - mae: 15.4548 - val_loss: 223.9957 - val_mae: 9.6447\n",
      "Epoch 108/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 509.2630 - mae: 15.1932 - val_loss: 211.5863 - val_mae: 9.1926\n",
      "Epoch 109/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 509.7146 - mae: 15.2440 - val_loss: 208.9989 - val_mae: 9.1793\n",
      "Epoch 110/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 485.9893 - mae: 14.8695 - val_loss: 179.2810 - val_mae: 7.9593\n",
      "Epoch 111/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 528.9793 - mae: 15.1412 - val_loss: 191.2551 - val_mae: 8.5058\n",
      "Epoch 112/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 513.3622 - mae: 15.1858 - val_loss: 194.4589 - val_mae: 8.6401\n",
      "Epoch 113/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 492.7936 - mae: 14.7877 - val_loss: 183.2543 - val_mae: 8.2368\n",
      "Epoch 114/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 511.3778 - mae: 15.1635 - val_loss: 205.5749 - val_mae: 9.2097\n",
      "Epoch 115/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 504.9567 - mae: 14.8080 - val_loss: 202.3184 - val_mae: 9.0338\n",
      "Epoch 116/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 487.8329 - mae: 14.7710 - val_loss: 206.5845 - val_mae: 9.1653\n",
      "Epoch 117/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 480.0107 - mae: 14.6426 - val_loss: 184.2235 - val_mae: 8.3425\n",
      "Epoch 118/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 510.3855 - mae: 15.0808 - val_loss: 191.8065 - val_mae: 8.6222\n",
      "Epoch 119/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 495.3828 - mae: 14.6896 - val_loss: 198.9144 - val_mae: 8.9694\n",
      "Epoch 120/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 499.9436 - mae: 14.7902 - val_loss: 198.7986 - val_mae: 8.9594\n",
      "Epoch 121/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 528.3958 - mae: 15.1031 - val_loss: 230.1521 - val_mae: 10.0362\n",
      "Epoch 122/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 488.0576 - mae: 14.6959 - val_loss: 186.7818 - val_mae: 8.4583\n",
      "Epoch 123/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 482.3304 - mae: 14.5595 - val_loss: 184.9850 - val_mae: 8.5053\n",
      "Epoch 124/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 488.9219 - mae: 14.6426 - val_loss: 189.9659 - val_mae: 8.5991\n",
      "Epoch 125/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 470.1612 - mae: 14.4467 - val_loss: 205.7854 - val_mae: 9.2475\n",
      "Epoch 126/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 472.9556 - mae: 14.4027 - val_loss: 184.7014 - val_mae: 8.4041\n",
      "Epoch 127/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 498.0967 - mae: 14.6270 - val_loss: 195.1875 - val_mae: 9.0084\n",
      "Epoch 128/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 435.3026 - mae: 13.9370 - val_loss: 202.7259 - val_mae: 9.2880\n",
      "Epoch 129/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 459.6013 - mae: 14.2703 - val_loss: 207.8498 - val_mae: 9.4129\n",
      "Epoch 130/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 460.7971 - mae: 14.1502 - val_loss: 200.4075 - val_mae: 9.0834\n",
      "Epoch 131/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 470.3844 - mae: 14.3363 - val_loss: 202.3718 - val_mae: 9.2368\n",
      "Epoch 132/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 466.2024 - mae: 14.3438 - val_loss: 197.6675 - val_mae: 8.9687\n",
      "Epoch 133/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 474.9236 - mae: 14.3395 - val_loss: 236.0698 - val_mae: 10.4238\n",
      "Epoch 134/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 470.9636 - mae: 14.4008 - val_loss: 169.4425 - val_mae: 7.9876\n",
      "Epoch 135/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 458.4956 - mae: 14.1615 - val_loss: 196.6457 - val_mae: 9.1035\n",
      "Epoch 136/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 470.9242 - mae: 14.3184 - val_loss: 206.0393 - val_mae: 9.4661\n",
      "Epoch 137/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 460.7698 - mae: 14.0935 - val_loss: 211.9051 - val_mae: 9.5546\n",
      "Epoch 138/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 458.9117 - mae: 14.1378 - val_loss: 211.8522 - val_mae: 9.5016\n",
      "Epoch 139/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 420.7077 - mae: 13.6315 - val_loss: 176.9051 - val_mae: 8.2552\n",
      "Epoch 140/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 443.3602 - mae: 13.7683 - val_loss: 234.5286 - val_mae: 10.2449\n",
      "Epoch 141/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 439.2925 - mae: 13.7460 - val_loss: 187.4014 - val_mae: 8.7104\n",
      "Epoch 142/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 443.9843 - mae: 13.8827 - val_loss: 181.3168 - val_mae: 8.5066\n",
      "Epoch 143/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 458.6862 - mae: 14.0430 - val_loss: 209.2295 - val_mae: 9.6427\n",
      "Epoch 144/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 427.8454 - mae: 13.6604 - val_loss: 238.0885 - val_mae: 10.4395\n",
      "Epoch 145/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 431.8531 - mae: 13.7225 - val_loss: 214.8225 - val_mae: 9.6502\n",
      "Epoch 146/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 471.7225 - mae: 13.9566 - val_loss: 201.4868 - val_mae: 9.2385\n",
      "Epoch 147/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 445.7675 - mae: 13.9145 - val_loss: 202.3052 - val_mae: 9.3494\n",
      "Epoch 148/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 421.8380 - mae: 13.4505 - val_loss: 201.7686 - val_mae: 9.3430\n",
      "Epoch 149/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 423.3806 - mae: 13.4563 - val_loss: 185.9133 - val_mae: 8.7740\n",
      "Epoch 150/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 423.3758 - mae: 13.4165 - val_loss: 190.2967 - val_mae: 8.8311\n",
      "Epoch 151/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 422.7528 - mae: 13.5261 - val_loss: 238.4516 - val_mae: 10.5638\n",
      "Epoch 152/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 416.5643 - mae: 13.4198 - val_loss: 193.7331 - val_mae: 9.1176\n",
      "Epoch 153/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 433.1299 - mae: 13.5477 - val_loss: 221.9581 - val_mae: 10.1303\n",
      "Epoch 154/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 426.0737 - mae: 13.5900 - val_loss: 206.1255 - val_mae: 9.4394\n",
      "Epoch 155/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 427.0196 - mae: 13.5110 - val_loss: 167.7250 - val_mae: 8.0609\n",
      "Epoch 156/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 435.5847 - mae: 13.6442 - val_loss: 196.3035 - val_mae: 9.1907\n",
      "Epoch 157/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 418.5276 - mae: 13.2983 - val_loss: 246.3997 - val_mae: 10.6202\n",
      "Epoch 158/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 415.1543 - mae: 13.2257 - val_loss: 234.3225 - val_mae: 10.4380\n",
      "Epoch 159/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 399.2737 - mae: 13.0834 - val_loss: 207.0584 - val_mae: 9.5774\n",
      "Epoch 160/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 392.4296 - mae: 13.0478 - val_loss: 226.4949 - val_mae: 10.2197\n",
      "Epoch 161/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 405.7425 - mae: 13.0679 - val_loss: 244.4750 - val_mae: 10.5919\n",
      "Epoch 162/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 413.4517 - mae: 13.0709 - val_loss: 280.0044 - val_mae: 11.6799\n",
      "Epoch 163/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 407.8457 - mae: 13.1036 - val_loss: 255.6312 - val_mae: 10.9935\n",
      "Epoch 164/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 394.9785 - mae: 12.8309 - val_loss: 264.9830 - val_mae: 11.2159\n",
      "Epoch 165/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 387.7267 - mae: 12.8077 - val_loss: 239.5467 - val_mae: 10.6474\n",
      "Epoch 166/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 398.4376 - mae: 12.8287 - val_loss: 242.7076 - val_mae: 10.5643\n",
      "Epoch 167/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 389.9003 - mae: 12.6523 - val_loss: 245.0081 - val_mae: 10.6560\n",
      "Epoch 168/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 396.8646 - mae: 12.8252 - val_loss: 306.8708 - val_mae: 12.1642\n",
      "Epoch 169/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 365.9618 - mae: 12.5102 - val_loss: 234.6738 - val_mae: 10.4393\n",
      "Epoch 170/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 389.2282 - mae: 12.7131 - val_loss: 236.6596 - val_mae: 10.3939\n",
      "Epoch 171/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 380.2549 - mae: 12.5619 - val_loss: 267.7060 - val_mae: 11.4148\n",
      "Epoch 172/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 389.4928 - mae: 12.7968 - val_loss: 215.8911 - val_mae: 9.9258\n",
      "Epoch 173/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 385.1178 - mae: 12.4916 - val_loss: 233.6769 - val_mae: 10.2139\n",
      "Epoch 174/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 363.3751 - mae: 12.2895 - val_loss: 246.7102 - val_mae: 10.6866\n",
      "Epoch 175/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 361.1803 - mae: 12.2978 - val_loss: 296.5918 - val_mae: 12.1160\n",
      "Epoch 176/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 360.1317 - mae: 12.3798 - val_loss: 268.7387 - val_mae: 11.3875\n",
      "Epoch 177/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 367.0938 - mae: 12.3209 - val_loss: 224.0382 - val_mae: 10.1865\n",
      "Epoch 178/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 354.6801 - mae: 12.1150 - val_loss: 258.2562 - val_mae: 11.0065\n",
      "Epoch 179/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 348.9052 - mae: 11.9642 - val_loss: 289.3676 - val_mae: 11.9841\n",
      "Epoch 180/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 359.8928 - mae: 12.1023 - val_loss: 286.3352 - val_mae: 11.8489\n",
      "Epoch 181/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 376.1271 - mae: 12.1304 - val_loss: 325.8123 - val_mae: 12.8852\n",
      "Epoch 182/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 352.7303 - mae: 11.9535 - val_loss: 353.0972 - val_mae: 13.1409\n",
      "Epoch 183/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 355.6945 - mae: 11.9843 - val_loss: 283.5590 - val_mae: 11.7915\n",
      "Epoch 184/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 343.5698 - mae: 11.8122 - val_loss: 329.8959 - val_mae: 12.8067\n",
      "Epoch 185/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 326.2410 - mae: 11.6149 - val_loss: 390.2686 - val_mae: 13.9872\n",
      "Epoch 186/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 344.1485 - mae: 11.7194 - val_loss: 395.5035 - val_mae: 13.9887\n",
      "Epoch 187/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 316.9608 - mae: 11.3750 - val_loss: 432.6089 - val_mae: 14.9162\n",
      "Epoch 188/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 319.6777 - mae: 11.3001 - val_loss: 414.6801 - val_mae: 14.8608\n",
      "Epoch 189/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 317.5526 - mae: 11.2517 - val_loss: 440.9699 - val_mae: 15.1147\n",
      "Epoch 190/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 312.8987 - mae: 11.2547 - val_loss: 406.2378 - val_mae: 14.6403\n",
      "Epoch 191/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 308.9946 - mae: 11.0079 - val_loss: 399.9160 - val_mae: 14.1856\n",
      "Epoch 192/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 313.6659 - mae: 11.1110 - val_loss: 430.5321 - val_mae: 14.8706\n",
      "Epoch 193/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 293.9862 - mae: 10.9126 - val_loss: 478.9813 - val_mae: 15.6998\n",
      "Epoch 194/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 309.2041 - mae: 11.0125 - val_loss: 560.3985 - val_mae: 17.2319\n",
      "Epoch 195/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 296.7341 - mae: 10.9140 - val_loss: 499.3595 - val_mae: 15.9293\n",
      "Epoch 196/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 299.8847 - mae: 10.8634 - val_loss: 563.1081 - val_mae: 16.8663\n",
      "Epoch 197/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 281.6378 - mae: 10.5906 - val_loss: 553.6481 - val_mae: 16.7938\n",
      "Epoch 198/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 296.4895 - mae: 10.7960 - val_loss: 690.9745 - val_mae: 18.6101\n",
      "Epoch 199/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 275.1042 - mae: 10.3756 - val_loss: 683.4197 - val_mae: 18.5383\n",
      "Epoch 200/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 288.4561 - mae: 10.4773 - val_loss: 739.9562 - val_mae: 19.1274\n",
      "Epoch 201/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 278.5930 - mae: 10.4643 - val_loss: 678.8200 - val_mae: 18.4931\n",
      "Epoch 202/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 281.4555 - mae: 10.4364 - val_loss: 799.1628 - val_mae: 20.1888\n",
      "Epoch 203/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 290.9841 - mae: 10.3717 - val_loss: 720.4305 - val_mae: 19.2494\n",
      "Epoch 204/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 272.9514 - mae: 10.1938 - val_loss: 847.8913 - val_mae: 20.6139\n",
      "Epoch 205/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 279.4838 - mae: 10.3895 - val_loss: 865.5310 - val_mae: 21.0056\n",
      "Epoch 206/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 253.9361 - mae: 9.9968 - val_loss: 771.3198 - val_mae: 19.6042\n",
      "Epoch 207/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 266.3719 - mae: 10.1932 - val_loss: 866.5383 - val_mae: 20.8915\n",
      "Epoch 208/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 242.1670 - mae: 9.9205 - val_loss: 915.1466 - val_mae: 21.7027\n",
      "Epoch 209/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 255.6979 - mae: 10.0257 - val_loss: 957.8776 - val_mae: 22.2836\n",
      "Epoch 210/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 239.5108 - mae: 9.6791 - val_loss: 979.1345 - val_mae: 22.3073\n",
      "Epoch 211/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 249.8709 - mae: 9.9955 - val_loss: 1089.9668 - val_mae: 23.3919\n",
      "Epoch 212/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 251.7537 - mae: 9.8363 - val_loss: 944.5989 - val_mae: 21.9309\n",
      "Epoch 213/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 262.3508 - mae: 9.7953 - val_loss: 893.7603 - val_mae: 21.4901\n",
      "Epoch 214/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 254.9599 - mae: 9.8161 - val_loss: 1039.2788 - val_mae: 22.8082\n",
      "Epoch 215/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 244.7971 - mae: 9.6795 - val_loss: 1056.6740 - val_mae: 23.3416\n",
      "Epoch 216/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 236.1689 - mae: 9.6563 - val_loss: 1096.2357 - val_mae: 23.7615\n",
      "Epoch 217/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 237.1939 - mae: 9.6604 - val_loss: 1073.4023 - val_mae: 23.1677\n",
      "Epoch 218/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 227.9050 - mae: 9.5164 - val_loss: 1018.9603 - val_mae: 23.1365\n",
      "Epoch 219/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 228.4577 - mae: 9.4355 - val_loss: 918.7656 - val_mae: 21.5201\n",
      "Epoch 220/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 235.4214 - mae: 9.4901 - val_loss: 1207.7928 - val_mae: 24.7023\n",
      "Epoch 221/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 227.8263 - mae: 9.2893 - val_loss: 1197.6105 - val_mae: 24.4894\n",
      "Epoch 222/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 210.7657 - mae: 9.1295 - val_loss: 1183.6914 - val_mae: 24.3024\n",
      "Epoch 223/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 240.2926 - mae: 9.4689 - val_loss: 1169.2144 - val_mae: 23.9743\n",
      "Epoch 224/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 227.1848 - mae: 9.3567 - val_loss: 1086.0658 - val_mae: 23.1937\n",
      "Epoch 225/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 220.3226 - mae: 9.2439 - val_loss: 1114.9941 - val_mae: 23.7614\n",
      "Epoch 226/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 214.6094 - mae: 9.2187 - val_loss: 1168.2788 - val_mae: 24.3077\n",
      "Epoch 227/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 231.6635 - mae: 9.3880 - val_loss: 1382.7454 - val_mae: 26.2173\n",
      "Epoch 228/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 209.2199 - mae: 8.9689 - val_loss: 1203.2236 - val_mae: 24.4296\n",
      "Epoch 229/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 217.6896 - mae: 9.0967 - val_loss: 1220.2094 - val_mae: 24.8862\n",
      "Epoch 230/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 224.2597 - mae: 9.1931 - val_loss: 1283.8777 - val_mae: 25.3681\n",
      "Epoch 231/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 211.0400 - mae: 8.9526 - val_loss: 1265.9244 - val_mae: 25.1700\n",
      "Epoch 232/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 202.8243 - mae: 8.8596 - val_loss: 1203.5663 - val_mae: 24.2457\n",
      "Epoch 233/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 205.3681 - mae: 8.8863 - val_loss: 1191.9288 - val_mae: 24.4527\n",
      "Epoch 234/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 209.6460 - mae: 8.9407 - val_loss: 1290.9835 - val_mae: 25.5843\n",
      "Epoch 235/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 217.3463 - mae: 8.9870 - val_loss: 1275.7223 - val_mae: 24.9064\n",
      "Epoch 236/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 200.8323 - mae: 8.9257 - val_loss: 1264.5929 - val_mae: 25.0668\n",
      "Epoch 237/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 208.7206 - mae: 8.7855 - val_loss: 1215.3745 - val_mae: 24.5497\n",
      "Epoch 238/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 217.3260 - mae: 9.0368 - val_loss: 1388.0417 - val_mae: 26.3476\n",
      "Epoch 239/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 211.2182 - mae: 8.8322 - val_loss: 1245.8959 - val_mae: 25.0199\n",
      "Epoch 240/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 204.8941 - mae: 8.9089 - val_loss: 1295.0459 - val_mae: 25.4679\n",
      "Epoch 241/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 196.2059 - mae: 8.6035 - val_loss: 1418.6393 - val_mae: 26.5209\n",
      "Epoch 242/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 201.5034 - mae: 8.7045 - val_loss: 1293.5879 - val_mae: 25.3791\n",
      "Epoch 243/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 203.8676 - mae: 8.7857 - val_loss: 1430.2577 - val_mae: 26.6982\n",
      "Epoch 244/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 204.4335 - mae: 8.8482 - val_loss: 1346.6019 - val_mae: 25.9704\n",
      "Epoch 245/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 196.5480 - mae: 8.5835 - val_loss: 1422.4280 - val_mae: 26.7710\n",
      "Epoch 246/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 209.7587 - mae: 8.7167 - val_loss: 1373.7058 - val_mae: 26.3563\n",
      "Epoch 247/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 199.7969 - mae: 8.6007 - val_loss: 1374.5765 - val_mae: 26.1671\n",
      "Epoch 248/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 186.9689 - mae: 8.4998 - val_loss: 1463.0908 - val_mae: 27.3759\n",
      "Epoch 249/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 200.0333 - mae: 8.5684 - val_loss: 1361.0505 - val_mae: 26.0786\n",
      "Epoch 250/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 187.4361 - mae: 8.4122 - val_loss: 1458.2043 - val_mae: 26.9948\n",
      "Epoch 251/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 183.8350 - mae: 8.2733 - val_loss: 1473.8091 - val_mae: 26.9958\n",
      "Epoch 252/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 190.3663 - mae: 8.4089 - val_loss: 1565.5571 - val_mae: 28.0421\n",
      "Epoch 253/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 205.7258 - mae: 8.6039 - val_loss: 1408.3503 - val_mae: 26.5614\n",
      "Epoch 254/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 197.7243 - mae: 8.4619 - val_loss: 1529.9436 - val_mae: 27.4908\n",
      "Epoch 255/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 188.2684 - mae: 8.3846 - val_loss: 1525.0302 - val_mae: 27.5436\n",
      "Epoch 256/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 183.1746 - mae: 8.3031 - val_loss: 1513.8679 - val_mae: 27.1772\n",
      "Epoch 257/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 181.5520 - mae: 8.2316 - val_loss: 1491.4799 - val_mae: 27.1874\n",
      "Epoch 258/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 180.1392 - mae: 8.1562 - val_loss: 1421.2385 - val_mae: 26.5099\n",
      "Epoch 259/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 183.0461 - mae: 8.2153 - val_loss: 1466.7305 - val_mae: 26.8946\n",
      "Epoch 260/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 192.3937 - mae: 8.3434 - val_loss: 1571.4753 - val_mae: 27.8548\n",
      "Epoch 261/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 189.6299 - mae: 8.4036 - val_loss: 1476.4220 - val_mae: 26.9732\n",
      "Epoch 262/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 182.7116 - mae: 8.2080 - val_loss: 1467.9386 - val_mae: 27.0748\n",
      "Epoch 263/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 188.4921 - mae: 8.1395 - val_loss: 1595.1348 - val_mae: 28.2163\n",
      "Epoch 264/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 177.2432 - mae: 8.1772 - val_loss: 1535.9983 - val_mae: 27.3253\n",
      "Epoch 265/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 183.3638 - mae: 8.1550 - val_loss: 1564.2302 - val_mae: 27.9299\n",
      "Epoch 266/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 179.9903 - mae: 8.0834 - val_loss: 1561.5377 - val_mae: 27.7737\n",
      "Epoch 267/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 178.8146 - mae: 8.1102 - val_loss: 1638.0066 - val_mae: 28.2764\n",
      "Epoch 268/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 190.8826 - mae: 8.2048 - val_loss: 1605.0607 - val_mae: 28.1331\n",
      "Epoch 269/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 191.7059 - mae: 8.1199 - val_loss: 1617.1648 - val_mae: 28.3857\n",
      "Epoch 270/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 178.8233 - mae: 8.1053 - val_loss: 1717.4297 - val_mae: 29.1495\n",
      "Epoch 271/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 172.2727 - mae: 7.9219 - val_loss: 1616.5780 - val_mae: 28.1421\n",
      "Epoch 272/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 177.2126 - mae: 8.0545 - val_loss: 1621.1295 - val_mae: 28.4493\n",
      "Epoch 273/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 184.2717 - mae: 8.0410 - val_loss: 1570.8278 - val_mae: 28.0468\n",
      "Epoch 274/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 184.3891 - mae: 7.9894 - val_loss: 1588.1667 - val_mae: 28.2305\n",
      "Epoch 275/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 180.3343 - mae: 8.0191 - val_loss: 1610.4867 - val_mae: 28.3758\n",
      "Epoch 276/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 172.1343 - mae: 7.8581 - val_loss: 1665.9323 - val_mae: 28.7188\n",
      "Epoch 277/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 179.8480 - mae: 8.0168 - val_loss: 1603.2026 - val_mae: 28.1645\n",
      "Epoch 278/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 171.4449 - mae: 7.8382 - val_loss: 1716.9255 - val_mae: 29.1995\n",
      "Epoch 279/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 172.8485 - mae: 7.8017 - val_loss: 1660.9344 - val_mae: 28.6107\n",
      "Epoch 280/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 165.8235 - mae: 7.8377 - val_loss: 1606.8535 - val_mae: 28.0957\n",
      "Epoch 281/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 165.2432 - mae: 7.8165 - val_loss: 1600.5640 - val_mae: 28.3936\n",
      "Epoch 282/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 178.2450 - mae: 7.9615 - val_loss: 1624.9365 - val_mae: 28.4405\n",
      "Epoch 283/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 168.9602 - mae: 7.9315 - val_loss: 1708.8678 - val_mae: 29.2077\n",
      "Epoch 284/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 163.4477 - mae: 7.7632 - val_loss: 1781.2104 - val_mae: 30.0522\n",
      "Epoch 285/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 178.7752 - mae: 7.9634 - val_loss: 1621.9722 - val_mae: 28.4559\n",
      "Epoch 286/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 162.2722 - mae: 7.7419 - val_loss: 1769.9722 - val_mae: 29.7229\n",
      "Epoch 287/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 169.5684 - mae: 7.7398 - val_loss: 1633.4860 - val_mae: 28.4439\n",
      "Epoch 288/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 172.9784 - mae: 7.8872 - val_loss: 1629.0331 - val_mae: 28.6219\n",
      "Epoch 289/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 171.0135 - mae: 7.7926 - val_loss: 1718.1202 - val_mae: 29.0070\n",
      "Epoch 290/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 162.2052 - mae: 7.6729 - val_loss: 1714.5518 - val_mae: 29.4091\n",
      "Epoch 291/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 167.2449 - mae: 7.7135 - val_loss: 1653.0688 - val_mae: 29.0073\n",
      "Epoch 292/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 164.6519 - mae: 7.6947 - val_loss: 1737.1792 - val_mae: 29.5652\n",
      "Epoch 293/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 159.4109 - mae: 7.5792 - val_loss: 1766.6143 - val_mae: 29.8031\n",
      "Epoch 294/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 182.3352 - mae: 7.7153 - val_loss: 1675.4607 - val_mae: 28.7861\n",
      "Epoch 295/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 164.7572 - mae: 7.7410 - val_loss: 1687.5563 - val_mae: 29.0738\n",
      "Epoch 296/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 174.9446 - mae: 7.7655 - val_loss: 1625.4364 - val_mae: 28.5596\n",
      "Epoch 297/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 162.8298 - mae: 7.5976 - val_loss: 1701.7946 - val_mae: 29.0518\n",
      "Epoch 298/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 163.1137 - mae: 7.5741 - val_loss: 1612.0796 - val_mae: 28.4618\n",
      "Epoch 299/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 164.5989 - mae: 7.5405 - val_loss: 1714.2539 - val_mae: 29.4448\n",
      "Epoch 300/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 162.9036 - mae: 7.6124 - val_loss: 1693.4929 - val_mae: 29.2813\n",
      "Epoch 301/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 169.7475 - mae: 7.6803 - val_loss: 1663.9686 - val_mae: 29.1279\n",
      "Epoch 302/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 153.3157 - mae: 7.5522 - val_loss: 1800.5740 - val_mae: 30.3794\n",
      "Epoch 303/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 162.6588 - mae: 7.5033 - val_loss: 1718.6869 - val_mae: 29.4239\n",
      "Epoch 304/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 163.5900 - mae: 7.6329 - val_loss: 1712.3007 - val_mae: 29.5927\n",
      "Epoch 305/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 176.7514 - mae: 7.6006 - val_loss: 1707.2780 - val_mae: 29.4079\n",
      "Epoch 306/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 165.0947 - mae: 7.5449 - val_loss: 1798.0532 - val_mae: 29.9757\n",
      "Epoch 307/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 162.7112 - mae: 7.3444 - val_loss: 1726.9523 - val_mae: 29.6811\n",
      "Epoch 308/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 151.6651 - mae: 7.5132 - val_loss: 1851.2073 - val_mae: 30.4345\n",
      "Epoch 309/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 162.1424 - mae: 7.5705 - val_loss: 1734.1167 - val_mae: 29.7296\n",
      "Epoch 310/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 160.0605 - mae: 7.4414 - val_loss: 1744.7585 - val_mae: 29.7363\n",
      "Epoch 311/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 159.3125 - mae: 7.4813 - val_loss: 1742.6355 - val_mae: 29.5211\n",
      "Epoch 312/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 154.5397 - mae: 7.4425 - val_loss: 1701.5986 - val_mae: 29.2605\n",
      "Epoch 313/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 170.5600 - mae: 7.5312 - val_loss: 1671.0808 - val_mae: 29.1332\n",
      "Epoch 314/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 157.9621 - mae: 7.4206 - val_loss: 1690.8335 - val_mae: 29.2347\n",
      "Epoch 315/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 161.3787 - mae: 7.4519 - val_loss: 1685.2904 - val_mae: 29.0849\n",
      "Epoch 316/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 155.8812 - mae: 7.3698 - val_loss: 1810.9875 - val_mae: 30.0388\n",
      "Epoch 317/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 164.7482 - mae: 7.4610 - val_loss: 1756.8546 - val_mae: 29.6958\n",
      "Epoch 318/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 170.4952 - mae: 7.5239 - val_loss: 1695.3538 - val_mae: 29.2412\n",
      "Epoch 319/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 157.5730 - mae: 7.4937 - val_loss: 1742.8700 - val_mae: 29.5449\n",
      "Epoch 320/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 165.6915 - mae: 7.3882 - val_loss: 1637.7334 - val_mae: 28.8389\n",
      "Epoch 321/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 148.3216 - mae: 7.3088 - val_loss: 1751.1188 - val_mae: 29.4958\n",
      "Epoch 322/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 151.2629 - mae: 7.3343 - val_loss: 1698.0869 - val_mae: 29.3252\n",
      "Epoch 323/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 159.3220 - mae: 7.4226 - val_loss: 1680.5472 - val_mae: 29.0470\n",
      "Epoch 324/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 158.9490 - mae: 7.3466 - val_loss: 1652.5005 - val_mae: 28.6403\n",
      "Epoch 325/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 151.2034 - mae: 7.2599 - val_loss: 1725.4933 - val_mae: 29.4701\n",
      "Epoch 326/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 158.4671 - mae: 7.3497 - val_loss: 1590.1239 - val_mae: 28.2944\n",
      "Epoch 327/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 158.6422 - mae: 7.3328 - val_loss: 1801.6174 - val_mae: 29.8812\n",
      "Epoch 328/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 153.0733 - mae: 7.2576 - val_loss: 1663.5109 - val_mae: 28.8175\n",
      "Epoch 329/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 162.0713 - mae: 7.3372 - val_loss: 1696.4485 - val_mae: 28.9308\n",
      "Epoch 330/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 155.8412 - mae: 7.3276 - val_loss: 1659.8188 - val_mae: 28.7854\n",
      "Epoch 331/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 170.5449 - mae: 7.4272 - val_loss: 1690.1313 - val_mae: 28.9184\n",
      "Epoch 332/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 150.8036 - mae: 7.1539 - val_loss: 1823.1582 - val_mae: 29.9131\n",
      "Epoch 333/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 154.6232 - mae: 7.2456 - val_loss: 1657.6510 - val_mae: 28.8130\n",
      "Epoch 334/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 148.7123 - mae: 7.2777 - val_loss: 1697.5397 - val_mae: 28.9138\n",
      "Epoch 335/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 154.9909 - mae: 7.2397 - val_loss: 1712.7177 - val_mae: 29.1566\n",
      "Epoch 336/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 142.2666 - mae: 7.1058 - val_loss: 1755.3127 - val_mae: 29.2890\n",
      "Epoch 337/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 155.9708 - mae: 7.2434 - val_loss: 1710.6803 - val_mae: 29.1292\n",
      "Epoch 338/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 149.4468 - mae: 7.2030 - val_loss: 1681.5717 - val_mae: 28.9454\n",
      "Epoch 339/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 144.2373 - mae: 7.0845 - val_loss: 1654.8988 - val_mae: 28.5484\n",
      "Epoch 340/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 153.9474 - mae: 7.2483 - val_loss: 1663.0585 - val_mae: 28.6964\n",
      "Epoch 341/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 153.0218 - mae: 7.2431 - val_loss: 1750.6305 - val_mae: 29.3416\n",
      "Epoch 342/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 158.1866 - mae: 7.2585 - val_loss: 1733.9790 - val_mae: 29.1591\n",
      "Epoch 343/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 146.7755 - mae: 7.1306 - val_loss: 1786.9731 - val_mae: 29.7507\n",
      "Epoch 344/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 148.8249 - mae: 7.1245 - val_loss: 1731.5579 - val_mae: 29.1345\n",
      "Epoch 345/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 141.2498 - mae: 7.0350 - val_loss: 1736.8435 - val_mae: 29.3170\n",
      "Epoch 346/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 152.2165 - mae: 7.2250 - val_loss: 1704.5479 - val_mae: 28.9379\n",
      "Epoch 347/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 138.0998 - mae: 7.0639 - val_loss: 1690.0917 - val_mae: 28.7241\n",
      "Epoch 348/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 148.0254 - mae: 7.1378 - val_loss: 1732.8334 - val_mae: 29.2999\n",
      "Epoch 349/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 152.9184 - mae: 7.1806 - val_loss: 1757.2196 - val_mae: 29.3578\n",
      "Epoch 350/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 153.0515 - mae: 7.1488 - val_loss: 1757.5912 - val_mae: 29.4146\n",
      "Epoch 351/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 147.2590 - mae: 7.1645 - val_loss: 1669.1027 - val_mae: 28.5912\n",
      "Epoch 352/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 146.9526 - mae: 7.0387 - val_loss: 1621.8478 - val_mae: 28.2107\n",
      "Epoch 353/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 148.0386 - mae: 7.1189 - val_loss: 1692.4891 - val_mae: 28.6536\n",
      "Epoch 354/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 141.4359 - mae: 6.9791 - val_loss: 1710.1586 - val_mae: 28.9192\n",
      "Epoch 355/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 142.8328 - mae: 7.0374 - val_loss: 1722.0586 - val_mae: 28.9504\n",
      "Epoch 356/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 155.9763 - mae: 7.1070 - val_loss: 1688.9802 - val_mae: 28.8568\n",
      "Epoch 357/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 139.0029 - mae: 7.0187 - val_loss: 1606.4891 - val_mae: 28.0474\n",
      "Epoch 358/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 144.6959 - mae: 7.0820 - val_loss: 1750.1608 - val_mae: 28.9135\n",
      "Epoch 359/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 140.5163 - mae: 7.0050 - val_loss: 1662.8058 - val_mae: 28.3644\n",
      "Epoch 360/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 146.9200 - mae: 6.9637 - val_loss: 1684.2802 - val_mae: 28.5313\n",
      "Epoch 361/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 146.1742 - mae: 7.0105 - val_loss: 1739.4836 - val_mae: 29.1465\n",
      "Epoch 362/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 137.8507 - mae: 7.0252 - val_loss: 1663.6025 - val_mae: 28.6360\n",
      "Epoch 363/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 147.8053 - mae: 7.1075 - val_loss: 1757.0952 - val_mae: 29.1542\n",
      "Epoch 364/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 143.3802 - mae: 6.8975 - val_loss: 1675.7914 - val_mae: 28.5429\n",
      "Epoch 365/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 136.9375 - mae: 6.8968 - val_loss: 1709.8236 - val_mae: 28.6989\n",
      "Epoch 366/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 154.1157 - mae: 7.0361 - val_loss: 1707.0831 - val_mae: 28.7017\n",
      "Epoch 367/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 154.6594 - mae: 7.0554 - val_loss: 1675.3011 - val_mae: 28.3955\n",
      "Epoch 368/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 148.3837 - mae: 6.8944 - val_loss: 1548.9442 - val_mae: 27.5123\n",
      "Epoch 369/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 140.2672 - mae: 6.9373 - val_loss: 1696.0067 - val_mae: 28.5872\n",
      "Epoch 370/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 133.3141 - mae: 6.8579 - val_loss: 1643.0690 - val_mae: 28.2305\n",
      "Epoch 371/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 141.1472 - mae: 6.8977 - val_loss: 1737.1434 - val_mae: 28.9590\n",
      "Epoch 372/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 146.3229 - mae: 6.9675 - val_loss: 1643.8206 - val_mae: 28.2778\n",
      "Epoch 373/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 152.9317 - mae: 7.1049 - val_loss: 1679.9722 - val_mae: 28.5090\n",
      "Epoch 374/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 166.7421 - mae: 7.0307 - val_loss: 1544.5875 - val_mae: 27.1637\n",
      "Epoch 375/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 142.1718 - mae: 6.9235 - val_loss: 1703.7092 - val_mae: 28.6730\n",
      "Epoch 376/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 142.6511 - mae: 6.9416 - val_loss: 1700.9609 - val_mae: 28.7000\n",
      "Epoch 377/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 153.2637 - mae: 7.0337 - val_loss: 1632.8047 - val_mae: 27.9003\n",
      "Epoch 378/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 145.3521 - mae: 6.9691 - val_loss: 1724.6364 - val_mae: 28.7358\n",
      "Epoch 379/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 148.5297 - mae: 6.9248 - val_loss: 1585.1890 - val_mae: 27.6911\n",
      "Epoch 380/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 154.5099 - mae: 6.9777 - val_loss: 1690.5740 - val_mae: 28.4620\n",
      "Epoch 381/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 166.4281 - mae: 7.1796 - val_loss: 1667.7560 - val_mae: 28.2347\n",
      "Epoch 382/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 135.4375 - mae: 6.7095 - val_loss: 1651.3538 - val_mae: 28.1620\n",
      "Epoch 383/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 141.6799 - mae: 6.8757 - val_loss: 1693.8092 - val_mae: 28.5016\n",
      "Epoch 384/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 136.3297 - mae: 6.7577 - val_loss: 1664.0354 - val_mae: 28.2175\n",
      "Epoch 385/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 139.5071 - mae: 6.8041 - val_loss: 1618.5072 - val_mae: 27.9891\n",
      "Epoch 386/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 142.0099 - mae: 6.9484 - val_loss: 1644.4149 - val_mae: 28.2191\n",
      "Epoch 387/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 136.7067 - mae: 6.7784 - val_loss: 1707.7073 - val_mae: 28.4972\n",
      "Epoch 388/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 153.3306 - mae: 7.1022 - val_loss: 1731.3145 - val_mae: 28.7666\n",
      "Epoch 389/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 148.0117 - mae: 6.9588 - val_loss: 1583.1611 - val_mae: 27.7377\n",
      "Epoch 390/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 141.0022 - mae: 6.8181 - val_loss: 1569.9702 - val_mae: 27.4863\n",
      "Epoch 391/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 147.5409 - mae: 6.8755 - val_loss: 1710.0951 - val_mae: 28.6798\n",
      "Epoch 392/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 136.3721 - mae: 6.7212 - val_loss: 1708.7372 - val_mae: 28.5844\n",
      "Epoch 393/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 145.0754 - mae: 6.9757 - val_loss: 1639.3556 - val_mae: 28.0078\n",
      "Epoch 394/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 138.5706 - mae: 6.7861 - val_loss: 1654.6423 - val_mae: 27.9987\n",
      "Epoch 395/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 135.8368 - mae: 6.8651 - val_loss: 1643.5187 - val_mae: 28.1410\n",
      "Epoch 396/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 136.3753 - mae: 6.7510 - val_loss: 1722.0479 - val_mae: 28.5882\n",
      "Epoch 397/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 156.4831 - mae: 7.0286 - val_loss: 1648.1953 - val_mae: 27.9062\n",
      "Epoch 398/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 142.0641 - mae: 6.7694 - val_loss: 1560.1880 - val_mae: 27.2721\n",
      "Epoch 399/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 135.7657 - mae: 6.7097 - val_loss: 1643.3896 - val_mae: 27.9938\n",
      "Epoch 400/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 147.9726 - mae: 6.8675 - val_loss: 1614.8119 - val_mae: 27.7942\n",
      "Epoch 401/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 145.4778 - mae: 6.8540 - val_loss: 1727.6724 - val_mae: 28.4713\n",
      "Epoch 402/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 142.5833 - mae: 6.8742 - val_loss: 1668.9637 - val_mae: 28.3002\n",
      "Epoch 403/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 155.3329 - mae: 6.9821 - val_loss: 1683.9446 - val_mae: 28.3363\n",
      "Epoch 404/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 132.0985 - mae: 6.7108 - val_loss: 1666.0120 - val_mae: 28.2405\n",
      "Epoch 405/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 147.6106 - mae: 6.8090 - val_loss: 1658.2114 - val_mae: 28.1413\n",
      "Epoch 406/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 134.0749 - mae: 6.8071 - val_loss: 1650.3182 - val_mae: 27.7609\n",
      "Epoch 407/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 142.4657 - mae: 6.7118 - val_loss: 1583.5939 - val_mae: 27.5362\n",
      "Epoch 408/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 140.9593 - mae: 6.8068 - val_loss: 1711.5754 - val_mae: 28.3736\n",
      "Epoch 409/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 148.4125 - mae: 6.8446 - val_loss: 1731.7335 - val_mae: 28.5411\n",
      "Epoch 410/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 138.4910 - mae: 6.8959 - val_loss: 1660.7296 - val_mae: 28.0361\n",
      "Epoch 411/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 141.2117 - mae: 6.7864 - val_loss: 1628.4896 - val_mae: 27.6214\n",
      "Epoch 412/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 147.4001 - mae: 6.7089 - val_loss: 1657.7880 - val_mae: 27.9645\n",
      "Epoch 413/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 133.1522 - mae: 6.7316 - val_loss: 1643.1392 - val_mae: 27.8239\n",
      "Epoch 414/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 148.7749 - mae: 6.7789 - val_loss: 1751.9702 - val_mae: 28.6740\n",
      "Epoch 415/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 134.0364 - mae: 6.6888 - val_loss: 1721.3875 - val_mae: 28.5168\n",
      "Epoch 416/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 143.6583 - mae: 6.7055 - val_loss: 1695.9465 - val_mae: 28.0844\n",
      "Epoch 417/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 140.0116 - mae: 6.7898 - val_loss: 1724.2700 - val_mae: 28.4617\n",
      "Epoch 418/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 134.7886 - mae: 6.6762 - val_loss: 1644.3741 - val_mae: 27.8352\n",
      "Epoch 419/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 143.3067 - mae: 6.7057 - val_loss: 1715.7485 - val_mae: 28.5567\n",
      "Epoch 420/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 140.8359 - mae: 6.6932 - val_loss: 1658.7122 - val_mae: 27.9830\n",
      "Epoch 421/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 136.1189 - mae: 6.8235 - val_loss: 1707.3999 - val_mae: 28.3267\n",
      "Epoch 422/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 126.7603 - mae: 6.6384 - val_loss: 1631.5073 - val_mae: 27.6424\n",
      "Epoch 423/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 137.1531 - mae: 6.7253 - val_loss: 1768.3911 - val_mae: 28.7386\n",
      "Epoch 424/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 138.1960 - mae: 6.6936 - val_loss: 1752.7476 - val_mae: 28.6468\n",
      "Epoch 425/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 121.3507 - mae: 6.4319 - val_loss: 1643.2971 - val_mae: 27.6963\n",
      "Epoch 426/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 138.4903 - mae: 6.7346 - val_loss: 1689.5363 - val_mae: 28.2580\n",
      "Epoch 427/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 137.5468 - mae: 6.7521 - val_loss: 1612.0219 - val_mae: 27.4782\n",
      "Epoch 428/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 121.5463 - mae: 6.5179 - val_loss: 1772.2017 - val_mae: 28.7812\n",
      "Epoch 429/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 132.2155 - mae: 6.6125 - val_loss: 1679.4927 - val_mae: 28.1398\n",
      "Epoch 430/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 134.9665 - mae: 6.6234 - val_loss: 1699.3799 - val_mae: 28.3010\n",
      "Epoch 431/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 126.8871 - mae: 6.5033 - val_loss: 1664.8824 - val_mae: 27.9319\n",
      "Epoch 432/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 129.0292 - mae: 6.6042 - val_loss: 1682.7745 - val_mae: 28.1348\n",
      "Epoch 433/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 133.1179 - mae: 6.5749 - val_loss: 1639.7529 - val_mae: 27.8567\n",
      "Epoch 434/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 134.3699 - mae: 6.4996 - val_loss: 1631.2395 - val_mae: 27.7248\n",
      "Epoch 435/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 133.3598 - mae: 6.6093 - val_loss: 1745.9399 - val_mae: 28.6225\n",
      "Epoch 436/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 141.5974 - mae: 6.7086 - val_loss: 1570.6844 - val_mae: 27.1722\n",
      "Epoch 437/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 123.6452 - mae: 6.5557 - val_loss: 1646.7728 - val_mae: 27.9476\n",
      "Epoch 438/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 127.3159 - mae: 6.5644 - val_loss: 1729.9375 - val_mae: 28.4996\n",
      "Epoch 439/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 133.0730 - mae: 6.6607 - val_loss: 1621.5898 - val_mae: 27.6051\n",
      "Epoch 440/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 134.8446 - mae: 6.6255 - val_loss: 1575.1860 - val_mae: 27.1333\n",
      "Epoch 441/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 125.2412 - mae: 6.4389 - val_loss: 1670.2303 - val_mae: 28.0612\n",
      "Epoch 442/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 123.7251 - mae: 6.5307 - val_loss: 1692.4185 - val_mae: 28.0912\n",
      "Epoch 443/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 125.8805 - mae: 6.5433 - val_loss: 1678.8376 - val_mae: 27.9521\n",
      "Epoch 444/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 117.5361 - mae: 6.3271 - val_loss: 1734.3671 - val_mae: 28.4803\n",
      "Epoch 445/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 127.0831 - mae: 6.5792 - val_loss: 1725.3909 - val_mae: 28.4917\n",
      "Epoch 446/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 127.1521 - mae: 6.5817 - val_loss: 1644.0673 - val_mae: 27.7400\n",
      "Epoch 447/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 131.4888 - mae: 6.6127 - val_loss: 1661.5355 - val_mae: 27.7933\n",
      "Epoch 448/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 129.0882 - mae: 6.5000 - val_loss: 1660.1146 - val_mae: 28.0178\n",
      "Epoch 449/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 138.1265 - mae: 6.6132 - val_loss: 1722.0958 - val_mae: 28.3840\n",
      "Epoch 450/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 145.5773 - mae: 6.7297 - val_loss: 1660.1525 - val_mae: 27.8332\n",
      "Epoch 451/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 126.5273 - mae: 6.5481 - val_loss: 1708.5826 - val_mae: 28.2431\n",
      "Epoch 452/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 135.9317 - mae: 6.5310 - val_loss: 1688.9028 - val_mae: 28.0368\n",
      "Epoch 453/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 130.9419 - mae: 6.5493 - val_loss: 1493.7845 - val_mae: 26.5907\n",
      "Epoch 454/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 134.2698 - mae: 6.6499 - val_loss: 1664.5964 - val_mae: 27.9048\n",
      "Epoch 455/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 135.6850 - mae: 6.5347 - val_loss: 1676.6320 - val_mae: 27.9392\n",
      "Epoch 456/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 127.2024 - mae: 6.4442 - val_loss: 1688.8055 - val_mae: 28.0682\n",
      "Epoch 457/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 126.1229 - mae: 6.4692 - val_loss: 1665.4486 - val_mae: 27.7694\n",
      "Epoch 458/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 128.4416 - mae: 6.4907 - val_loss: 1633.1166 - val_mae: 27.6879\n",
      "Epoch 459/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 129.6832 - mae: 6.4481 - val_loss: 1621.3999 - val_mae: 27.4869\n",
      "Epoch 460/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 132.4775 - mae: 6.5289 - val_loss: 1607.9819 - val_mae: 27.3344\n",
      "Epoch 461/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 126.9042 - mae: 6.4834 - val_loss: 1703.1680 - val_mae: 28.0169\n",
      "Epoch 462/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 125.1121 - mae: 6.5817 - val_loss: 1578.0992 - val_mae: 27.1315\n",
      "Epoch 463/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 131.3755 - mae: 6.5980 - val_loss: 1636.5516 - val_mae: 27.6603\n",
      "Epoch 464/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 129.1214 - mae: 6.4940 - val_loss: 1635.9583 - val_mae: 27.4581\n",
      "Epoch 465/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 135.7375 - mae: 6.5900 - val_loss: 1629.4559 - val_mae: 27.4961\n",
      "Epoch 466/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 133.9613 - mae: 6.4856 - val_loss: 1625.8110 - val_mae: 27.4519\n",
      "Epoch 467/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 130.0946 - mae: 6.5024 - val_loss: 1685.6112 - val_mae: 27.9318\n",
      "Epoch 468/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 123.1646 - mae: 6.4227 - val_loss: 1688.5228 - val_mae: 28.0120\n",
      "Epoch 469/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 116.9534 - mae: 6.3685 - val_loss: 1598.7831 - val_mae: 27.2244\n",
      "Epoch 470/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 124.8955 - mae: 6.4624 - val_loss: 1617.5208 - val_mae: 27.4282\n",
      "Epoch 471/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 123.5587 - mae: 6.3991 - val_loss: 1649.4241 - val_mae: 27.5968\n",
      "Epoch 472/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 140.9973 - mae: 6.5941 - val_loss: 1703.1732 - val_mae: 28.0876\n",
      "Epoch 473/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 126.7545 - mae: 6.4544 - val_loss: 1620.7632 - val_mae: 27.3152\n",
      "Epoch 474/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 133.5396 - mae: 6.4624 - val_loss: 1615.5172 - val_mae: 27.2453\n",
      "Epoch 475/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 123.2531 - mae: 6.4144 - val_loss: 1653.2452 - val_mae: 27.7823\n",
      "Epoch 476/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 121.9495 - mae: 6.4634 - val_loss: 1580.7524 - val_mae: 27.0571\n",
      "Epoch 477/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 132.8461 - mae: 6.5129 - val_loss: 1638.6288 - val_mae: 27.5701\n",
      "Epoch 478/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 124.5220 - mae: 6.3116 - val_loss: 1596.6838 - val_mae: 27.2546\n",
      "Epoch 479/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 138.9432 - mae: 6.5283 - val_loss: 1690.8453 - val_mae: 28.0783\n",
      "Epoch 480/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 129.1007 - mae: 6.3332 - val_loss: 1665.9594 - val_mae: 27.9475\n",
      "Epoch 481/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 124.4548 - mae: 6.3683 - val_loss: 1588.8417 - val_mae: 27.2127\n",
      "Epoch 482/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 130.4279 - mae: 6.4680 - val_loss: 1617.7697 - val_mae: 27.3924\n",
      "Epoch 483/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 141.7092 - mae: 6.5370 - val_loss: 1612.1804 - val_mae: 27.3842\n",
      "Epoch 484/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 124.3312 - mae: 6.3875 - val_loss: 1629.0162 - val_mae: 27.5105\n",
      "Epoch 485/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 133.7837 - mae: 6.4152 - val_loss: 1580.8983 - val_mae: 27.0598\n",
      "Epoch 486/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 134.5901 - mae: 6.4611 - val_loss: 1681.1449 - val_mae: 27.9693\n",
      "Epoch 487/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 124.0685 - mae: 6.3128 - val_loss: 1619.8447 - val_mae: 27.3834\n",
      "Epoch 488/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 128.2582 - mae: 6.4217 - val_loss: 1573.3375 - val_mae: 27.0910\n",
      "Epoch 489/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 125.4665 - mae: 6.2710 - val_loss: 1567.8857 - val_mae: 26.9430\n",
      "Epoch 490/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 126.0677 - mae: 6.3639 - val_loss: 1616.5974 - val_mae: 27.3609\n",
      "Epoch 491/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 124.9318 - mae: 6.3271 - val_loss: 1647.1497 - val_mae: 27.6600\n",
      "Epoch 492/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 129.7404 - mae: 6.3987 - val_loss: 1606.5573 - val_mae: 27.2225\n",
      "Epoch 493/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 121.6141 - mae: 6.3318 - val_loss: 1665.5004 - val_mae: 27.5795\n",
      "Epoch 494/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 130.1113 - mae: 6.3651 - val_loss: 1572.5129 - val_mae: 26.8973\n",
      "Epoch 495/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 126.7009 - mae: 6.3540 - val_loss: 1646.8452 - val_mae: 27.5473\n",
      "Epoch 496/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 121.5827 - mae: 6.2440 - val_loss: 1694.5579 - val_mae: 27.9829\n",
      "Epoch 497/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 144.1983 - mae: 6.4196 - val_loss: 1664.5730 - val_mae: 27.7401\n",
      "Epoch 498/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 126.7007 - mae: 6.3151 - val_loss: 1634.6248 - val_mae: 27.4707\n",
      "Epoch 499/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 119.6007 - mae: 6.2529 - val_loss: 1614.1464 - val_mae: 27.3265\n",
      "Epoch 500/500\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 116.2496 - mae: 6.2387 - val_loss: 1571.2240 - val_mae: 26.9565\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "🔹 Enhanced LSTM Model - RMSE: 39.60\n",
      "🔹 Enhanced LSTM Model - R² Score: 0.86\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_clean)\n",
    "X_test_scaled = scaler.transform(X_test_df)\n",
    "\n",
    "X_train_scaled = X_train_scaled.reshape(X_train_scaled.shape[0], 1, X_train_scaled.shape[1])  # (samples, time_steps, features)\n",
    "X_test_scaled = X_test_scaled.reshape(X_test_scaled.shape[0], 1, X_test_scaled.shape[1])  # (samples, time_steps, features)\n",
    "\n",
    "model = Sequential([\n",
    "    Bidirectional(LSTM(128, activation='relu', kernel_regularizer=l2(0.001), \n",
    "                        input_shape=(X_train_scaled.shape[1], X_train_scaled.shape[2]), return_sequences=True)),\n",
    "    Dropout(0.3),  \n",
    "    LSTM(64, activation='relu', kernel_regularizer=l2(0.001), return_sequences=False),\n",
    "    Dropout(0.3),  \n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='linear')  \n",
    "])\n",
    "\n",
    "optimizer = Adam(learning_rate=0.0001, clipnorm=0.5)  \n",
    "model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train_clean,\n",
    "    validation_data=(X_test_scaled, y_test),\n",
    "    epochs=500,  \n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "y_pred_lstm = model.predict(X_test_scaled)\n",
    "\n",
    "rmse_lstm = np.sqrt(mean_squared_error(y_test, y_pred_lstm))\n",
    "r2_lstm = r2_score(y_test, y_pred_lstm)\n",
    "\n",
    "print(f\" Enhanced LSTM Model - RMSE: {rmse_lstm:.2f}\")\n",
    "print(f\" Enhanced LSTM Model - R² Score: {r2_lstm:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/me.pranesh/Desktop/new clean/.venv/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 36053.7539 - mae: 155.8159 - val_loss: 31506.1328 - val_mae: 147.2040\n",
      "Epoch 2/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 27073.7988 - mae: 137.0638 - val_loss: 12189.0156 - val_mae: 92.1118\n",
      "Epoch 3/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 11307.7012 - mae: 87.4172 - val_loss: 7335.5552 - val_mae: 70.8438\n",
      "Epoch 4/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 7002.4722 - mae: 67.6276 - val_loss: 3548.0964 - val_mae: 46.8494\n",
      "Epoch 5/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 3441.5942 - mae: 43.5162 - val_loss: 1562.5120 - val_mae: 27.3037\n",
      "Epoch 6/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1947.1537 - mae: 30.2907 - val_loss: 1067.2522 - val_mae: 22.0190\n",
      "Epoch 7/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1594.2986 - mae: 27.2498 - val_loss: 893.4990 - val_mae: 19.9518\n",
      "Epoch 8/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1425.4492 - mae: 25.5720 - val_loss: 784.3925 - val_mae: 18.7357\n",
      "Epoch 9/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1341.7683 - mae: 24.9208 - val_loss: 720.1112 - val_mae: 17.9359\n",
      "Epoch 10/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1277.1215 - mae: 23.9911 - val_loss: 663.6894 - val_mae: 17.3148\n",
      "Epoch 11/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1225.2390 - mae: 23.7978 - val_loss: 615.2399 - val_mae: 16.8338\n",
      "Epoch 12/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1226.8512 - mae: 23.6174 - val_loss: 584.9712 - val_mae: 16.1414\n",
      "Epoch 13/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1111.8984 - mae: 22.9151 - val_loss: 555.5521 - val_mae: 15.8005\n",
      "Epoch 14/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1145.5175 - mae: 22.8960 - val_loss: 519.0603 - val_mae: 15.3626\n",
      "Epoch 15/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1056.7155 - mae: 22.4296 - val_loss: 495.8152 - val_mae: 14.9245\n",
      "Epoch 16/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1054.4686 - mae: 22.1463 - val_loss: 474.7367 - val_mae: 14.6399\n",
      "Epoch 17/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1013.8333 - mae: 21.8224 - val_loss: 462.6316 - val_mae: 14.3488\n",
      "Epoch 18/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1017.1354 - mae: 21.8057 - val_loss: 441.9656 - val_mae: 14.0460\n",
      "Epoch 19/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 958.6256 - mae: 21.2336 - val_loss: 432.7183 - val_mae: 13.7939\n",
      "Epoch 20/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 988.4534 - mae: 21.5574 - val_loss: 417.2028 - val_mae: 13.6337\n",
      "Epoch 21/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 986.8629 - mae: 21.5275 - val_loss: 407.5677 - val_mae: 13.4321\n",
      "Epoch 22/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 945.5881 - mae: 21.2348 - val_loss: 398.6012 - val_mae: 13.3301\n",
      "Epoch 23/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 932.4784 - mae: 20.9767 - val_loss: 387.6106 - val_mae: 13.1191\n",
      "Epoch 24/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 898.9909 - mae: 20.5988 - val_loss: 382.8669 - val_mae: 13.0755\n",
      "Epoch 25/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 877.2711 - mae: 20.3442 - val_loss: 374.6914 - val_mae: 12.9633\n",
      "Epoch 26/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 929.7355 - mae: 20.9690 - val_loss: 371.7362 - val_mae: 12.8107\n",
      "Epoch 27/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 906.0198 - mae: 20.6280 - val_loss: 370.6485 - val_mae: 12.9596\n",
      "Epoch 28/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 862.9448 - mae: 20.1924 - val_loss: 371.1059 - val_mae: 12.6836\n",
      "Epoch 29/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 938.8156 - mae: 20.7699 - val_loss: 361.6304 - val_mae: 12.5320\n",
      "Epoch 30/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 918.5416 - mae: 20.5754 - val_loss: 355.7313 - val_mae: 12.4796\n",
      "Epoch 31/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 873.4548 - mae: 20.2795 - val_loss: 351.0408 - val_mae: 12.3700\n",
      "Epoch 32/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 853.5654 - mae: 20.4610 - val_loss: 348.8362 - val_mae: 12.4019\n",
      "Epoch 33/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 847.5421 - mae: 20.0251 - val_loss: 340.3277 - val_mae: 12.2023\n",
      "Epoch 34/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 831.3594 - mae: 19.8491 - val_loss: 345.7207 - val_mae: 12.0811\n",
      "Epoch 35/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 851.4844 - mae: 20.1539 - val_loss: 333.7918 - val_mae: 11.9399\n",
      "Epoch 36/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 848.4410 - mae: 20.0870 - val_loss: 333.2053 - val_mae: 11.8626\n",
      "Epoch 37/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 834.1687 - mae: 19.8710 - val_loss: 337.9520 - val_mae: 12.0325\n",
      "Epoch 38/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 803.2673 - mae: 19.3562 - val_loss: 331.8390 - val_mae: 11.7773\n",
      "Epoch 39/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 831.8804 - mae: 19.8287 - val_loss: 326.0612 - val_mae: 11.6675\n",
      "Epoch 40/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 817.5534 - mae: 19.6644 - val_loss: 327.8951 - val_mae: 11.6948\n",
      "Epoch 41/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 837.4398 - mae: 19.7380 - val_loss: 317.5445 - val_mae: 11.5131\n",
      "Epoch 42/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 804.0882 - mae: 19.2944 - val_loss: 319.3721 - val_mae: 11.5058\n",
      "Epoch 43/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 855.9168 - mae: 19.8896 - val_loss: 313.7110 - val_mae: 11.5243\n",
      "Epoch 44/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 824.8752 - mae: 19.7645 - val_loss: 319.3185 - val_mae: 11.4505\n",
      "Epoch 45/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 826.4948 - mae: 19.5915 - val_loss: 320.2576 - val_mae: 11.4832\n",
      "Epoch 46/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 819.1713 - mae: 19.3974 - val_loss: 306.9750 - val_mae: 11.2531\n",
      "Epoch 47/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 847.5164 - mae: 19.9249 - val_loss: 313.5519 - val_mae: 11.3134\n",
      "Epoch 48/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 821.0148 - mae: 19.5096 - val_loss: 300.7979 - val_mae: 11.0867\n",
      "Epoch 49/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 811.9684 - mae: 19.4149 - val_loss: 298.9643 - val_mae: 11.1014\n",
      "Epoch 50/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 801.9508 - mae: 19.2105 - val_loss: 301.4729 - val_mae: 11.0671\n",
      "Epoch 51/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 809.7865 - mae: 19.2474 - val_loss: 299.1155 - val_mae: 11.1172\n",
      "Epoch 52/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 800.3531 - mae: 19.2693 - val_loss: 296.1170 - val_mae: 10.9566\n",
      "Epoch 53/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 785.3704 - mae: 19.1939 - val_loss: 312.2183 - val_mae: 11.4115\n",
      "Epoch 54/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 792.6629 - mae: 19.1983 - val_loss: 289.8321 - val_mae: 10.8094\n",
      "Epoch 55/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 787.6424 - mae: 19.2125 - val_loss: 293.8327 - val_mae: 10.8549\n",
      "Epoch 56/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 771.7516 - mae: 18.9680 - val_loss: 290.0069 - val_mae: 10.8631\n",
      "Epoch 57/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 798.0313 - mae: 19.0831 - val_loss: 298.7014 - val_mae: 11.0292\n",
      "Epoch 58/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 794.7999 - mae: 19.1797 - val_loss: 291.5085 - val_mae: 10.8661\n",
      "Epoch 59/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 770.6414 - mae: 18.8272 - val_loss: 288.4754 - val_mae: 10.7447\n",
      "Epoch 60/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 757.1013 - mae: 18.8498 - val_loss: 282.8027 - val_mae: 10.6283\n",
      "Epoch 61/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 779.2654 - mae: 18.9390 - val_loss: 285.0338 - val_mae: 10.7113\n",
      "Epoch 62/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 742.9314 - mae: 18.6006 - val_loss: 290.3730 - val_mae: 10.8734\n",
      "Epoch 63/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 781.0580 - mae: 18.8998 - val_loss: 275.7388 - val_mae: 10.4626\n",
      "Epoch 64/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 756.8799 - mae: 18.6903 - val_loss: 275.0332 - val_mae: 10.3684\n",
      "Epoch 65/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 759.3519 - mae: 18.7505 - val_loss: 293.6493 - val_mae: 10.8765\n",
      "Epoch 66/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 777.1928 - mae: 19.0514 - val_loss: 275.1102 - val_mae: 10.4714\n",
      "Epoch 67/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 743.0538 - mae: 18.4506 - val_loss: 268.3562 - val_mae: 10.3135\n",
      "Epoch 68/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 758.9672 - mae: 18.4084 - val_loss: 282.3000 - val_mae: 10.5501\n",
      "Epoch 69/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 772.5676 - mae: 18.8496 - val_loss: 262.8568 - val_mae: 10.1504\n",
      "Epoch 70/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 765.3749 - mae: 18.8044 - val_loss: 287.6920 - val_mae: 10.8518\n",
      "Epoch 71/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 721.6984 - mae: 18.3081 - val_loss: 278.8686 - val_mae: 10.6115\n",
      "Epoch 72/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 721.5161 - mae: 18.2261 - val_loss: 266.5530 - val_mae: 10.1981\n",
      "Epoch 73/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 725.3215 - mae: 18.1911 - val_loss: 277.4672 - val_mae: 10.6616\n",
      "Epoch 74/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 741.4532 - mae: 18.3148 - val_loss: 266.1944 - val_mae: 10.1208\n",
      "Epoch 75/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 704.9427 - mae: 18.0596 - val_loss: 277.6480 - val_mae: 10.4507\n",
      "Epoch 76/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 713.7949 - mae: 18.0680 - val_loss: 292.0346 - val_mae: 11.0134\n",
      "Epoch 77/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 746.6889 - mae: 18.3602 - val_loss: 278.2674 - val_mae: 10.4395\n",
      "Epoch 78/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 701.0599 - mae: 17.9723 - val_loss: 278.9958 - val_mae: 10.5837\n",
      "Epoch 79/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 688.9719 - mae: 17.8272 - val_loss: 263.0430 - val_mae: 10.1247\n",
      "Epoch 80/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 714.3283 - mae: 18.0114 - val_loss: 273.1620 - val_mae: 10.3507\n",
      "Epoch 81/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 676.5842 - mae: 17.6476 - val_loss: 292.6698 - val_mae: 10.9196\n",
      "Epoch 82/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 677.6320 - mae: 17.4829 - val_loss: 274.9000 - val_mae: 10.3866\n",
      "Epoch 83/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 670.2744 - mae: 17.4532 - val_loss: 278.7783 - val_mae: 10.6082\n",
      "Epoch 84/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 681.0800 - mae: 17.5836 - val_loss: 292.9142 - val_mae: 10.9661\n",
      "Epoch 85/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 644.9043 - mae: 17.2113 - val_loss: 290.1186 - val_mae: 10.9268\n",
      "Epoch 86/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 661.3859 - mae: 17.3809 - val_loss: 302.0359 - val_mae: 11.2072\n",
      "Epoch 87/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 623.0983 - mae: 16.8716 - val_loss: 275.5766 - val_mae: 10.4998\n",
      "Epoch 88/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 682.0294 - mae: 17.4324 - val_loss: 310.0265 - val_mae: 11.4659\n",
      "Epoch 89/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 671.4086 - mae: 17.3152 - val_loss: 320.4243 - val_mae: 11.7110\n",
      "Epoch 90/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 627.9641 - mae: 16.7889 - val_loss: 311.3368 - val_mae: 11.5049\n",
      "Epoch 91/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 638.7475 - mae: 17.0485 - val_loss: 338.7574 - val_mae: 12.1906\n",
      "Epoch 92/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 606.2099 - mae: 16.4821 - val_loss: 318.9304 - val_mae: 11.6504\n",
      "Epoch 93/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 607.8000 - mae: 16.5314 - val_loss: 359.1250 - val_mae: 12.5556\n",
      "Epoch 94/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 609.3136 - mae: 16.5926 - val_loss: 335.8256 - val_mae: 12.0864\n",
      "Epoch 95/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 593.4147 - mae: 16.4393 - val_loss: 334.8782 - val_mae: 12.0287\n",
      "Epoch 96/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 620.1446 - mae: 16.7022 - val_loss: 348.9968 - val_mae: 12.3637\n",
      "Epoch 97/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 596.5729 - mae: 16.4353 - val_loss: 330.4496 - val_mae: 11.9392\n",
      "Epoch 98/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 592.5783 - mae: 16.3363 - val_loss: 330.4241 - val_mae: 11.9494\n",
      "Epoch 99/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 585.5005 - mae: 16.1910 - val_loss: 375.9616 - val_mae: 13.0600\n",
      "Epoch 100/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 570.4424 - mae: 15.7929 - val_loss: 409.9944 - val_mae: 13.6677\n",
      "Epoch 101/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 583.9565 - mae: 15.9968 - val_loss: 405.0714 - val_mae: 13.5743\n",
      "Epoch 102/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 563.2172 - mae: 15.7848 - val_loss: 380.8008 - val_mae: 13.0227\n",
      "Epoch 103/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 552.3759 - mae: 15.7002 - val_loss: 446.7057 - val_mae: 14.4530\n",
      "Epoch 104/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 535.0753 - mae: 15.4074 - val_loss: 457.1151 - val_mae: 14.5618\n",
      "Epoch 105/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 558.0734 - mae: 15.5367 - val_loss: 457.2753 - val_mae: 14.6169\n",
      "Epoch 106/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 511.2052 - mae: 15.2102 - val_loss: 520.9294 - val_mae: 15.8534\n",
      "Epoch 107/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 511.4651 - mae: 15.0890 - val_loss: 535.0627 - val_mae: 16.0988\n",
      "Epoch 108/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 490.9769 - mae: 14.8367 - val_loss: 603.7531 - val_mae: 17.1846\n",
      "Epoch 109/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 485.3641 - mae: 14.6145 - val_loss: 585.7476 - val_mae: 16.9531\n",
      "Epoch 110/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 506.9034 - mae: 14.9506 - val_loss: 559.1472 - val_mae: 16.4608\n",
      "Epoch 111/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 467.7094 - mae: 14.4021 - val_loss: 703.4356 - val_mae: 18.7348\n",
      "Epoch 112/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 436.7807 - mae: 14.0002 - val_loss: 616.7536 - val_mae: 17.3181\n",
      "Epoch 113/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 433.4648 - mae: 13.9631 - val_loss: 775.5266 - val_mae: 19.8347\n",
      "Epoch 114/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 445.8036 - mae: 13.9841 - val_loss: 722.3718 - val_mae: 19.1599\n",
      "Epoch 115/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 425.7082 - mae: 13.6432 - val_loss: 724.2969 - val_mae: 19.0541\n",
      "Epoch 116/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 427.6513 - mae: 13.7455 - val_loss: 839.4940 - val_mae: 20.8521\n",
      "Epoch 117/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 407.9063 - mae: 13.3615 - val_loss: 1028.5551 - val_mae: 23.1977\n",
      "Epoch 118/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 393.1276 - mae: 13.0786 - val_loss: 898.4883 - val_mae: 21.5836\n",
      "Epoch 119/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 430.0276 - mae: 13.2273 - val_loss: 906.7504 - val_mae: 21.6245\n",
      "Epoch 120/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 379.1585 - mae: 12.9463 - val_loss: 912.3859 - val_mae: 21.8517\n",
      "Epoch 121/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 376.3869 - mae: 12.7845 - val_loss: 999.8617 - val_mae: 22.9808\n",
      "Epoch 122/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 388.4865 - mae: 12.7081 - val_loss: 1145.0498 - val_mae: 24.6694\n",
      "Epoch 123/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 353.2726 - mae: 12.4980 - val_loss: 1050.2473 - val_mae: 23.6103\n",
      "Epoch 124/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 374.1367 - mae: 12.6980 - val_loss: 1158.3325 - val_mae: 24.6807\n",
      "Epoch 125/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 384.3645 - mae: 12.7090 - val_loss: 1085.1761 - val_mae: 24.0633\n",
      "Epoch 126/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 349.7118 - mae: 12.2680 - val_loss: 1094.6594 - val_mae: 24.0174\n",
      "Epoch 127/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 351.6850 - mae: 12.2859 - val_loss: 1160.8876 - val_mae: 24.9080\n",
      "Epoch 128/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 354.6141 - mae: 12.2746 - val_loss: 1157.0834 - val_mae: 24.7960\n",
      "Epoch 129/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 340.2283 - mae: 12.0919 - val_loss: 1135.7225 - val_mae: 24.5509\n",
      "Epoch 130/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 341.7275 - mae: 11.9799 - val_loss: 1132.6547 - val_mae: 24.5031\n",
      "Epoch 131/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 368.3992 - mae: 12.2348 - val_loss: 1200.8973 - val_mae: 25.5285\n",
      "Epoch 132/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 337.7601 - mae: 12.0730 - val_loss: 1038.5841 - val_mae: 23.6094\n",
      "Epoch 133/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 325.8266 - mae: 11.7898 - val_loss: 1153.4528 - val_mae: 24.7817\n",
      "Epoch 134/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 311.7854 - mae: 11.7100 - val_loss: 1158.9852 - val_mae: 24.9292\n",
      "Epoch 135/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 335.0044 - mae: 11.8332 - val_loss: 1059.0167 - val_mae: 23.9638\n",
      "Epoch 136/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 339.4481 - mae: 11.9213 - val_loss: 1082.4219 - val_mae: 24.2227\n",
      "Epoch 137/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 308.0249 - mae: 11.5558 - val_loss: 1072.1490 - val_mae: 23.9805\n",
      "Epoch 138/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 322.8884 - mae: 11.8064 - val_loss: 1156.5566 - val_mae: 25.1251\n",
      "Epoch 139/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 330.8515 - mae: 11.7243 - val_loss: 1090.0151 - val_mae: 24.1270\n",
      "Epoch 140/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 327.4222 - mae: 11.6145 - val_loss: 1076.0111 - val_mae: 24.2906\n",
      "Epoch 141/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 313.4675 - mae: 11.4649 - val_loss: 1078.3417 - val_mae: 24.2437\n",
      "Epoch 142/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 321.6791 - mae: 11.4391 - val_loss: 1123.3521 - val_mae: 24.6977\n",
      "Epoch 143/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 311.5727 - mae: 11.4245 - val_loss: 1064.4333 - val_mae: 24.0564\n",
      "Epoch 144/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 309.7332 - mae: 11.3845 - val_loss: 1123.4042 - val_mae: 24.8155\n",
      "Epoch 145/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 300.6514 - mae: 11.2070 - val_loss: 1030.2329 - val_mae: 23.7041\n",
      "Epoch 146/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 317.3188 - mae: 11.4442 - val_loss: 1122.3217 - val_mae: 24.7341\n",
      "Epoch 147/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 304.4441 - mae: 11.1604 - val_loss: 1072.8730 - val_mae: 24.3682\n",
      "Epoch 148/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 291.5697 - mae: 11.0877 - val_loss: 1051.5834 - val_mae: 24.1196\n",
      "Epoch 149/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 306.4246 - mae: 11.1640 - val_loss: 1103.4426 - val_mae: 24.6937\n",
      "Epoch 150/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 304.0637 - mae: 11.1371 - val_loss: 1137.8015 - val_mae: 25.1624\n",
      "Epoch 151/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 295.4974 - mae: 11.1668 - val_loss: 1131.4701 - val_mae: 25.1030\n",
      "Epoch 152/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 294.7190 - mae: 11.0367 - val_loss: 1182.7570 - val_mae: 25.5438\n",
      "Epoch 153/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 276.5930 - mae: 10.8120 - val_loss: 1150.6307 - val_mae: 25.2716\n",
      "Epoch 154/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 299.4294 - mae: 10.9055 - val_loss: 1158.6019 - val_mae: 25.3541\n",
      "Epoch 155/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 284.9332 - mae: 10.8407 - val_loss: 1160.0294 - val_mae: 25.3091\n",
      "Epoch 156/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 285.9353 - mae: 10.8750 - val_loss: 1153.5887 - val_mae: 25.4468\n",
      "Epoch 157/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 282.9272 - mae: 10.6314 - val_loss: 1078.5747 - val_mae: 24.3260\n",
      "Epoch 158/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 281.2215 - mae: 10.8405 - val_loss: 1090.4867 - val_mae: 24.6842\n",
      "Epoch 159/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 275.9030 - mae: 10.6029 - val_loss: 1073.5266 - val_mae: 24.3820\n",
      "Epoch 160/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 286.1862 - mae: 10.6426 - val_loss: 1062.0922 - val_mae: 24.2725\n",
      "Epoch 161/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 292.6252 - mae: 10.9099 - val_loss: 1096.5879 - val_mae: 24.5939\n",
      "Epoch 162/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 274.2711 - mae: 10.5646 - val_loss: 1094.9249 - val_mae: 24.6953\n",
      "Epoch 163/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 264.5964 - mae: 10.3753 - val_loss: 1111.3324 - val_mae: 24.9007\n",
      "Epoch 164/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 284.5762 - mae: 10.6928 - val_loss: 1114.6259 - val_mae: 24.7679\n",
      "Epoch 165/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 268.7464 - mae: 10.4304 - val_loss: 1070.8563 - val_mae: 24.2138\n",
      "Epoch 166/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 265.2446 - mae: 10.4627 - val_loss: 1084.5482 - val_mae: 24.4708\n",
      "Epoch 167/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 267.1100 - mae: 10.4392 - val_loss: 1099.1270 - val_mae: 24.5577\n",
      "Epoch 168/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 267.4246 - mae: 10.3865 - val_loss: 1150.2463 - val_mae: 25.3457\n",
      "Epoch 169/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 277.7340 - mae: 10.4046 - val_loss: 1011.0448 - val_mae: 23.7382\n",
      "Epoch 170/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 257.3681 - mae: 10.1328 - val_loss: 1083.4338 - val_mae: 24.6014\n",
      "Epoch 171/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 265.0043 - mae: 10.3692 - val_loss: 1046.3582 - val_mae: 24.1291\n",
      "Epoch 172/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 278.8962 - mae: 10.5127 - val_loss: 1213.9701 - val_mae: 25.9878\n",
      "Epoch 173/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 250.1794 - mae: 10.2057 - val_loss: 1048.1810 - val_mae: 24.1672\n",
      "Epoch 174/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 250.5554 - mae: 10.0836 - val_loss: 1070.9246 - val_mae: 24.2944\n",
      "Epoch 175/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 260.1712 - mae: 10.2569 - val_loss: 1058.4492 - val_mae: 24.2590\n",
      "Epoch 176/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 268.5641 - mae: 10.3765 - val_loss: 1061.5566 - val_mae: 24.1904\n",
      "Epoch 177/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 276.5501 - mae: 10.2951 - val_loss: 1064.3005 - val_mae: 24.4001\n",
      "Epoch 178/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 238.5257 - mae: 9.9370 - val_loss: 1135.5729 - val_mae: 24.9415\n",
      "Epoch 179/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 254.0344 - mae: 10.0407 - val_loss: 1156.0073 - val_mae: 25.3675\n",
      "Epoch 180/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 260.0781 - mae: 10.1763 - val_loss: 1044.5833 - val_mae: 24.1469\n",
      "Epoch 181/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 249.3226 - mae: 9.9529 - val_loss: 1093.9449 - val_mae: 24.6309\n",
      "Epoch 182/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 259.4820 - mae: 10.1157 - val_loss: 1087.9814 - val_mae: 24.4131\n",
      "Epoch 183/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 248.6966 - mae: 9.9626 - val_loss: 1130.2152 - val_mae: 24.9381\n",
      "Epoch 184/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 250.5321 - mae: 9.9756 - val_loss: 1068.0789 - val_mae: 24.3375\n",
      "Epoch 185/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 247.7949 - mae: 9.8568 - val_loss: 1076.9059 - val_mae: 24.3200\n",
      "Epoch 186/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 253.3182 - mae: 10.0248 - val_loss: 1114.3340 - val_mae: 24.7015\n",
      "Epoch 187/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 236.0004 - mae: 9.7756 - val_loss: 1100.4905 - val_mae: 24.6323\n",
      "Epoch 188/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 250.4867 - mae: 9.9175 - val_loss: 1088.9277 - val_mae: 24.8148\n",
      "Epoch 189/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 246.5464 - mae: 9.7769 - val_loss: 1181.8759 - val_mae: 25.6683\n",
      "Epoch 190/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 250.3710 - mae: 9.8297 - val_loss: 1082.1180 - val_mae: 24.3901\n",
      "Epoch 191/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 233.6719 - mae: 9.6815 - val_loss: 1084.2594 - val_mae: 24.5708\n",
      "Epoch 192/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 233.1074 - mae: 9.6101 - val_loss: 1152.3752 - val_mae: 25.2865\n",
      "Epoch 193/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 226.1221 - mae: 9.6235 - val_loss: 1129.5890 - val_mae: 25.0880\n",
      "Epoch 194/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 240.0710 - mae: 9.7347 - val_loss: 1147.7288 - val_mae: 25.2581\n",
      "Epoch 195/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 236.6768 - mae: 9.5947 - val_loss: 1153.3196 - val_mae: 25.5464\n",
      "Epoch 196/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 235.1549 - mae: 9.5332 - val_loss: 1134.9071 - val_mae: 25.2131\n",
      "Epoch 197/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 227.7814 - mae: 9.5279 - val_loss: 1035.1956 - val_mae: 23.9200\n",
      "Epoch 198/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 235.1605 - mae: 9.5783 - val_loss: 1220.3762 - val_mae: 26.1269\n",
      "Epoch 199/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 237.5108 - mae: 9.5760 - val_loss: 1149.0957 - val_mae: 25.0963\n",
      "Epoch 200/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 239.7171 - mae: 9.5486 - val_loss: 1095.9216 - val_mae: 24.5942\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "🔹 Improved LSTM Model - RMSE: 33.08\n",
      "🔹 Improved LSTM Model - R² Score: 0.90\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = X_train_scaled.reshape(X_train_scaled.shape[0], 1, X_train_scaled.shape[1])\n",
    "X_test_scaled = X_test_scaled.reshape(X_test_scaled.shape[0], 1, X_test_scaled.shape[1])\n",
    "\n",
    "model = Sequential([\n",
    "    Bidirectional(LSTM(64, activation='relu', kernel_regularizer=l2(0.001), \n",
    "                        input_shape=(X_train_scaled.shape[1], X_train_scaled.shape[2]), return_sequences=True)),\n",
    "    Dropout(0.2), \n",
    "    LSTM(32, activation='relu', kernel_regularizer=l2(0.001), return_sequences=False),\n",
    "    Dropout(0.2),  \n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1, activation='linear')  \n",
    "])\n",
    "\n",
    "optimizer = Adam(learning_rate=0.0001, clipnorm=0.5)  \n",
    "model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train_clean,\n",
    "    validation_data=(X_test_scaled, y_test),\n",
    "    epochs=200,  \n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "y_pred_nn = model.predict(X_test_scaled)\n",
    "\n",
    "rmse_nn = np.sqrt(mean_squared_error(y_test, y_pred_nn))\n",
    "r2_nn = r2_score(y_test, y_pred_nn)\n",
    "\n",
    "print(f\" Improved LSTM Model - RMSE: {rmse_nn:.2f}\")\n",
    "print(f\" Improved LSTM Model - R² Score: {r2_nn:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/me.pranesh/Desktop/new clean/.venv/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 16026.8760 - mae: 89.9005 - val_loss: 473.3083 - val_mae: 14.8315\n",
      "Epoch 2/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 728.1965 - mae: 19.1057 - val_loss: 355.9323 - val_mae: 12.7564\n",
      "Epoch 3/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 608.9411 - mae: 17.5169 - val_loss: 327.1999 - val_mae: 12.0257\n",
      "Epoch 4/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 570.8800 - mae: 16.3688 - val_loss: 341.5822 - val_mae: 12.8819\n",
      "Epoch 5/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 537.1350 - mae: 15.7689 - val_loss: 271.1088 - val_mae: 10.5245\n",
      "Epoch 6/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 477.3278 - mae: 14.8282 - val_loss: 241.0396 - val_mae: 9.6102\n",
      "Epoch 7/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 452.8282 - mae: 14.4045 - val_loss: 233.0960 - val_mae: 9.5375\n",
      "Epoch 8/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 437.7289 - mae: 13.9013 - val_loss: 221.6906 - val_mae: 9.0389\n",
      "Epoch 9/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 395.0334 - mae: 13.2028 - val_loss: 203.1200 - val_mae: 8.3801\n",
      "Epoch 10/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 393.1436 - mae: 13.1797 - val_loss: 200.3944 - val_mae: 8.3188\n",
      "Epoch 11/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 375.5858 - mae: 12.8980 - val_loss: 202.5710 - val_mae: 8.5717\n",
      "Epoch 12/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 366.8845 - mae: 12.7648 - val_loss: 167.0724 - val_mae: 7.2734\n",
      "Epoch 13/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 322.5890 - mae: 11.8701 - val_loss: 172.3033 - val_mae: 7.5567\n",
      "Epoch 14/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 297.9093 - mae: 11.3666 - val_loss: 185.5505 - val_mae: 8.3870\n",
      "Epoch 15/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 292.8667 - mae: 11.2664 - val_loss: 151.0648 - val_mae: 6.7304\n",
      "Epoch 16/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 281.8285 - mae: 10.9025 - val_loss: 152.1564 - val_mae: 6.4020\n",
      "Epoch 17/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 263.5930 - mae: 10.6620 - val_loss: 192.0409 - val_mae: 8.0027\n",
      "Epoch 18/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 260.3180 - mae: 10.5135 - val_loss: 148.9689 - val_mae: 6.7682\n",
      "Epoch 19/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 231.5649 - mae: 10.1339 - val_loss: 148.0663 - val_mae: 6.8545\n",
      "Epoch 20/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 234.8329 - mae: 10.0597 - val_loss: 141.9474 - val_mae: 6.2318\n",
      "Epoch 21/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 237.9625 - mae: 9.9251 - val_loss: 130.3117 - val_mae: 5.8193\n",
      "Epoch 22/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 229.4809 - mae: 9.8120 - val_loss: 136.1538 - val_mae: 6.2224\n",
      "Epoch 23/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 212.2991 - mae: 9.4170 - val_loss: 133.4434 - val_mae: 5.7836\n",
      "Epoch 24/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 216.6091 - mae: 9.3500 - val_loss: 132.8984 - val_mae: 5.9807\n",
      "Epoch 25/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 210.9795 - mae: 9.2808 - val_loss: 125.0483 - val_mae: 5.3541\n",
      "Epoch 26/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 201.6058 - mae: 9.0468 - val_loss: 134.7667 - val_mae: 5.5892\n",
      "Epoch 27/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 204.2561 - mae: 8.9845 - val_loss: 126.3419 - val_mae: 5.4058\n",
      "Epoch 28/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 185.3770 - mae: 8.6510 - val_loss: 131.2545 - val_mae: 5.7554\n",
      "Epoch 29/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 199.4694 - mae: 8.7559 - val_loss: 119.4606 - val_mae: 4.9742\n",
      "Epoch 30/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 198.7699 - mae: 8.6473 - val_loss: 127.2687 - val_mae: 5.7264\n",
      "Epoch 31/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 194.9277 - mae: 8.5015 - val_loss: 125.5288 - val_mae: 5.0203\n",
      "Epoch 32/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 184.7452 - mae: 8.4030 - val_loss: 122.1895 - val_mae: 4.8866\n",
      "Epoch 33/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 182.9585 - mae: 8.2587 - val_loss: 121.4850 - val_mae: 5.3734\n",
      "Epoch 34/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 184.1559 - mae: 8.1045 - val_loss: 163.0107 - val_mae: 7.3078\n",
      "Epoch 35/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 189.0513 - mae: 8.2307 - val_loss: 119.6489 - val_mae: 5.0682\n",
      "Epoch 36/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 179.6436 - mae: 8.0606 - val_loss: 121.3991 - val_mae: 4.8384\n",
      "Epoch 37/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 156.5278 - mae: 7.7359 - val_loss: 120.9727 - val_mae: 4.8564\n",
      "Epoch 38/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 185.8614 - mae: 7.9615 - val_loss: 119.4390 - val_mae: 5.0363\n",
      "Epoch 39/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 184.9622 - mae: 7.7885 - val_loss: 117.7793 - val_mae: 4.7834\n",
      "Epoch 40/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 169.2483 - mae: 7.6788 - val_loss: 164.3023 - val_mae: 6.8353\n",
      "Epoch 41/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 163.5843 - mae: 7.6377 - val_loss: 129.6333 - val_mae: 5.6433\n",
      "Epoch 42/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 157.9872 - mae: 7.4048 - val_loss: 114.8144 - val_mae: 4.8192\n",
      "Epoch 43/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 152.9140 - mae: 7.5588 - val_loss: 127.1880 - val_mae: 5.0769\n",
      "Epoch 44/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 162.4909 - mae: 7.3525 - val_loss: 116.8726 - val_mae: 4.6430\n",
      "Epoch 45/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 155.3278 - mae: 7.1866 - val_loss: 137.8759 - val_mae: 5.8919\n",
      "Epoch 46/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 143.1900 - mae: 7.1356 - val_loss: 114.3014 - val_mae: 5.0043\n",
      "Epoch 47/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 172.4398 - mae: 7.2926 - val_loss: 130.1804 - val_mae: 5.4225\n",
      "Epoch 48/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 141.8962 - mae: 6.9295 - val_loss: 115.0676 - val_mae: 4.7433\n",
      "Epoch 49/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 153.5746 - mae: 7.0946 - val_loss: 115.6129 - val_mae: 4.3091\n",
      "Epoch 50/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 138.8783 - mae: 6.9497 - val_loss: 125.5966 - val_mae: 5.4948\n",
      "Epoch 51/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 159.5743 - mae: 6.9166 - val_loss: 112.6957 - val_mae: 4.4027\n",
      "Epoch 52/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 136.5151 - mae: 6.7369 - val_loss: 127.5950 - val_mae: 5.7590\n",
      "Epoch 53/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 134.4945 - mae: 6.7719 - val_loss: 113.3446 - val_mae: 4.4547\n",
      "Epoch 54/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 132.8043 - mae: 6.6915 - val_loss: 119.6513 - val_mae: 4.7807\n",
      "Epoch 55/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 133.6336 - mae: 6.6513 - val_loss: 111.2900 - val_mae: 4.4053\n",
      "Epoch 56/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 122.4360 - mae: 6.5622 - val_loss: 106.9428 - val_mae: 4.0704\n",
      "Epoch 57/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 137.8079 - mae: 6.6021 - val_loss: 113.0560 - val_mae: 4.1824\n",
      "Epoch 58/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 122.8449 - mae: 6.5011 - val_loss: 122.3615 - val_mae: 4.5151\n",
      "Epoch 59/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 121.0653 - mae: 6.4013 - val_loss: 113.3331 - val_mae: 4.5270\n",
      "Epoch 60/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 122.0676 - mae: 6.4244 - val_loss: 113.2855 - val_mae: 4.1992\n",
      "Epoch 61/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 117.9687 - mae: 6.2779 - val_loss: 121.3035 - val_mae: 5.1100\n",
      "Epoch 62/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 127.3950 - mae: 6.4301 - val_loss: 124.3871 - val_mae: 5.3079\n",
      "Epoch 63/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 131.9585 - mae: 6.2360 - val_loss: 107.8750 - val_mae: 4.4569\n",
      "Epoch 64/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 131.4879 - mae: 6.3428 - val_loss: 112.1680 - val_mae: 4.5352\n",
      "Epoch 65/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 120.9258 - mae: 6.1937 - val_loss: 138.3009 - val_mae: 6.3234\n",
      "Epoch 66/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 123.5926 - mae: 6.1761 - val_loss: 115.4680 - val_mae: 4.6917\n",
      "Epoch 67/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 109.3406 - mae: 6.0309 - val_loss: 108.1723 - val_mae: 4.2487\n",
      "Epoch 68/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 108.7580 - mae: 6.0153 - val_loss: 118.2355 - val_mae: 4.5562\n",
      "Epoch 69/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 126.5947 - mae: 6.1394 - val_loss: 119.3986 - val_mae: 4.8818\n",
      "Epoch 70/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 117.2901 - mae: 5.9987 - val_loss: 114.5942 - val_mae: 4.6371\n",
      "Epoch 71/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 119.5973 - mae: 6.0326 - val_loss: 111.5691 - val_mae: 3.9999\n",
      "Epoch 72/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 123.9627 - mae: 6.0590 - val_loss: 109.7136 - val_mae: 4.2029\n",
      "Epoch 73/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 125.1701 - mae: 5.9865 - val_loss: 109.0743 - val_mae: 3.8304\n",
      "Epoch 74/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 116.9008 - mae: 5.8749 - val_loss: 111.9374 - val_mae: 4.5116\n",
      "Epoch 75/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 107.2786 - mae: 5.7823 - val_loss: 116.5769 - val_mae: 4.5043\n",
      "Epoch 76/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 109.2778 - mae: 5.8222 - val_loss: 121.5598 - val_mae: 4.8796\n",
      "Epoch 77/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 124.4572 - mae: 5.8658 - val_loss: 107.9657 - val_mae: 4.2033\n",
      "Epoch 78/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 121.5723 - mae: 5.8507 - val_loss: 105.0997 - val_mae: 3.7891\n",
      "Epoch 79/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 114.1584 - mae: 5.8337 - val_loss: 109.5113 - val_mae: 4.1597\n",
      "Epoch 80/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 112.5238 - mae: 5.8720 - val_loss: 106.2725 - val_mae: 4.0801\n",
      "Epoch 81/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 124.5254 - mae: 5.8103 - val_loss: 111.0620 - val_mae: 4.3356\n",
      "Epoch 82/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 105.9933 - mae: 5.6729 - val_loss: 107.4784 - val_mae: 3.8410\n",
      "Epoch 83/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 92.9093 - mae: 5.5154 - val_loss: 116.8610 - val_mae: 4.4170\n",
      "Epoch 84/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 102.4011 - mae: 5.5441 - val_loss: 116.6338 - val_mae: 4.0021\n",
      "Epoch 85/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 102.9907 - mae: 5.5361 - val_loss: 107.5234 - val_mae: 3.7854\n",
      "Epoch 86/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 89.2070 - mae: 5.4660 - val_loss: 107.0383 - val_mae: 4.0186\n",
      "Epoch 87/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 95.0799 - mae: 5.4672 - val_loss: 110.6406 - val_mae: 3.9242\n",
      "Epoch 88/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 100.9706 - mae: 5.5182 - val_loss: 110.1243 - val_mae: 4.2149\n",
      "Epoch 89/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 86.6440 - mae: 5.3253 - val_loss: 105.0755 - val_mae: 3.6819\n",
      "Epoch 90/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 115.0889 - mae: 5.4718 - val_loss: 107.5963 - val_mae: 3.8919\n",
      "Epoch 91/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 115.7934 - mae: 5.5387 - val_loss: 104.4009 - val_mae: 3.9016\n",
      "Epoch 92/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 94.9571 - mae: 5.3933 - val_loss: 110.0886 - val_mae: 4.2324\n",
      "Epoch 93/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 106.4167 - mae: 5.4583 - val_loss: 114.2086 - val_mae: 4.5983\n",
      "Epoch 94/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 95.8633 - mae: 5.2883 - val_loss: 107.3647 - val_mae: 3.6137\n",
      "Epoch 95/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 113.7151 - mae: 5.4230 - val_loss: 108.0223 - val_mae: 4.1126\n",
      "Epoch 96/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 93.2859 - mae: 5.3013 - val_loss: 110.9229 - val_mae: 3.9643\n",
      "Epoch 97/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 99.0957 - mae: 5.3108 - val_loss: 111.0912 - val_mae: 4.1466\n",
      "Epoch 98/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 93.5560 - mae: 5.2180 - val_loss: 108.2962 - val_mae: 3.9190\n",
      "Epoch 99/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 104.5367 - mae: 5.2722 - val_loss: 112.9379 - val_mae: 4.0876\n",
      "Epoch 100/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 89.3330 - mae: 5.2083 - val_loss: 114.6065 - val_mae: 4.4120\n",
      "Epoch 101/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 191.3862 - mae: 5.3927 - val_loss: 106.1178 - val_mae: 3.6340\n",
      "Epoch 102/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 108.2709 - mae: 5.3155 - val_loss: 111.7808 - val_mae: 4.2272\n",
      "Epoch 103/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 91.6173 - mae: 5.0899 - val_loss: 106.7990 - val_mae: 3.5893\n",
      "Epoch 104/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 86.7093 - mae: 5.1565 - val_loss: 123.1533 - val_mae: 4.8401\n",
      "Epoch 105/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 84.8261 - mae: 4.9978 - val_loss: 108.8619 - val_mae: 3.7416\n",
      "Epoch 106/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 97.4108 - mae: 5.1151 - val_loss: 109.8036 - val_mae: 4.1153\n",
      "Epoch 107/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 85.1429 - mae: 5.0527 - val_loss: 109.7859 - val_mae: 3.9847\n",
      "Epoch 108/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 84.7137 - mae: 5.0492 - val_loss: 109.6757 - val_mae: 3.5890\n",
      "Epoch 109/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 85.7564 - mae: 4.9568 - val_loss: 111.5638 - val_mae: 3.9427\n",
      "Epoch 110/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 103.0904 - mae: 5.1141 - val_loss: 111.9121 - val_mae: 4.1651\n",
      "Epoch 111/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 94.3062 - mae: 4.9927 - val_loss: 109.4179 - val_mae: 4.0927\n",
      "Epoch 112/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 70.3746 - mae: 4.7654 - val_loss: 109.2378 - val_mae: 4.1249\n",
      "Epoch 113/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 94.0590 - mae: 5.0412 - val_loss: 112.7214 - val_mae: 3.5031\n",
      "Epoch 114/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 76.2545 - mae: 4.8459 - val_loss: 108.8518 - val_mae: 3.3691\n",
      "Epoch 115/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 93.1882 - mae: 4.9333 - val_loss: 115.1203 - val_mae: 4.1716\n",
      "Epoch 116/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 88.2103 - mae: 4.7898 - val_loss: 108.9125 - val_mae: 3.5347\n",
      "Epoch 117/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 79.0564 - mae: 4.7937 - val_loss: 106.5989 - val_mae: 3.8613\n",
      "Epoch 118/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 87.2696 - mae: 4.8728 - val_loss: 109.8164 - val_mae: 3.5678\n",
      "Epoch 119/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 77.8941 - mae: 4.7379 - val_loss: 117.4831 - val_mae: 4.2033\n",
      "Epoch 120/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 86.5668 - mae: 4.8357 - val_loss: 106.8646 - val_mae: 3.9455\n",
      "Epoch 121/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 82.8401 - mae: 4.8121 - val_loss: 118.4682 - val_mae: 4.3796\n",
      "Epoch 122/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 102.1829 - mae: 4.8245 - val_loss: 113.7433 - val_mae: 4.1833\n",
      "Epoch 123/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 75.3137 - mae: 4.6223 - val_loss: 115.6949 - val_mae: 4.1104\n",
      "Epoch 124/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 83.6261 - mae: 4.7301 - val_loss: 105.5099 - val_mae: 3.4422\n",
      "Epoch 125/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 128.2368 - mae: 4.8154 - val_loss: 106.3169 - val_mae: 3.8234\n",
      "Epoch 126/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 83.9144 - mae: 4.6707 - val_loss: 117.2063 - val_mae: 4.2460\n",
      "Epoch 127/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 79.6810 - mae: 4.6400 - val_loss: 112.2175 - val_mae: 3.7014\n",
      "Epoch 128/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 78.4216 - mae: 4.6709 - val_loss: 109.5994 - val_mae: 3.5650\n",
      "Epoch 129/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 86.2083 - mae: 4.6506 - val_loss: 109.3233 - val_mae: 3.7559\n",
      "Epoch 130/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 86.7600 - mae: 4.5904 - val_loss: 107.2182 - val_mae: 3.7028\n",
      "Epoch 131/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 76.5253 - mae: 4.5879 - val_loss: 106.7058 - val_mae: 3.4458\n",
      "Epoch 132/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 80.8924 - mae: 4.6789 - val_loss: 117.3295 - val_mae: 4.5377\n",
      "Epoch 133/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 81.8810 - mae: 4.6533 - val_loss: 111.4966 - val_mae: 3.4569\n",
      "Epoch 134/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 75.7941 - mae: 4.6057 - val_loss: 109.9373 - val_mae: 3.8477\n",
      "Epoch 135/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 82.9959 - mae: 4.6245 - val_loss: 110.9420 - val_mae: 3.5175\n",
      "Epoch 136/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 86.5301 - mae: 4.6477 - val_loss: 109.7778 - val_mae: 3.2743\n",
      "Epoch 137/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 86.8342 - mae: 4.5808 - val_loss: 115.2404 - val_mae: 4.2336\n",
      "Epoch 138/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 85.4244 - mae: 4.5675 - val_loss: 108.0701 - val_mae: 3.7698\n",
      "Epoch 139/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 119.6318 - mae: 4.7173 - val_loss: 111.8708 - val_mae: 3.7609\n",
      "Epoch 140/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 81.1924 - mae: 4.5669 - val_loss: 105.9529 - val_mae: 3.5861\n",
      "Epoch 141/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 82.5936 - mae: 4.6359 - val_loss: 105.1095 - val_mae: 3.7303\n",
      "Epoch 142/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 77.3749 - mae: 4.4235 - val_loss: 109.4882 - val_mae: 3.7677\n",
      "Epoch 143/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 71.5847 - mae: 4.4667 - val_loss: 111.6692 - val_mae: 4.4400\n",
      "Epoch 144/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 76.2203 - mae: 4.4844 - val_loss: 110.2318 - val_mae: 3.6056\n",
      "Epoch 145/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 82.9991 - mae: 4.4414 - val_loss: 124.1628 - val_mae: 4.4934\n",
      "Epoch 146/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 72.2840 - mae: 4.4241 - val_loss: 100.4036 - val_mae: 3.3923\n",
      "Epoch 147/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 72.4417 - mae: 4.4170 - val_loss: 109.8233 - val_mae: 3.7006\n",
      "Epoch 148/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 69.2668 - mae: 4.3841 - val_loss: 112.0544 - val_mae: 3.5162\n",
      "Epoch 149/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 91.8793 - mae: 4.4613 - val_loss: 109.9918 - val_mae: 3.5319\n",
      "Epoch 150/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 78.4201 - mae: 4.4738 - val_loss: 120.7326 - val_mae: 4.4795\n",
      "Epoch 151/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 83.7663 - mae: 4.4861 - val_loss: 113.4505 - val_mae: 3.8742\n",
      "Epoch 152/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 106.3317 - mae: 4.4925 - val_loss: 111.0102 - val_mae: 3.5848\n",
      "Epoch 153/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 80.7795 - mae: 4.4650 - val_loss: 107.9023 - val_mae: 3.4563\n",
      "Epoch 154/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 74.6904 - mae: 4.3861 - val_loss: 105.5580 - val_mae: 3.5465\n",
      "Epoch 155/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 95.7868 - mae: 4.5009 - val_loss: 116.3964 - val_mae: 3.9919\n",
      "Epoch 156/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 79.8613 - mae: 4.4293 - val_loss: 105.4025 - val_mae: 3.3482\n",
      "Epoch 157/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 71.0645 - mae: 4.2766 - val_loss: 108.6757 - val_mae: 3.2598\n",
      "Epoch 158/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 80.8725 - mae: 4.4037 - val_loss: 108.6681 - val_mae: 3.7486\n",
      "Epoch 159/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 70.7371 - mae: 4.3026 - val_loss: 111.0980 - val_mae: 3.8557\n",
      "Epoch 160/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 75.9089 - mae: 4.3807 - val_loss: 108.7666 - val_mae: 3.4775\n",
      "Epoch 161/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 81.0262 - mae: 4.3262 - val_loss: 111.4347 - val_mae: 3.5258\n",
      "Epoch 162/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 74.8830 - mae: 4.3293 - val_loss: 111.2097 - val_mae: 3.5453\n",
      "Epoch 163/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 78.4507 - mae: 4.3245 - val_loss: 113.2208 - val_mae: 3.6516\n",
      "Epoch 164/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 77.0972 - mae: 4.3874 - val_loss: 114.8363 - val_mae: 3.8535\n",
      "Epoch 165/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 70.9097 - mae: 4.3368 - val_loss: 107.7151 - val_mae: 3.3308\n",
      "Epoch 166/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 73.1744 - mae: 4.3641 - val_loss: 104.5257 - val_mae: 3.3678\n",
      "Epoch 167/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 72.9541 - mae: 4.2736 - val_loss: 110.2029 - val_mae: 3.2399\n",
      "Epoch 168/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 72.6152 - mae: 4.2913 - val_loss: 112.6234 - val_mae: 3.7226\n",
      "Epoch 169/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 66.7381 - mae: 4.2611 - val_loss: 108.3533 - val_mae: 3.2549\n",
      "Epoch 170/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 76.8742 - mae: 4.2528 - val_loss: 106.0464 - val_mae: 3.5619\n",
      "Epoch 171/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 62.1038 - mae: 4.2167 - val_loss: 110.4407 - val_mae: 3.9574\n",
      "Epoch 172/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 72.8456 - mae: 4.2152 - val_loss: 108.1670 - val_mae: 3.6175\n",
      "Epoch 173/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 65.1206 - mae: 4.1870 - val_loss: 111.4004 - val_mae: 3.8277\n",
      "Epoch 174/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 71.2021 - mae: 4.2441 - val_loss: 105.9062 - val_mae: 3.2153\n",
      "Epoch 175/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 66.6160 - mae: 4.1557 - val_loss: 109.3532 - val_mae: 3.4126\n",
      "Epoch 176/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 74.2459 - mae: 4.2869 - val_loss: 106.5046 - val_mae: 3.5767\n",
      "Epoch 177/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 60.5427 - mae: 4.1002 - val_loss: 109.2829 - val_mae: 3.4196\n",
      "Epoch 178/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 82.0520 - mae: 4.3659 - val_loss: 106.0388 - val_mae: 3.2398\n",
      "Epoch 179/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 61.4008 - mae: 4.1169 - val_loss: 110.8438 - val_mae: 3.5816\n",
      "Epoch 180/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 68.7460 - mae: 4.1346 - val_loss: 107.9198 - val_mae: 3.5435\n",
      "Epoch 181/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 66.2760 - mae: 4.1567 - val_loss: 113.7946 - val_mae: 3.7967\n",
      "Epoch 182/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 69.0447 - mae: 4.1899 - val_loss: 106.8656 - val_mae: 3.3366\n",
      "Epoch 183/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 72.6989 - mae: 4.2170 - val_loss: 109.7997 - val_mae: 3.5497\n",
      "Epoch 184/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 79.7109 - mae: 4.2212 - val_loss: 107.1060 - val_mae: 3.5926\n",
      "Epoch 185/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 73.1407 - mae: 4.1284 - val_loss: 114.6827 - val_mae: 3.6883\n",
      "Epoch 186/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 65.9766 - mae: 4.1173 - val_loss: 105.6689 - val_mae: 3.3878\n",
      "Epoch 187/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 70.0830 - mae: 4.1103 - val_loss: 104.1939 - val_mae: 3.3625\n",
      "Epoch 188/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 70.0662 - mae: 4.1289 - val_loss: 109.6740 - val_mae: 3.6109\n",
      "Epoch 189/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 71.8962 - mae: 4.1855 - val_loss: 106.0254 - val_mae: 3.2340\n",
      "Epoch 190/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 93.4344 - mae: 4.1878 - val_loss: 110.4624 - val_mae: 3.8671\n",
      "Epoch 191/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 73.5042 - mae: 4.0990 - val_loss: 111.4032 - val_mae: 3.6793\n",
      "Epoch 192/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 73.1796 - mae: 4.1284 - val_loss: 110.5620 - val_mae: 3.4379\n",
      "Epoch 193/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 63.4770 - mae: 4.0610 - val_loss: 105.3588 - val_mae: 3.4048\n",
      "Epoch 194/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 67.9342 - mae: 4.0179 - val_loss: 103.4773 - val_mae: 3.5808\n",
      "Epoch 195/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 63.6430 - mae: 4.0723 - val_loss: 105.3547 - val_mae: 3.3372\n",
      "Epoch 196/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 70.6150 - mae: 4.1418 - val_loss: 109.8054 - val_mae: 3.4208\n",
      "Epoch 197/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 67.4778 - mae: 4.1233 - val_loss: 113.9099 - val_mae: 3.4651\n",
      "Epoch 198/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 71.5809 - mae: 4.1316 - val_loss: 114.2227 - val_mae: 3.6741\n",
      "Epoch 199/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 69.9082 - mae: 4.1106 - val_loss: 110.9420 - val_mae: 3.4002\n",
      "Epoch 200/200\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 81.9507 - mae: 4.1334 - val_loss: 109.5888 - val_mae: 3.2918\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 GRU Model - RMSE: 10.27\n",
      "🔹 GRU Model - R² Score: 0.99\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout, BatchNormalization, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_clean).reshape(X_train_clean.shape[0], 1, X_train_clean.shape[1])\n",
    "X_test_scaled = scaler.transform(X_test_df).reshape(X_test_df.shape[0], 1, X_test_df.shape[1])\n",
    "\n",
    "model = Sequential([\n",
    "    Bidirectional(GRU(128, activation='swish', return_sequences=True, kernel_regularizer=l2(0.0005), input_shape=(X_train_scaled.shape[1], X_train_scaled.shape[2]))),\n",
    "    Dropout(0.2),\n",
    "    GRU(64, activation='swish', return_sequences=False, kernel_regularizer=l2(0.0005)),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='swish'),\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001, clipnorm=1.0)  \n",
    "model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train_clean,\n",
    "    validation_data=(X_test_scaled, y_test),\n",
    "    epochs=200,  \n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "y_pred_gru = model.predict(X_test_scaled)\n",
    "\n",
    "rmse_gru = np.sqrt(mean_squared_error(y_test, y_pred_gru))\n",
    "r2_gru = r2_score(y_test, y_pred_gru)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"🔹 GRU Model - RMSE: {rmse_gru:.2f}\")\n",
    "print(f\"🔹 GRU Model - R² Score: {r2_gru:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/me.pranesh/Desktop/new clean/.venv/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - loss: 18750.8145 - mae: 100.5882 - val_loss: 636.7643 - val_mae: 16.9665\n",
      "Epoch 2/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 913.9343 - mae: 21.3068 - val_loss: 413.4159 - val_mae: 13.6451\n",
      "Epoch 3/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 778.4268 - mae: 19.5393 - val_loss: 377.0806 - val_mae: 12.8709\n",
      "Epoch 4/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 691.6331 - mae: 18.2448 - val_loss: 433.2992 - val_mae: 14.1003\n",
      "Epoch 5/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 619.2137 - mae: 17.3817 - val_loss: 310.9102 - val_mae: 11.2781\n",
      "Epoch 6/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 600.6378 - mae: 17.0142 - val_loss: 291.2910 - val_mae: 10.9845\n",
      "Epoch 7/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 588.3708 - mae: 16.5042 - val_loss: 290.9907 - val_mae: 10.9563\n",
      "Epoch 8/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 606.8854 - mae: 16.6305 - val_loss: 267.5975 - val_mae: 10.2297\n",
      "Epoch 9/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 558.5901 - mae: 15.9181 - val_loss: 243.9745 - val_mae: 9.5152\n",
      "Epoch 10/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 553.7010 - mae: 15.5552 - val_loss: 254.5549 - val_mae: 9.9873\n",
      "Epoch 11/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 515.7198 - mae: 15.2590 - val_loss: 236.3253 - val_mae: 9.3626\n",
      "Epoch 12/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 526.8726 - mae: 15.2174 - val_loss: 217.7386 - val_mae: 9.0800\n",
      "Epoch 13/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 489.3238 - mae: 14.6414 - val_loss: 335.3686 - val_mae: 12.0484\n",
      "Epoch 14/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 451.6827 - mae: 14.0989 - val_loss: 259.4955 - val_mae: 10.2593\n",
      "Epoch 15/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 447.4740 - mae: 13.7258 - val_loss: 304.8124 - val_mae: 11.0961\n",
      "Epoch 16/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 406.3334 - mae: 13.3204 - val_loss: 301.5347 - val_mae: 11.3964\n",
      "Epoch 17/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 375.8098 - mae: 12.7094 - val_loss: 303.1335 - val_mae: 11.4857\n",
      "Epoch 18/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 367.6906 - mae: 12.5402 - val_loss: 411.9016 - val_mae: 12.5483\n",
      "Epoch 19/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 346.6393 - mae: 12.0894 - val_loss: 308.0948 - val_mae: 10.7455\n",
      "Epoch 20/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 297.8826 - mae: 11.3131 - val_loss: 398.2596 - val_mae: 12.1210\n",
      "Epoch 21/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 310.7437 - mae: 11.3989 - val_loss: 624.8947 - val_mae: 15.1377\n",
      "Epoch 22/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 291.2087 - mae: 10.9582 - val_loss: 591.3098 - val_mae: 15.0770\n",
      "Epoch 23/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 281.0279 - mae: 10.6504 - val_loss: 571.7523 - val_mae: 13.8999\n",
      "Epoch 24/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 259.4516 - mae: 10.2471 - val_loss: 954.9045 - val_mae: 18.8941\n",
      "Epoch 25/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 256.1862 - mae: 10.2258 - val_loss: 562.9150 - val_mae: 14.6747\n",
      "Epoch 26/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 236.9328 - mae: 9.9409 - val_loss: 803.4708 - val_mae: 18.0685\n",
      "Epoch 27/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 249.1610 - mae: 9.9176 - val_loss: 801.7092 - val_mae: 17.8304\n",
      "Epoch 28/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 235.6674 - mae: 9.6655 - val_loss: 689.4401 - val_mae: 16.8242\n",
      "Epoch 29/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 221.4105 - mae: 9.4329 - val_loss: 829.7908 - val_mae: 17.7423\n",
      "Epoch 30/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 225.0787 - mae: 9.4640 - val_loss: 624.0342 - val_mae: 15.2231\n",
      "Epoch 31/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 217.8728 - mae: 9.1853 - val_loss: 806.3213 - val_mae: 17.5183\n",
      "Epoch 32/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 198.7437 - mae: 8.8890 - val_loss: 691.5255 - val_mae: 16.2979\n",
      "Epoch 33/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 201.9045 - mae: 8.8987 - val_loss: 589.8915 - val_mae: 14.9027\n",
      "Epoch 34/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 183.7010 - mae: 8.6181 - val_loss: 410.9478 - val_mae: 12.9417\n",
      "Epoch 35/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 197.6163 - mae: 8.6651 - val_loss: 588.2349 - val_mae: 15.9073\n",
      "Epoch 36/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 177.7532 - mae: 8.3937 - val_loss: 584.1820 - val_mae: 15.6474\n",
      "Epoch 37/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 173.3064 - mae: 8.1993 - val_loss: 717.3849 - val_mae: 16.8427\n",
      "Epoch 38/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 179.5221 - mae: 8.2631 - val_loss: 632.5345 - val_mae: 15.3416\n",
      "Epoch 39/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 168.3146 - mae: 8.1144 - val_loss: 762.1134 - val_mae: 17.1372\n",
      "Epoch 40/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 178.1009 - mae: 8.1464 - val_loss: 716.4707 - val_mae: 16.2078\n",
      "Epoch 41/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 185.5397 - mae: 8.1419 - val_loss: 699.9368 - val_mae: 17.3954\n",
      "Epoch 42/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 174.0496 - mae: 8.0585 - val_loss: 921.6199 - val_mae: 19.5631\n",
      "Epoch 43/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 164.3305 - mae: 7.7320 - val_loss: 592.8294 - val_mae: 15.7445\n",
      "Epoch 44/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 159.6440 - mae: 7.6516 - val_loss: 603.4900 - val_mae: 15.5263\n",
      "Epoch 45/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 151.9009 - mae: 7.4705 - val_loss: 724.6436 - val_mae: 16.2895\n",
      "Epoch 46/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 145.0723 - mae: 7.3834 - val_loss: 733.7369 - val_mae: 17.5111\n",
      "Epoch 47/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 152.2521 - mae: 7.4401 - val_loss: 861.1577 - val_mae: 17.7696\n",
      "Epoch 48/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 152.0205 - mae: 7.3042 - val_loss: 733.3659 - val_mae: 17.5618\n",
      "Epoch 49/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 157.4209 - mae: 7.2612 - val_loss: 620.6531 - val_mae: 16.2899\n",
      "Epoch 50/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 160.2069 - mae: 7.2845 - val_loss: 764.7750 - val_mae: 17.6268\n",
      "Epoch 51/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 146.6572 - mae: 7.1526 - val_loss: 737.3660 - val_mae: 17.5805\n",
      "Epoch 52/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 156.4266 - mae: 7.1122 - val_loss: 716.7470 - val_mae: 17.6593\n",
      "Epoch 53/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 142.3286 - mae: 6.9991 - val_loss: 709.0262 - val_mae: 17.9204\n",
      "Epoch 54/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 169.5634 - mae: 7.1892 - val_loss: 692.6523 - val_mae: 16.5181\n",
      "Epoch 55/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 133.1860 - mae: 6.9073 - val_loss: 840.0952 - val_mae: 18.5386\n",
      "Epoch 56/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 149.0934 - mae: 6.9911 - val_loss: 667.5018 - val_mae: 16.1303\n",
      "Epoch 57/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 129.6626 - mae: 6.6565 - val_loss: 651.7368 - val_mae: 16.8331\n",
      "Epoch 58/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 130.6709 - mae: 6.7015 - val_loss: 737.5048 - val_mae: 18.1437\n",
      "Epoch 59/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 133.2295 - mae: 6.6666 - val_loss: 813.5137 - val_mae: 18.8053\n",
      "Epoch 60/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 130.1106 - mae: 6.6917 - val_loss: 718.5331 - val_mae: 17.0995\n",
      "Epoch 61/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 136.2679 - mae: 6.5862 - val_loss: 726.0950 - val_mae: 17.2806\n",
      "Epoch 62/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 137.4642 - mae: 6.5526 - val_loss: 667.6608 - val_mae: 17.1020\n",
      "Epoch 63/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 124.2602 - mae: 6.3702 - val_loss: 711.5118 - val_mae: 17.9777\n",
      "Epoch 64/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 130.9305 - mae: 6.4588 - val_loss: 791.7324 - val_mae: 18.3512\n",
      "Epoch 65/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 117.3466 - mae: 6.2611 - val_loss: 751.0472 - val_mae: 17.4844\n",
      "Epoch 66/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 135.9797 - mae: 6.4779 - val_loss: 786.3582 - val_mae: 18.2967\n",
      "Epoch 67/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 124.4984 - mae: 6.2219 - val_loss: 734.9058 - val_mae: 17.8795\n",
      "Epoch 68/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 122.4700 - mae: 6.2579 - val_loss: 629.9731 - val_mae: 16.2827\n",
      "Epoch 69/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 130.1869 - mae: 6.2050 - val_loss: 744.0980 - val_mae: 18.3662\n",
      "Epoch 70/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 136.1559 - mae: 6.3437 - val_loss: 723.5176 - val_mae: 18.2182\n",
      "Epoch 71/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 118.6074 - mae: 6.1173 - val_loss: 890.0602 - val_mae: 19.2504\n",
      "Epoch 72/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 118.0995 - mae: 5.9893 - val_loss: 902.5395 - val_mae: 19.4567\n",
      "Epoch 73/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 117.4949 - mae: 5.9801 - val_loss: 746.1989 - val_mae: 18.1774\n",
      "Epoch 74/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 114.1624 - mae: 5.9752 - val_loss: 801.8287 - val_mae: 18.1271\n",
      "Epoch 75/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 128.1372 - mae: 5.9848 - val_loss: 905.0525 - val_mae: 19.2539\n",
      "Epoch 76/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 133.0290 - mae: 6.1452 - val_loss: 938.5947 - val_mae: 19.2622\n",
      "Epoch 77/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 122.4668 - mae: 6.0605 - val_loss: 761.0573 - val_mae: 17.8891\n",
      "Epoch 78/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 108.6299 - mae: 5.8946 - val_loss: 966.4095 - val_mae: 19.7967\n",
      "Epoch 79/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 121.5558 - mae: 5.9305 - val_loss: 795.9634 - val_mae: 17.5287\n",
      "Epoch 80/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 115.9698 - mae: 5.8580 - val_loss: 814.2891 - val_mae: 18.3530\n",
      "Epoch 81/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 127.2189 - mae: 5.8611 - val_loss: 715.5670 - val_mae: 17.4505\n",
      "Epoch 82/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 102.3301 - mae: 5.7580 - val_loss: 752.0007 - val_mae: 18.0343\n",
      "Epoch 83/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 126.4164 - mae: 5.9266 - val_loss: 807.8101 - val_mae: 18.0015\n",
      "Epoch 84/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 108.2197 - mae: 5.7403 - val_loss: 607.6008 - val_mae: 16.4085\n",
      "Epoch 85/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 114.8682 - mae: 5.8985 - val_loss: 642.6370 - val_mae: 16.3191\n",
      "Epoch 86/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 116.0443 - mae: 5.8221 - val_loss: 789.4200 - val_mae: 18.2840\n",
      "Epoch 87/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 124.9942 - mae: 5.8469 - val_loss: 830.0247 - val_mae: 19.0062\n",
      "Epoch 88/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 118.5034 - mae: 5.7659 - val_loss: 767.7724 - val_mae: 17.9333\n",
      "Epoch 89/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 112.9368 - mae: 5.7320 - val_loss: 768.8624 - val_mae: 18.0606\n",
      "Epoch 90/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 104.1341 - mae: 5.5787 - val_loss: 801.4284 - val_mae: 18.2540\n",
      "Epoch 91/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 120.4456 - mae: 5.8150 - val_loss: 712.7640 - val_mae: 17.4446\n",
      "Epoch 92/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 116.9218 - mae: 5.7063 - val_loss: 797.1092 - val_mae: 18.0312\n",
      "Epoch 93/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 99.0177 - mae: 5.5073 - val_loss: 883.9363 - val_mae: 19.1407\n",
      "Epoch 94/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 118.1173 - mae: 5.5926 - val_loss: 772.3929 - val_mae: 17.5947\n",
      "Epoch 95/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 100.2272 - mae: 5.5089 - val_loss: 676.9100 - val_mae: 16.6686\n",
      "Epoch 96/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 101.6172 - mae: 5.4848 - val_loss: 630.5180 - val_mae: 16.8790\n",
      "Epoch 97/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 103.5812 - mae: 5.4254 - val_loss: 846.3953 - val_mae: 18.4940\n",
      "Epoch 98/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 111.2256 - mae: 5.5470 - val_loss: 788.7995 - val_mae: 18.7282\n",
      "Epoch 99/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 99.4370 - mae: 5.4010 - val_loss: 728.4344 - val_mae: 17.2899\n",
      "Epoch 100/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 104.7016 - mae: 5.4058 - val_loss: 591.8445 - val_mae: 15.9337\n",
      "Epoch 101/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 107.0986 - mae: 5.4896 - val_loss: 889.6768 - val_mae: 19.2468\n",
      "Epoch 102/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 103.1365 - mae: 5.4081 - val_loss: 852.6418 - val_mae: 18.4599\n",
      "Epoch 103/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 101.5299 - mae: 5.4502 - val_loss: 658.3956 - val_mae: 16.2252\n",
      "Epoch 104/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 100.1820 - mae: 5.4636 - val_loss: 806.3448 - val_mae: 17.9929\n",
      "Epoch 105/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 116.1348 - mae: 5.3608 - val_loss: 787.3492 - val_mae: 18.4603\n",
      "Epoch 106/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 101.0568 - mae: 5.3072 - val_loss: 721.4666 - val_mae: 17.6889\n",
      "Epoch 107/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 97.2734 - mae: 5.3164 - val_loss: 569.7625 - val_mae: 16.6851\n",
      "Epoch 108/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 100.1023 - mae: 5.3330 - val_loss: 641.1183 - val_mae: 17.0435\n",
      "Epoch 109/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 109.0264 - mae: 5.2762 - val_loss: 674.9741 - val_mae: 17.2461\n",
      "Epoch 110/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 103.2645 - mae: 5.3260 - val_loss: 751.3295 - val_mae: 17.9442\n",
      "Epoch 111/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 99.7833 - mae: 5.1848 - val_loss: 721.8191 - val_mae: 17.9263\n",
      "Epoch 112/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 100.6951 - mae: 5.2583 - val_loss: 529.6106 - val_mae: 15.7670\n",
      "Epoch 113/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 95.4968 - mae: 5.2327 - val_loss: 708.4359 - val_mae: 17.7547\n",
      "Epoch 114/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 95.0216 - mae: 5.0877 - val_loss: 923.4100 - val_mae: 20.1237\n",
      "Epoch 115/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 95.6157 - mae: 5.1803 - val_loss: 615.7679 - val_mae: 16.7552\n",
      "Epoch 116/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 97.0676 - mae: 5.2142 - val_loss: 789.5668 - val_mae: 18.5464\n",
      "Epoch 117/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 93.5478 - mae: 5.1442 - val_loss: 759.0862 - val_mae: 18.2708\n",
      "Epoch 118/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 94.1720 - mae: 5.1082 - val_loss: 755.2633 - val_mae: 18.5068\n",
      "Epoch 119/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 113.5576 - mae: 5.3046 - val_loss: 534.1978 - val_mae: 15.4733\n",
      "Epoch 120/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 89.7987 - mae: 5.1183 - val_loss: 931.3196 - val_mae: 19.4894\n",
      "Epoch 121/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 94.9271 - mae: 5.0899 - val_loss: 628.5153 - val_mae: 16.9906\n",
      "Epoch 122/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 81.9746 - mae: 4.9842 - val_loss: 641.5145 - val_mae: 16.6565\n",
      "Epoch 123/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 109.5108 - mae: 5.1104 - val_loss: 887.3491 - val_mae: 19.6390\n",
      "Epoch 124/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 95.8742 - mae: 5.1868 - val_loss: 646.7800 - val_mae: 17.0471\n",
      "Epoch 125/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 86.8778 - mae: 4.9198 - val_loss: 676.6976 - val_mae: 17.1688\n",
      "Epoch 126/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 84.4170 - mae: 4.8960 - val_loss: 603.3430 - val_mae: 16.9143\n",
      "Epoch 127/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 95.6973 - mae: 5.0857 - val_loss: 540.0532 - val_mae: 16.0576\n",
      "Epoch 128/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 96.5669 - mae: 5.0231 - val_loss: 624.8784 - val_mae: 17.4003\n",
      "Epoch 129/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 91.6996 - mae: 5.0012 - val_loss: 750.7311 - val_mae: 18.4033\n",
      "Epoch 130/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 93.8589 - mae: 4.9883 - val_loss: 727.3073 - val_mae: 18.3762\n",
      "Epoch 131/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 94.8368 - mae: 4.8889 - val_loss: 770.5609 - val_mae: 18.7211\n",
      "Epoch 132/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 86.9619 - mae: 4.8224 - val_loss: 713.5573 - val_mae: 17.4951\n",
      "Epoch 133/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 105.9510 - mae: 5.0254 - val_loss: 716.9628 - val_mae: 17.9621\n",
      "Epoch 134/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 97.0217 - mae: 4.9876 - val_loss: 708.4467 - val_mae: 17.8788\n",
      "Epoch 135/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 88.6387 - mae: 4.9174 - val_loss: 749.7803 - val_mae: 18.5708\n",
      "Epoch 136/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 88.7181 - mae: 4.7525 - val_loss: 655.7797 - val_mae: 17.8412\n",
      "Epoch 137/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 101.1645 - mae: 4.9162 - val_loss: 863.8955 - val_mae: 18.9291\n",
      "Epoch 138/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 101.7033 - mae: 4.8330 - val_loss: 680.3869 - val_mae: 17.6686\n",
      "Epoch 139/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 84.4514 - mae: 4.7378 - val_loss: 799.6860 - val_mae: 18.7593\n",
      "Epoch 140/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 95.8721 - mae: 4.8363 - val_loss: 860.4729 - val_mae: 19.8412\n",
      "Epoch 141/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 90.0420 - mae: 4.9003 - val_loss: 650.8833 - val_mae: 17.3560\n",
      "Epoch 142/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 87.1968 - mae: 4.6914 - val_loss: 786.1316 - val_mae: 18.6091\n",
      "Epoch 143/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 82.3396 - mae: 4.7090 - val_loss: 754.3708 - val_mae: 18.5032\n",
      "Epoch 144/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 88.6244 - mae: 4.7638 - val_loss: 673.3837 - val_mae: 17.9453\n",
      "Epoch 145/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 92.2576 - mae: 4.7980 - val_loss: 801.3051 - val_mae: 19.4100\n",
      "Epoch 146/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 90.1748 - mae: 4.7021 - val_loss: 795.3503 - val_mae: 19.3588\n",
      "Epoch 147/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 81.4521 - mae: 4.8215 - val_loss: 670.7972 - val_mae: 17.7961\n",
      "Epoch 148/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 98.9648 - mae: 4.8057 - val_loss: 724.8677 - val_mae: 18.3769\n",
      "Epoch 149/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 78.2485 - mae: 4.7065 - val_loss: 656.6254 - val_mae: 17.5615\n",
      "Epoch 150/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 83.0945 - mae: 4.6442 - val_loss: 805.6449 - val_mae: 19.5199\n",
      "Epoch 151/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 90.9159 - mae: 4.6714 - val_loss: 801.5647 - val_mae: 18.8393\n",
      "Epoch 152/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 80.0841 - mae: 4.6665 - val_loss: 749.6299 - val_mae: 18.7721\n",
      "Epoch 153/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 97.9478 - mae: 4.6516 - val_loss: 723.3805 - val_mae: 18.4392\n",
      "Epoch 154/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 108.5158 - mae: 4.7986 - val_loss: 681.1221 - val_mae: 18.1596\n",
      "Epoch 155/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 85.6849 - mae: 4.6201 - val_loss: 832.3597 - val_mae: 19.6508\n",
      "Epoch 156/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 108.3556 - mae: 4.8115 - val_loss: 692.3160 - val_mae: 17.7672\n",
      "Epoch 157/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 82.5428 - mae: 4.4768 - val_loss: 737.8602 - val_mae: 18.5630\n",
      "Epoch 158/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 96.7474 - mae: 4.7471 - val_loss: 1016.0021 - val_mae: 21.4270\n",
      "Epoch 159/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 88.7159 - mae: 4.7150 - val_loss: 767.9342 - val_mae: 18.7785\n",
      "Epoch 160/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 85.2669 - mae: 4.5756 - val_loss: 759.2963 - val_mae: 18.8786\n",
      "Epoch 161/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 84.6352 - mae: 4.5468 - val_loss: 907.6756 - val_mae: 20.0489\n",
      "Epoch 162/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 79.5408 - mae: 4.5569 - val_loss: 748.0449 - val_mae: 18.6931\n",
      "Epoch 163/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 87.2095 - mae: 4.5838 - val_loss: 928.5024 - val_mae: 20.9638\n",
      "Epoch 164/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 82.2803 - mae: 4.5289 - val_loss: 762.7499 - val_mae: 18.8394\n",
      "Epoch 165/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 101.5485 - mae: 4.7289 - val_loss: 789.6576 - val_mae: 19.3684\n",
      "Epoch 166/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 90.4119 - mae: 4.5251 - val_loss: 801.2433 - val_mae: 19.6097\n",
      "Epoch 167/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 92.4590 - mae: 4.5897 - val_loss: 800.5446 - val_mae: 19.4078\n",
      "Epoch 168/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 84.0300 - mae: 4.6272 - val_loss: 770.2470 - val_mae: 18.6761\n",
      "Epoch 169/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 97.4226 - mae: 4.5814 - val_loss: 667.8514 - val_mae: 18.3034\n",
      "Epoch 170/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 90.8798 - mae: 4.5705 - val_loss: 673.8322 - val_mae: 18.3112\n",
      "Epoch 171/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 85.3827 - mae: 4.5210 - val_loss: 777.9747 - val_mae: 18.9911\n",
      "Epoch 172/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 77.2938 - mae: 4.5036 - val_loss: 729.4741 - val_mae: 18.8060\n",
      "Epoch 173/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 79.7506 - mae: 4.4857 - val_loss: 767.1082 - val_mae: 19.5312\n",
      "Epoch 174/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 84.5977 - mae: 4.4958 - val_loss: 768.7115 - val_mae: 19.3557\n",
      "Epoch 175/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 82.2301 - mae: 4.4256 - val_loss: 844.3062 - val_mae: 19.6461\n",
      "Epoch 176/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 68.6165 - mae: 4.3050 - val_loss: 732.9183 - val_mae: 18.6965\n",
      "Epoch 177/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 74.5436 - mae: 4.4776 - val_loss: 803.1110 - val_mae: 19.4261\n",
      "Epoch 178/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 83.7609 - mae: 4.4304 - val_loss: 731.3253 - val_mae: 18.1631\n",
      "Epoch 179/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 78.3834 - mae: 4.4115 - val_loss: 682.3303 - val_mae: 17.9707\n",
      "Epoch 180/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 79.5158 - mae: 4.4350 - val_loss: 716.5185 - val_mae: 18.8421\n",
      "Epoch 181/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 82.8661 - mae: 4.4761 - val_loss: 798.8655 - val_mae: 19.4642\n",
      "Epoch 182/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 93.1617 - mae: 4.4206 - val_loss: 572.1366 - val_mae: 17.0624\n",
      "Epoch 183/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 73.2026 - mae: 4.3370 - val_loss: 836.6661 - val_mae: 19.4972\n",
      "Epoch 184/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 72.9001 - mae: 4.2693 - val_loss: 854.9600 - val_mae: 20.1950\n",
      "Epoch 185/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 82.1701 - mae: 4.3986 - val_loss: 753.6664 - val_mae: 18.9794\n",
      "Epoch 186/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 69.4463 - mae: 4.2806 - val_loss: 768.5939 - val_mae: 19.4598\n",
      "Epoch 187/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 84.9257 - mae: 4.4603 - val_loss: 841.0212 - val_mae: 19.7461\n",
      "Epoch 188/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 68.4017 - mae: 4.2182 - val_loss: 719.3307 - val_mae: 18.9243\n",
      "Epoch 189/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 81.5501 - mae: 4.3847 - val_loss: 849.6072 - val_mae: 20.0230\n",
      "Epoch 190/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 85.0675 - mae: 4.4034 - val_loss: 928.5363 - val_mae: 20.6934\n",
      "Epoch 191/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 77.8389 - mae: 4.3818 - val_loss: 876.1042 - val_mae: 20.3960\n",
      "Epoch 192/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 76.9799 - mae: 4.3425 - val_loss: 719.4479 - val_mae: 18.6311\n",
      "Epoch 193/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 70.4299 - mae: 4.3145 - val_loss: 700.2932 - val_mae: 18.8468\n",
      "Epoch 194/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 69.9748 - mae: 4.2834 - val_loss: 832.8055 - val_mae: 19.4625\n",
      "Epoch 195/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 76.5086 - mae: 4.3116 - val_loss: 660.4745 - val_mae: 17.8151\n",
      "Epoch 196/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 107.6849 - mae: 4.4198 - val_loss: 758.6110 - val_mae: 18.9235\n",
      "Epoch 197/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 66.9872 - mae: 4.2171 - val_loss: 747.8897 - val_mae: 19.1877\n",
      "Epoch 198/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 84.7836 - mae: 4.3546 - val_loss: 893.5464 - val_mae: 20.1486\n",
      "Epoch 199/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 73.4603 - mae: 4.2032 - val_loss: 772.2838 - val_mae: 19.2078\n",
      "Epoch 200/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 83.1604 - mae: 4.2901 - val_loss: 880.7576 - val_mae: 20.4268\n",
      "Epoch 201/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 79.8029 - mae: 4.2873 - val_loss: 932.0342 - val_mae: 20.8120\n",
      "Epoch 202/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 76.2801 - mae: 4.2149 - val_loss: 721.6275 - val_mae: 19.0168\n",
      "Epoch 203/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 78.4450 - mae: 4.3196 - val_loss: 785.2745 - val_mae: 19.6663\n",
      "Epoch 204/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 82.7839 - mae: 4.3366 - val_loss: 876.8494 - val_mae: 20.0366\n",
      "Epoch 205/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 88.4417 - mae: 4.3830 - val_loss: 799.2198 - val_mae: 19.6781\n",
      "Epoch 206/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 81.3407 - mae: 4.3237 - val_loss: 733.1580 - val_mae: 19.3261\n",
      "Epoch 207/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 75.3744 - mae: 4.3070 - val_loss: 727.2429 - val_mae: 18.8425\n",
      "Epoch 208/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 90.0694 - mae: 4.3370 - val_loss: 817.5061 - val_mae: 19.3756\n",
      "Epoch 209/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 78.6390 - mae: 4.3300 - val_loss: 758.0255 - val_mae: 19.4766\n",
      "Epoch 210/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 76.3922 - mae: 4.3109 - val_loss: 837.5708 - val_mae: 20.0107\n",
      "Epoch 211/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 73.1168 - mae: 4.1619 - val_loss: 641.4841 - val_mae: 17.7532\n",
      "Epoch 212/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 85.5398 - mae: 4.3133 - val_loss: 722.8196 - val_mae: 18.5841\n",
      "Epoch 213/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 76.1582 - mae: 4.2969 - val_loss: 804.7587 - val_mae: 19.6532\n",
      "Epoch 214/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 73.3221 - mae: 4.2280 - val_loss: 744.9539 - val_mae: 19.3216\n",
      "Epoch 215/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 85.1957 - mae: 4.2512 - val_loss: 821.2476 - val_mae: 19.8691\n",
      "Epoch 216/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 81.2689 - mae: 4.2385 - val_loss: 791.0865 - val_mae: 19.2850\n",
      "Epoch 217/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 72.6626 - mae: 4.2054 - val_loss: 757.8727 - val_mae: 19.2714\n",
      "Epoch 218/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 80.7105 - mae: 4.2561 - val_loss: 781.9087 - val_mae: 19.5816\n",
      "Epoch 219/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 76.3472 - mae: 4.2110 - val_loss: 790.0608 - val_mae: 19.7300\n",
      "Epoch 220/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 77.8779 - mae: 4.2351 - val_loss: 702.7115 - val_mae: 18.7779\n",
      "Epoch 221/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 75.1227 - mae: 4.2236 - val_loss: 841.4973 - val_mae: 20.0559\n",
      "Epoch 222/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 88.3841 - mae: 4.3078 - val_loss: 871.7005 - val_mae: 20.6334\n",
      "Epoch 223/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 85.0040 - mae: 4.2618 - val_loss: 729.7901 - val_mae: 19.2417\n",
      "Epoch 224/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 114.1038 - mae: 4.3463 - val_loss: 772.4606 - val_mae: 19.6421\n",
      "Epoch 225/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 78.3238 - mae: 4.2301 - val_loss: 829.9528 - val_mae: 20.4299\n",
      "Epoch 226/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 75.4247 - mae: 4.1962 - val_loss: 772.6392 - val_mae: 19.6450\n",
      "Epoch 227/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 77.2810 - mae: 4.2733 - val_loss: 745.7213 - val_mae: 19.2419\n",
      "Epoch 228/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 69.9841 - mae: 4.1395 - val_loss: 890.1550 - val_mae: 20.6434\n",
      "Epoch 229/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 75.3642 - mae: 4.1667 - val_loss: 869.7949 - val_mae: 20.4235\n",
      "Epoch 230/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 68.3554 - mae: 4.1074 - val_loss: 756.0256 - val_mae: 19.2389\n",
      "Epoch 231/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 93.4229 - mae: 4.3806 - val_loss: 823.2355 - val_mae: 19.9761\n",
      "Epoch 232/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 81.1727 - mae: 4.3170 - val_loss: 838.9119 - val_mae: 20.1560\n",
      "Epoch 233/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 74.6846 - mae: 4.1835 - val_loss: 799.2329 - val_mae: 19.6958\n",
      "Epoch 234/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 70.6782 - mae: 4.1961 - val_loss: 738.5396 - val_mae: 19.4571\n",
      "Epoch 235/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 68.9985 - mae: 4.1656 - val_loss: 749.4809 - val_mae: 19.2595\n",
      "Epoch 236/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 75.9953 - mae: 4.2495 - val_loss: 691.9720 - val_mae: 18.2907\n",
      "Epoch 237/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 70.8603 - mae: 4.0741 - val_loss: 829.0363 - val_mae: 20.2078\n",
      "Epoch 238/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 86.6674 - mae: 4.2554 - val_loss: 897.7297 - val_mae: 20.8326\n",
      "Epoch 239/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 77.4164 - mae: 4.2852 - val_loss: 874.6983 - val_mae: 20.5556\n",
      "Epoch 240/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 75.0827 - mae: 4.1532 - val_loss: 915.0034 - val_mae: 20.8624\n",
      "Epoch 241/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 71.0761 - mae: 4.1399 - val_loss: 721.0903 - val_mae: 19.3861\n",
      "Epoch 242/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 83.5330 - mae: 4.2010 - val_loss: 818.2206 - val_mae: 19.7056\n",
      "Epoch 243/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 77.6051 - mae: 4.1085 - val_loss: 816.0475 - val_mae: 20.3021\n",
      "Epoch 244/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 62.9947 - mae: 4.1152 - val_loss: 786.5994 - val_mae: 19.6598\n",
      "Epoch 245/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 75.3166 - mae: 4.1938 - val_loss: 888.5776 - val_mae: 20.6370\n",
      "Epoch 246/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 68.1561 - mae: 4.0559 - val_loss: 778.0042 - val_mae: 19.5878\n",
      "Epoch 247/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 66.7262 - mae: 4.0878 - val_loss: 820.7681 - val_mae: 19.7138\n",
      "Epoch 248/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 72.6698 - mae: 4.0907 - val_loss: 815.1390 - val_mae: 20.0978\n",
      "Epoch 249/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 69.0647 - mae: 4.1240 - val_loss: 772.5606 - val_mae: 19.3409\n",
      "Epoch 250/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 74.7165 - mae: 4.0634 - val_loss: 868.1655 - val_mae: 20.5178\n",
      "Epoch 251/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 69.5424 - mae: 4.1551 - val_loss: 824.0156 - val_mae: 20.0576\n",
      "Epoch 252/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 94.7377 - mae: 4.1679 - val_loss: 734.1530 - val_mae: 18.9019\n",
      "Epoch 253/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 74.4566 - mae: 4.1458 - val_loss: 804.8306 - val_mae: 20.0666\n",
      "Epoch 254/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 72.5698 - mae: 4.1748 - val_loss: 798.0338 - val_mae: 19.4255\n",
      "Epoch 255/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 79.1474 - mae: 4.1815 - val_loss: 813.2393 - val_mae: 19.8985\n",
      "Epoch 256/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 69.7112 - mae: 4.1467 - val_loss: 803.4551 - val_mae: 19.5243\n",
      "Epoch 257/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 71.5032 - mae: 4.1421 - val_loss: 838.7056 - val_mae: 19.8499\n",
      "Epoch 258/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 75.9437 - mae: 4.1074 - val_loss: 720.2906 - val_mae: 19.1657\n",
      "Epoch 259/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 84.7526 - mae: 4.1196 - val_loss: 865.3330 - val_mae: 20.2853\n",
      "Epoch 260/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 68.7951 - mae: 4.1112 - val_loss: 743.9446 - val_mae: 19.1049\n",
      "Epoch 261/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 75.1203 - mae: 4.2112 - val_loss: 810.8235 - val_mae: 20.2317\n",
      "Epoch 262/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 63.5186 - mae: 3.9829 - val_loss: 796.3030 - val_mae: 19.9051\n",
      "Epoch 263/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 71.0377 - mae: 4.0668 - val_loss: 932.1431 - val_mae: 21.3532\n",
      "Epoch 264/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 68.6654 - mae: 4.0426 - val_loss: 766.2666 - val_mae: 19.2795\n",
      "Epoch 265/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 83.8981 - mae: 4.0803 - val_loss: 775.8083 - val_mae: 19.8138\n",
      "Epoch 266/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 64.8811 - mae: 4.0201 - val_loss: 843.4073 - val_mae: 19.9709\n",
      "Epoch 267/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 68.2348 - mae: 4.0535 - val_loss: 901.3519 - val_mae: 20.7031\n",
      "Epoch 268/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 68.5793 - mae: 4.0630 - val_loss: 889.2368 - val_mae: 20.6377\n",
      "Epoch 269/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 61.2500 - mae: 3.8806 - val_loss: 746.8947 - val_mae: 19.3960\n",
      "Epoch 270/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 70.2618 - mae: 4.0616 - val_loss: 858.9844 - val_mae: 20.3036\n",
      "Epoch 271/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 63.2846 - mae: 3.9560 - val_loss: 871.9348 - val_mae: 20.4860\n",
      "Epoch 272/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 68.3539 - mae: 4.0969 - val_loss: 875.0458 - val_mae: 20.5671\n",
      "Epoch 273/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 67.5110 - mae: 3.9911 - val_loss: 873.5631 - val_mae: 20.6255\n",
      "Epoch 274/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 66.5044 - mae: 4.0401 - val_loss: 840.4399 - val_mae: 20.4165\n",
      "Epoch 275/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 55.7379 - mae: 3.8065 - val_loss: 932.6031 - val_mae: 20.8364\n",
      "Epoch 276/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 66.3578 - mae: 4.0110 - val_loss: 906.3244 - val_mae: 20.9905\n",
      "Epoch 277/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 66.7000 - mae: 3.9713 - val_loss: 840.9286 - val_mae: 20.3777\n",
      "Epoch 278/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 82.5463 - mae: 4.1097 - val_loss: 852.4628 - val_mae: 20.6451\n",
      "Epoch 279/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 58.9384 - mae: 3.8792 - val_loss: 779.1039 - val_mae: 19.7809\n",
      "Epoch 280/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 67.1561 - mae: 4.0051 - val_loss: 940.9462 - val_mae: 21.3089\n",
      "Epoch 281/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 57.0224 - mae: 3.9708 - val_loss: 783.4183 - val_mae: 20.1644\n",
      "Epoch 282/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 63.5582 - mae: 3.9797 - val_loss: 720.0505 - val_mae: 19.0977\n",
      "Epoch 283/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 67.5822 - mae: 3.9761 - val_loss: 832.2532 - val_mae: 20.2177\n",
      "Epoch 284/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 66.1684 - mae: 4.0464 - val_loss: 832.7744 - val_mae: 20.2406\n",
      "Epoch 285/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 59.6850 - mae: 3.8996 - val_loss: 732.2388 - val_mae: 19.1063\n",
      "Epoch 286/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 71.9384 - mae: 4.0601 - val_loss: 820.5991 - val_mae: 19.9151\n",
      "Epoch 287/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 63.7081 - mae: 3.9307 - val_loss: 820.0298 - val_mae: 20.5239\n",
      "Epoch 288/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 57.2213 - mae: 3.9053 - val_loss: 878.9206 - val_mae: 20.6129\n",
      "Epoch 289/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 67.4408 - mae: 4.0056 - val_loss: 892.6210 - val_mae: 20.7226\n",
      "Epoch 290/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 66.4293 - mae: 3.9810 - val_loss: 820.9754 - val_mae: 19.8372\n",
      "Epoch 291/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 69.0573 - mae: 4.0574 - val_loss: 775.8382 - val_mae: 19.3434\n",
      "Epoch 292/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 65.4129 - mae: 3.9090 - val_loss: 809.0912 - val_mae: 19.9527\n",
      "Epoch 293/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 66.7884 - mae: 3.9683 - val_loss: 742.2234 - val_mae: 19.4069\n",
      "Epoch 294/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 66.4906 - mae: 3.9710 - val_loss: 771.9381 - val_mae: 19.6173\n",
      "Epoch 295/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 63.0970 - mae: 3.9850 - val_loss: 791.2390 - val_mae: 20.0168\n",
      "Epoch 296/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 60.1263 - mae: 3.8965 - val_loss: 837.2346 - val_mae: 20.3292\n",
      "Epoch 297/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 69.3212 - mae: 3.9447 - val_loss: 764.0969 - val_mae: 19.7541\n",
      "Epoch 298/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 67.5514 - mae: 3.9919 - val_loss: 999.5020 - val_mae: 21.5658\n",
      "Epoch 299/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 64.5272 - mae: 3.9697 - val_loss: 1041.0165 - val_mae: 22.1473\n",
      "Epoch 300/300\n",
      "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - loss: 62.9163 - mae: 3.9040 - val_loss: 798.9763 - val_mae: 19.8240\n",
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Enhanced GRU Model - RMSE: 28.20\n",
      "🔹 Enhanced GRU Model - R² Score: 0.93\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Bidirectional, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_clean)\n",
    "X_test_scaled = scaler.transform(X_test_df)\n",
    "\n",
    "X_train_scaled = np.reshape(X_train_scaled, (X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\n",
    "X_test_scaled = np.reshape(X_test_scaled, (X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))\n",
    "\n",
    "model = Sequential([\n",
    "    Bidirectional(GRU(256, activation='swish', return_sequences=True, kernel_regularizer=l2(0.0005), input_shape=(X_train_scaled.shape[1], X_train_scaled.shape[2]))),\n",
    "    Dropout(0.2),\n",
    "    GRU(128, activation='swish', return_sequences=True, kernel_regularizer=l2(0.0005)),\n",
    "    Dropout(0.2),\n",
    "    GRU(64, activation='swish', return_sequences=False, kernel_regularizer=l2(0.0005)),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='swish'),\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "optimizer = Adam(learning_rate=0.0005, clipnorm=1.0)\n",
    "model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train_clean,\n",
    "    validation_data=(X_test_scaled, y_test),\n",
    "    epochs=300, \n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "y_pred_gru = model.predict(X_test_scaled)\n",
    "\n",
    "rmse_gru = np.sqrt(np.mean((y_test - y_pred_gru.squeeze()) ** 2))\n",
    "r2_gru = 1 - (np.sum((y_test - y_pred_gru.squeeze())**2) / np.sum((y_test - np.mean(y_test))**2))\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"🔹 Enhanced GRU Model - RMSE: {rmse_gru:.2f}\")\n",
    "print(f\"🔹 Enhanced GRU Model - R² Score: {r2_gru:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/my_new_env/lib/python3.9/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 8.7376 - val_loss: 7.7942\n",
      "Epoch 2/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 8.3035 - val_loss: 7.2321\n",
      "Epoch 3/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 7.7714 - val_loss: 6.7585\n",
      "Epoch 4/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 6.9177 - val_loss: 6.2990\n",
      "Epoch 5/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 6.5725 - val_loss: 5.8138\n",
      "Epoch 6/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 5.6887 - val_loss: 5.3015\n",
      "Epoch 7/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 5.3981 - val_loss: 4.7443\n",
      "Epoch 8/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 4.9759 - val_loss: 4.1609\n",
      "Epoch 9/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 4.1631 - val_loss: 3.5546\n",
      "Epoch 10/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 3.5077 - val_loss: 2.9374\n",
      "Epoch 11/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 2.8234 - val_loss: 2.3482\n",
      "Epoch 12/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2.3057 - val_loss: 1.8093\n",
      "Epoch 13/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.8839 - val_loss: 1.3520\n",
      "Epoch 14/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.4351 - val_loss: 0.9989\n",
      "Epoch 15/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1.0831 - val_loss: 0.7559\n",
      "Epoch 16/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.8717 - val_loss: 0.5834\n",
      "Epoch 17/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.7121 - val_loss: 0.4613\n",
      "Epoch 18/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.6217 - val_loss: 0.3749\n",
      "Epoch 19/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.4641 - val_loss: 0.3166\n",
      "Epoch 20/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.3966 - val_loss: 0.2757\n",
      "Epoch 21/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.3732 - val_loss: 0.2453\n",
      "Epoch 22/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.3255 - val_loss: 0.2247\n",
      "Epoch 23/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2353 - val_loss: 0.2138\n",
      "Epoch 24/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2518 - val_loss: 0.2092\n",
      "Epoch 25/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2180 - val_loss: 0.2072\n",
      "Epoch 26/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1936 - val_loss: 0.2051\n",
      "Epoch 27/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1921 - val_loss: 0.2050\n",
      "Epoch 28/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1599 - val_loss: 0.2022\n",
      "Epoch 29/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1823 - val_loss: 0.1974\n",
      "Epoch 30/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1827 - val_loss: 0.1935\n",
      "Epoch 31/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.1795 - val_loss: 0.1907\n",
      "Epoch 32/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1709 - val_loss: 0.1894\n",
      "Epoch 33/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1624 - val_loss: 0.1848\n",
      "Epoch 34/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1574 - val_loss: 0.1809\n",
      "Epoch 35/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1536 - val_loss: 0.1734\n",
      "Epoch 36/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1457 - val_loss: 0.1711\n",
      "Epoch 37/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1502 - val_loss: 0.1741\n",
      "Epoch 38/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1556 - val_loss: 0.1752\n",
      "Epoch 39/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1287 - val_loss: 0.1760\n",
      "Epoch 40/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1290 - val_loss: 0.1751\n",
      "Epoch 41/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1237 - val_loss: 0.1720\n",
      "Epoch 42/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1282 - val_loss: 0.1664\n",
      "Epoch 43/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1357 - val_loss: 0.1632\n",
      "Epoch 44/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1271 - val_loss: 0.1621\n",
      "Epoch 45/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1352 - val_loss: 0.1638\n",
      "Epoch 46/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1151 - val_loss: 0.1675\n",
      "Epoch 47/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1227 - val_loss: 0.1729\n",
      "Epoch 48/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1224 - val_loss: 0.1745\n",
      "Epoch 49/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1202 - val_loss: 0.1715\n",
      "Epoch 50/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1241 - val_loss: 0.1634\n",
      "Epoch 51/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1252 - val_loss: 0.1568\n",
      "Epoch 52/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1086 - val_loss: 0.1520\n",
      "Epoch 53/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1173 - val_loss: 0.1501\n",
      "Epoch 54/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1174 - val_loss: 0.1494\n",
      "Epoch 55/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1088 - val_loss: 0.1476\n",
      "Epoch 56/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1034 - val_loss: 0.1441\n",
      "Epoch 57/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1137 - val_loss: 0.1411\n",
      "Epoch 58/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1018 - val_loss: 0.1422\n",
      "Epoch 59/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0970 - val_loss: 0.1456\n",
      "Epoch 60/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1046 - val_loss: 0.1446\n",
      "Epoch 61/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1090 - val_loss: 0.1409\n",
      "Epoch 62/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1055 - val_loss: 0.1404\n",
      "Epoch 63/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0949 - val_loss: 0.1422\n",
      "Epoch 64/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0925 - val_loss: 0.1420\n",
      "Epoch 65/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1056 - val_loss: 0.1385\n",
      "Epoch 66/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1041 - val_loss: 0.1357\n",
      "Epoch 67/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0994 - val_loss: 0.1313\n",
      "Epoch 68/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0971 - val_loss: 0.1273\n",
      "Epoch 69/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0908 - val_loss: 0.1252\n",
      "Epoch 70/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1005 - val_loss: 0.1230\n",
      "Epoch 71/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0994 - val_loss: 0.1211\n",
      "Epoch 72/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0924 - val_loss: 0.1236\n",
      "Epoch 73/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0965 - val_loss: 0.1282\n",
      "Epoch 74/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0982 - val_loss: 0.1282\n",
      "Epoch 75/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0845 - val_loss: 0.1288\n",
      "Epoch 76/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0928 - val_loss: 0.1263\n",
      "Epoch 77/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0868 - val_loss: 0.1231\n",
      "Epoch 78/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0905 - val_loss: 0.1227\n",
      "Epoch 79/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0822 - val_loss: 0.1202\n",
      "Epoch 80/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0783 - val_loss: 0.1189\n",
      "Epoch 81/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0731 - val_loss: 0.1185\n",
      "Epoch 82/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0808 - val_loss: 0.1205\n",
      "Epoch 83/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0795 - val_loss: 0.1227\n",
      "Epoch 84/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0754 - val_loss: 0.1238\n",
      "Epoch 85/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0663 - val_loss: 0.1193\n",
      "Epoch 86/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0801 - val_loss: 0.1117\n",
      "Epoch 87/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0833 - val_loss: 0.1059\n",
      "Epoch 88/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0753 - val_loss: 0.1074\n",
      "Epoch 89/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0764 - val_loss: 0.1087\n",
      "Epoch 90/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0710 - val_loss: 0.1077\n",
      "Epoch 91/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0698 - val_loss: 0.1065\n",
      "Epoch 92/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0639 - val_loss: 0.1048\n",
      "Epoch 93/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0711 - val_loss: 0.1045\n",
      "Epoch 94/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0747 - val_loss: 0.1055\n",
      "Epoch 95/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0687 - val_loss: 0.1041\n",
      "Epoch 96/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0657 - val_loss: 0.1050\n",
      "Epoch 97/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0692 - val_loss: 0.1040\n",
      "Epoch 98/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0636 - val_loss: 0.0998\n",
      "Epoch 99/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0635 - val_loss: 0.0965\n",
      "Epoch 100/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0616 - val_loss: 0.0939\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "🔹 ANFIS Model - RMSE: 0.31\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import skfuzzy as fuzz\n",
    "from skfuzzy import control as ctrl\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_clean)\n",
    "X_test_scaled = scaler.transform(X_test_df)\n",
    "\n",
    "def fuzzy_membership(x, low, mid, high):\n",
    "    return fuzz.trimf(x, [low, mid, high])\n",
    "\n",
    "fuzzy_inputs = []\n",
    "for i in range(X_train_scaled.shape[1]):\n",
    "    var = ctrl.Antecedent(np.linspace(-3, 3, 100), f'feature_{i}')\n",
    "    var['low'] = fuzz.trimf(var.universe, [-3, -2, -1])\n",
    "    var['medium'] = fuzz.trimf(var.universe, [-1, 0, 1])\n",
    "    var['high'] = fuzz.trimf(var.universe, [1, 2, 3])\n",
    "    fuzzy_inputs.append(var)\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),  \n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1, activation='linear')  \n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mse')\n",
    "\n",
    "history = model.fit(X_train_scaled, y_train_clean, validation_data=(X_test_scaled, y_test), epochs=100, batch_size=32, verbose=1)\n",
    "\n",
    "y_pred_anfis = model.predict(X_test_scaled)\n",
    "rmse_anfis = np.sqrt(mean_squared_error(y_test, y_pred_anfis))\n",
    "\n",
    "print(f\"🔹 ANFIS Model - RMSE: {rmse_anfis:.2f}\")\n",
    "\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('ANFIS Training Curve')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/my_new_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[I 2025-02-20 00:58:27,778] A new study created in memory with name: no-name-e61a4eb4-ee5e-4fe6-bb58-ddb81c666e2d\n",
      "[I 2025-02-20 00:58:27,910] Trial 0 finished with value: 0.2327153385751782 and parameters: {'n_estimators': 322, 'learning_rate': 0.025283600420828882, 'max_depth': 7, 'min_child_weight': 2, 'subsample': 0.7203617890267988, 'colsample_bytree': 0.8882567859845867, 'gamma': 0.6557606866255726, 'lambda': 5.411039738814689, 'alpha': 0.08902598348579538}. Best is trial 0 with value: 0.2327153385751782.\n",
      "[I 2025-02-20 00:58:28,131] Trial 1 finished with value: 0.30726675497014744 and parameters: {'n_estimators': 994, 'learning_rate': 0.240310464323584, 'max_depth': 9, 'min_child_weight': 3, 'subsample': 0.6205633597268083, 'colsample_bytree': 0.6112756406354524, 'gamma': 0.8287052174874479, 'lambda': 8.228918264838773, 'alpha': 1.985850961920852}. Best is trial 0 with value: 0.2327153385751782.\n",
      "[I 2025-02-20 00:58:28,249] Trial 2 finished with value: 0.4597710205979227 and parameters: {'n_estimators': 469, 'learning_rate': 0.06732084891585609, 'max_depth': 11, 'min_child_weight': 8, 'subsample': 0.536057601972854, 'colsample_bytree': 0.59422924982691, 'gamma': 0.6950720751408528, 'lambda': 7.2454337106919, 'alpha': 5.094437270411695}. Best is trial 0 with value: 0.2327153385751782.\n",
      "[I 2025-02-20 00:58:28,321] Trial 3 finished with value: 0.3474199683871128 and parameters: {'n_estimators': 179, 'learning_rate': 0.03269977947509804, 'max_depth': 10, 'min_child_weight': 8, 'subsample': 0.63955803693084, 'colsample_bytree': 0.8350411544405045, 'gamma': 0.9711870393254585, 'lambda': 8.275015340743465, 'alpha': 1.517400269460124}. Best is trial 0 with value: 0.2327153385751782.\n",
      "[I 2025-02-20 00:58:28,419] Trial 4 finished with value: 0.5023842381822619 and parameters: {'n_estimators': 394, 'learning_rate': 0.11619687694051628, 'max_depth': 7, 'min_child_weight': 8, 'subsample': 0.9365573822346013, 'colsample_bytree': 0.8187363254953219, 'gamma': 0.3479391930869642, 'lambda': 4.402724180082869, 'alpha': 9.689508538082311}. Best is trial 0 with value: 0.2327153385751782.\n",
      "[I 2025-02-20 00:58:28,547] Trial 5 finished with value: 0.40540740606520176 and parameters: {'n_estimators': 242, 'learning_rate': 0.02129024489646723, 'max_depth': 4, 'min_child_weight': 1, 'subsample': 0.6990884775855443, 'colsample_bytree': 0.5750698278053046, 'gamma': 0.2618905200720093, 'lambda': 1.6949657141739949, 'alpha': 0.35471961418910847}. Best is trial 0 with value: 0.2327153385751782.\n",
      "[I 2025-02-20 00:58:28,652] Trial 6 finished with value: 0.4601629888999739 and parameters: {'n_estimators': 455, 'learning_rate': 0.29266283322408976, 'max_depth': 7, 'min_child_weight': 1, 'subsample': 0.6274917149863285, 'colsample_bytree': 0.876447367698801, 'gamma': 0.6863349875447483, 'lambda': 0.8380891369150034, 'alpha': 7.159616946865549}. Best is trial 0 with value: 0.2327153385751782.\n",
      "[I 2025-02-20 00:58:28,800] Trial 7 finished with value: 0.3685756770481172 and parameters: {'n_estimators': 648, 'learning_rate': 0.22756672891916946, 'max_depth': 7, 'min_child_weight': 6, 'subsample': 0.5889052223441967, 'colsample_bytree': 0.7516661585969459, 'gamma': 0.4557717452247251, 'lambda': 3.0838331670602104, 'alpha': 4.062010769524322}. Best is trial 0 with value: 0.2327153385751782.\n",
      "[I 2025-02-20 00:58:28,946] Trial 8 finished with value: 0.29220760386365896 and parameters: {'n_estimators': 595, 'learning_rate': 0.09095994931308055, 'max_depth': 7, 'min_child_weight': 8, 'subsample': 0.8693808751838714, 'colsample_bytree': 0.6330951863170573, 'gamma': 0.698728933388518, 'lambda': 5.095966354110787, 'alpha': 1.509203223045682}. Best is trial 0 with value: 0.2327153385751782.\n",
      "[I 2025-02-20 00:58:29,169] Trial 9 finished with value: 0.3510144800192502 and parameters: {'n_estimators': 959, 'learning_rate': 0.20607393950508424, 'max_depth': 3, 'min_child_weight': 6, 'subsample': 0.626346778168629, 'colsample_bytree': 0.8933976575071874, 'gamma': 0.04487873258577213, 'lambda': 9.327667223148273, 'alpha': 6.422322805556817}. Best is trial 0 with value: 0.2327153385751782.\n",
      "[I 2025-02-20 00:58:29,358] Trial 10 finished with value: 0.33250435115004345 and parameters: {'n_estimators': 757, 'learning_rate': 0.15414603050354073, 'max_depth': 5, 'min_child_weight': 4, 'subsample': 0.7968736321148111, 'colsample_bytree': 0.9942436248854986, 'gamma': 0.5316273370771958, 'lambda': 5.993237231665302, 'alpha': 3.7707251781055335}. Best is trial 0 with value: 0.2327153385751782.\n",
      "[I 2025-02-20 00:58:29,456] Trial 11 finished with value: 0.22741496720437626 and parameters: {'n_estimators': 303, 'learning_rate': 0.09326747050315862, 'max_depth': 5, 'min_child_weight': 10, 'subsample': 0.8244177581561709, 'colsample_bytree': 0.7112140850472166, 'gamma': 0.6522634011724513, 'lambda': 4.803462856290475, 'alpha': 0.06329248404495935}. Best is trial 11 with value: 0.22741496720437626.\n",
      "[I 2025-02-20 00:58:29,561] Trial 12 finished with value: 0.23133889128009483 and parameters: {'n_estimators': 302, 'learning_rate': 0.058188000695924516, 'max_depth': 5, 'min_child_weight': 10, 'subsample': 0.7788430521856707, 'colsample_bytree': 0.7040054343467369, 'gamma': 0.5432588483804348, 'lambda': 3.475848864450825, 'alpha': 0.311361486054594}. Best is trial 11 with value: 0.22741496720437626.\n",
      "[I 2025-02-20 00:58:29,615] Trial 13 finished with value: 0.31954258810054664 and parameters: {'n_estimators': 130, 'learning_rate': 0.12507274177063854, 'max_depth': 5, 'min_child_weight': 10, 'subsample': 0.822709922900431, 'colsample_bytree': 0.7087498463413381, 'gamma': 0.5098577580383841, 'lambda': 3.0950393981087836, 'alpha': 2.925446036126708}. Best is trial 11 with value: 0.22741496720437626.\n",
      "[I 2025-02-20 00:58:29,731] Trial 14 finished with value: 0.2632858983978672 and parameters: {'n_estimators': 301, 'learning_rate': 0.06643135102052723, 'max_depth': 5, 'min_child_weight': 10, 'subsample': 0.9011461023428646, 'colsample_bytree': 0.5114196340844561, 'gamma': 0.21164281775415827, 'lambda': 3.6073009035132158, 'alpha': 0.24347447392449856}. Best is trial 11 with value: 0.22741496720437626.\n",
      "[I 2025-02-20 00:58:29,830] Trial 15 finished with value: 0.33599442257122064 and parameters: {'n_estimators': 364, 'learning_rate': 0.15919207464363777, 'max_depth': 3, 'min_child_weight': 10, 'subsample': 0.9934313070678049, 'colsample_bytree': 0.7078035035539127, 'gamma': 0.8888041451939054, 'lambda': 2.2439444529184795, 'alpha': 2.3777260416445043}. Best is trial 11 with value: 0.22741496720437626.\n",
      "[I 2025-02-20 00:58:29,882] Trial 16 finished with value: 0.5458312153391318 and parameters: {'n_estimators': 112, 'learning_rate': 0.06518247564522536, 'max_depth': 5, 'min_child_weight': 9, 'subsample': 0.78103396578486, 'colsample_bytree': 0.7489693029575791, 'gamma': 0.4090990665500009, 'lambda': 0.17561390125611132, 'alpha': 9.818449227112414}. Best is trial 11 with value: 0.22741496720437626.\n",
      "[I 2025-02-20 00:58:30,025] Trial 17 finished with value: 0.24897131461259725 and parameters: {'n_estimators': 506, 'learning_rate': 0.10995392651473716, 'max_depth': 9, 'min_child_weight': 5, 'subsample': 0.8609637334269541, 'colsample_bytree': 0.6660315867440265, 'gamma': 0.582053122564645, 'lambda': 5.945385964061913, 'alpha': 0.9006589206229503}. Best is trial 11 with value: 0.22741496720437626.\n",
      "[I 2025-02-20 00:58:30,110] Trial 18 finished with value: 0.3464892266952959 and parameters: {'n_estimators': 266, 'learning_rate': 0.1496203365145333, 'max_depth': 6, 'min_child_weight': 7, 'subsample': 0.752663665231915, 'colsample_bytree': 0.7589383335782205, 'gamma': 0.7959134841893499, 'lambda': 4.220695452119477, 'alpha': 3.3075575418636913}. Best is trial 11 with value: 0.22741496720437626.\n",
      "[I 2025-02-20 00:58:30,393] Trial 19 finished with value: 0.40945078425899006 and parameters: {'n_estimators': 741, 'learning_rate': 0.05576577491332306, 'max_depth': 4, 'min_child_weight': 9, 'subsample': 0.6815172024659386, 'colsample_bytree': 0.6755293650702664, 'gamma': 0.5842844056079537, 'lambda': 6.824009431463248, 'alpha': 5.014852605450973}. Best is trial 11 with value: 0.22741496720437626.\n",
      "[I 2025-02-20 00:58:30,498] Trial 20 finished with value: 0.3822230483953765 and parameters: {'n_estimators': 208, 'learning_rate': 0.09440911092033247, 'max_depth': 12, 'min_child_weight': 9, 'subsample': 0.8373911988431841, 'colsample_bytree': 0.5347827502161708, 'gamma': 0.019356199167949728, 'lambda': 2.0609192396984364, 'alpha': 6.804164943575763}. Best is trial 11 with value: 0.22741496720437626.\n",
      "[I 2025-02-20 00:58:30,743] Trial 21 finished with value: 0.29237107816384034 and parameters: {'n_estimators': 357, 'learning_rate': 0.010153544301911566, 'max_depth': 6, 'min_child_weight': 3, 'subsample': 0.7206651868438179, 'colsample_bytree': 0.9815244372341, 'gamma': 0.6398737339373844, 'lambda': 5.231363111926822, 'alpha': 0.017028606363380716}. Best is trial 11 with value: 0.22741496720437626.\n",
      "[I 2025-02-20 00:58:30,865] Trial 22 finished with value: 0.29238377709645813 and parameters: {'n_estimators': 318, 'learning_rate': 0.04068962334754973, 'max_depth': 8, 'min_child_weight': 3, 'subsample': 0.7565032965591608, 'colsample_bytree': 0.7991117065796567, 'gamma': 0.7583373052279563, 'lambda': 4.205085839801909, 'alpha': 1.2142495615763633}. Best is trial 11 with value: 0.22741496720437626.\n",
      "[I 2025-02-20 00:58:31,017] Trial 23 finished with value: 0.2428949645236842 and parameters: {'n_estimators': 423, 'learning_rate': 0.08521909392887395, 'max_depth': 6, 'min_child_weight': 2, 'subsample': 0.7949808081099128, 'colsample_bytree': 0.9350245753988441, 'gamma': 0.6045141240565015, 'lambda': 5.750184494972986, 'alpha': 0.9236243316854909}. Best is trial 11 with value: 0.22741496720437626.\n",
      "[I 2025-02-20 00:58:31,184] Trial 24 finished with value: 0.29632312396939003 and parameters: {'n_estimators': 523, 'learning_rate': 0.041692271271712406, 'max_depth': 4, 'min_child_weight': 5, 'subsample': 0.7275051758082547, 'colsample_bytree': 0.786893406598749, 'gamma': 0.3888495844480717, 'lambda': 6.483533349716934, 'alpha': 2.24970923608999}. Best is trial 11 with value: 0.22741496720437626.\n",
      "[I 2025-02-20 00:58:31,299] Trial 25 finished with value: 0.41013335371068715 and parameters: {'n_estimators': 195, 'learning_rate': 0.01392914466126061, 'max_depth': 8, 'min_child_weight': 7, 'subsample': 0.6671865964870187, 'colsample_bytree': 0.7107372320269648, 'gamma': 0.8705951882103513, 'lambda': 2.967566236675407, 'alpha': 0.13714785861680262}. Best is trial 11 with value: 0.22741496720437626.\n",
      "[I 2025-02-20 00:58:31,404] Trial 26 finished with value: 0.49797264616710485 and parameters: {'n_estimators': 290, 'learning_rate': 0.042930691315989034, 'max_depth': 6, 'min_child_weight': 4, 'subsample': 0.9127188242735945, 'colsample_bytree': 0.8660260609703169, 'gamma': 0.4853862462668482, 'lambda': 4.626623467701903, 'alpha': 8.634376851848781}. Best is trial 11 with value: 0.22741496720437626.\n",
      "[I 2025-02-20 00:58:31,557] Trial 27 finished with value: 0.3323109003982205 and parameters: {'n_estimators': 577, 'learning_rate': 0.07968607299720709, 'max_depth': 4, 'min_child_weight': 10, 'subsample': 0.7723611464562338, 'colsample_bytree': 0.6695567155207951, 'gamma': 0.7479212101860256, 'lambda': 3.594822111146345, 'alpha': 2.695156773332446}. Best is trial 11 with value: 0.22741496720437626.\n",
      "[I 2025-02-20 00:58:31,665] Trial 28 finished with value: 0.20823891609372028 and parameters: {'n_estimators': 357, 'learning_rate': 0.13295266297649444, 'max_depth': 8, 'min_child_weight': 7, 'subsample': 0.8198527649505399, 'colsample_bytree': 0.947363877901289, 'gamma': 0.30189807344387665, 'lambda': 7.720360594213104, 'alpha': 0.7436557826107952}. Best is trial 28 with value: 0.20823891609372028.\n",
      "[I 2025-02-20 00:58:31,776] Trial 29 finished with value: 0.20479555947292477 and parameters: {'n_estimators': 365, 'learning_rate': 0.184446139729029, 'max_depth': 9, 'min_child_weight': 7, 'subsample': 0.9635564893515538, 'colsample_bytree': 0.9102980510856824, 'gamma': 0.15134349926758484, 'lambda': 9.878926229465293, 'alpha': 0.814017914776404}. Best is trial 29 with value: 0.20479555947292477.\n",
      "[I 2025-02-20 00:58:31,926] Trial 30 finished with value: 0.3294020032390348 and parameters: {'n_estimators': 397, 'learning_rate': 0.1998815838190892, 'max_depth': 9, 'min_child_weight': 7, 'subsample': 0.9823678990381253, 'colsample_bytree': 0.9339555107095376, 'gamma': 0.12629600868850777, 'lambda': 9.83586435241423, 'alpha': 4.399922943639756}. Best is trial 29 with value: 0.20479555947292477.\n",
      "[I 2025-02-20 00:58:32,031] Trial 31 finished with value: 0.2122519090737767 and parameters: {'n_estimators': 341, 'learning_rate': 0.13932835410257247, 'max_depth': 10, 'min_child_weight': 9, 'subsample': 0.9403305039610493, 'colsample_bytree': 0.9447108865992455, 'gamma': 0.26624159559377936, 'lambda': 8.658308347141986, 'alpha': 0.7747513290731722}. Best is trial 29 with value: 0.20479555947292477.\n",
      "[I 2025-02-20 00:58:32,155] Trial 32 finished with value: 0.25202393900651526 and parameters: {'n_estimators': 446, 'learning_rate': 0.18107817497151213, 'max_depth': 10, 'min_child_weight': 9, 'subsample': 0.9488899637919676, 'colsample_bytree': 0.943081997244527, 'gamma': 0.2743888546890219, 'lambda': 8.531704316879377, 'alpha': 1.7647478291664878}. Best is trial 29 with value: 0.20479555947292477.\n",
      "[I 2025-02-20 00:58:32,244] Trial 33 finished with value: 0.20197188985360964 and parameters: {'n_estimators': 241, 'learning_rate': 0.13474002826202636, 'max_depth': 10, 'min_child_weight': 7, 'subsample': 0.957986373572933, 'colsample_bytree': 0.911791067207605, 'gamma': 0.13991802041335133, 'lambda': 7.733021158115949, 'alpha': 0.7505269587244652}. Best is trial 33 with value: 0.20197188985360964.\n",
      "[I 2025-02-20 00:58:32,328] Trial 34 finished with value: 0.21008647864725102 and parameters: {'n_estimators': 233, 'learning_rate': 0.1279251323944565, 'max_depth': 10, 'min_child_weight': 6, 'subsample': 0.9592263376601797, 'colsample_bytree': 0.9084866806446025, 'gamma': 0.167522079156528, 'lambda': 7.686691724947624, 'alpha': 0.8654060180517584}. Best is trial 33 with value: 0.20197188985360964.\n",
      "[I 2025-02-20 00:58:32,408] Trial 35 finished with value: 0.24914999972547894 and parameters: {'n_estimators': 234, 'learning_rate': 0.16916739621290217, 'max_depth': 11, 'min_child_weight': 6, 'subsample': 0.9655680538857727, 'colsample_bytree': 0.9005301772658637, 'gamma': 0.1308887002918799, 'lambda': 7.637339968995817, 'alpha': 1.9255442102838438}. Best is trial 33 with value: 0.20197188985360964.\n",
      "[I 2025-02-20 00:58:32,474] Trial 36 finished with value: 0.2167790311102513 and parameters: {'n_estimators': 152, 'learning_rate': 0.1371666671876026, 'max_depth': 9, 'min_child_weight': 7, 'subsample': 0.8979157117740542, 'colsample_bytree': 0.8525289681792598, 'gamma': 0.15047127666423707, 'lambda': 7.5929326120784575, 'alpha': 1.3001474878497805}. Best is trial 33 with value: 0.20197188985360964.\n",
      "[I 2025-02-20 00:58:32,546] Trial 37 finished with value: 0.3461799789602139 and parameters: {'n_estimators': 199, 'learning_rate': 0.1837997523740476, 'max_depth': 11, 'min_child_weight': 6, 'subsample': 0.91682821693407, 'colsample_bytree': 0.9695640830774743, 'gamma': 0.06855052060913439, 'lambda': 9.282629506393112, 'alpha': 5.91813383480776}. Best is trial 33 with value: 0.20197188985360964.\n",
      "[I 2025-02-20 00:58:32,637] Trial 38 finished with value: 0.21033990559255925 and parameters: {'n_estimators': 253, 'learning_rate': 0.11530640920818302, 'max_depth': 8, 'min_child_weight': 8, 'subsample': 0.9980620070889087, 'colsample_bytree': 0.909042993817757, 'gamma': 0.20137427447651823, 'lambda': 7.954602540800084, 'alpha': 0.7619276464903478}. Best is trial 33 with value: 0.20197188985360964.\n",
      "[I 2025-02-20 00:58:32,745] Trial 39 finished with value: 0.3000691678205602 and parameters: {'n_estimators': 395, 'learning_rate': 0.2543743994807779, 'max_depth': 10, 'min_child_weight': 5, 'subsample': 0.8668263262880173, 'colsample_bytree': 0.8367284484423764, 'gamma': 0.32540969428867594, 'lambda': 7.0284306959763, 'alpha': 3.0722633656743414}. Best is trial 33 with value: 0.20197188985360964.\n",
      "[I 2025-02-20 00:58:32,884] Trial 40 finished with value: 0.2082867435073745 and parameters: {'n_estimators': 485, 'learning_rate': 0.2221543773630532, 'max_depth': 12, 'min_child_weight': 7, 'subsample': 0.5333414602402209, 'colsample_bytree': 0.9214057792078562, 'gamma': 0.08798559794296722, 'lambda': 9.395382529200472, 'alpha': 1.816013510916046}. Best is trial 33 with value: 0.20197188985360964.\n",
      "[I 2025-02-20 00:58:33,042] Trial 41 finished with value: 0.20551461624994385 and parameters: {'n_estimators': 482, 'learning_rate': 0.21783640615199973, 'max_depth': 12, 'min_child_weight': 7, 'subsample': 0.538843565424919, 'colsample_bytree': 0.9159106761055151, 'gamma': 0.1003403276054083, 'lambda': 9.969916640577704, 'alpha': 1.6696842148085076}. Best is trial 33 with value: 0.20197188985360964.\n",
      "[I 2025-02-20 00:58:33,235] Trial 42 finished with value: 0.20183193536928995 and parameters: {'n_estimators': 633, 'learning_rate': 0.23241481017641016, 'max_depth': 12, 'min_child_weight': 7, 'subsample': 0.5124614756495186, 'colsample_bytree': 0.9559520265991094, 'gamma': 0.08311572536833538, 'lambda': 9.796362761189425, 'alpha': 1.6507959315951521}. Best is trial 42 with value: 0.20183193536928995.\n",
      "[I 2025-02-20 00:58:33,439] Trial 43 finished with value: 0.19879962059806158 and parameters: {'n_estimators': 648, 'learning_rate': 0.2735228174462143, 'max_depth': 12, 'min_child_weight': 8, 'subsample': 0.5774184934567081, 'colsample_bytree': 0.9628517712057771, 'gamma': 0.002808376195995624, 'lambda': 9.737174066383016, 'alpha': 2.4417912054495785}. Best is trial 43 with value: 0.19879962059806158.\n",
      "[I 2025-02-20 00:58:33,614] Trial 44 finished with value: 0.25100091162690275 and parameters: {'n_estimators': 659, 'learning_rate': 0.2631589358896365, 'max_depth': 12, 'min_child_weight': 8, 'subsample': 0.505844245376566, 'colsample_bytree': 0.9985807334721968, 'gamma': 0.020043397305416776, 'lambda': 9.904906334424277, 'alpha': 3.506645065027183}. Best is trial 43 with value: 0.19879962059806158.\n",
      "[I 2025-02-20 00:58:33,775] Trial 45 finished with value: 0.21680637314915918 and parameters: {'n_estimators': 649, 'learning_rate': 0.2997905897134604, 'max_depth': 11, 'min_child_weight': 8, 'subsample': 0.5870350460377262, 'colsample_bytree': 0.9684909087933485, 'gamma': 0.08471814516331297, 'lambda': 8.850639974021952, 'alpha': 2.3590220852771777}. Best is trial 43 with value: 0.19879962059806158.\n",
      "[I 2025-02-20 00:58:33,979] Trial 46 finished with value: 0.17621872568947375 and parameters: {'n_estimators': 706, 'learning_rate': 0.2698713756699468, 'max_depth': 11, 'min_child_weight': 8, 'subsample': 0.5825600800688615, 'colsample_bytree': 0.8786123732854083, 'gamma': 0.005010945539420749, 'lambda': 9.064078512341762, 'alpha': 1.5488945146006134}. Best is trial 46 with value: 0.17621872568947375.\n",
      "[I 2025-02-20 00:58:34,160] Trial 47 finished with value: 0.2565763787080192 and parameters: {'n_estimators': 740, 'learning_rate': 0.27694113598360165, 'max_depth': 11, 'min_child_weight': 8, 'subsample': 0.5826337094258397, 'colsample_bytree': 0.8798872163868858, 'gamma': 0.224964580133388, 'lambda': 9.178202691590887, 'alpha': 2.649796268975038}. Best is trial 46 with value: 0.17621872568947375.\n",
      "[I 2025-02-20 00:58:34,374] Trial 48 finished with value: 0.25282540791975144 and parameters: {'n_estimators': 819, 'learning_rate': 0.23928321492599358, 'max_depth': 12, 'min_child_weight': 8, 'subsample': 0.5600508544147469, 'colsample_bytree': 0.9590715370333525, 'gamma': 0.006764189489756434, 'lambda': 8.410547148370833, 'alpha': 4.287679992734489}. Best is trial 46 with value: 0.17621872568947375.\n",
      "[I 2025-02-20 00:58:34,577] Trial 49 finished with value: 0.19234434758385457 and parameters: {'n_estimators': 814, 'learning_rate': 0.2763472016156676, 'max_depth': 11, 'min_child_weight': 6, 'subsample': 0.6091452668673041, 'colsample_bytree': 0.8474003446055144, 'gamma': 0.05776093041125028, 'lambda': 8.941271006294862, 'alpha': 1.4097865750632388}. Best is trial 46 with value: 0.17621872568947375.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Optimized XGBoost Model - RMSE: 0.19\n",
      "🔹 Optimized XGBoost Model - Best Parameters: {'n_estimators': 706, 'learning_rate': 0.2698713756699468, 'max_depth': 11, 'min_child_weight': 8, 'subsample': 0.5825600800688615, 'colsample_bytree': 0.8786123732854083, 'gamma': 0.005010945539420749, 'lambda': 9.064078512341762, 'alpha': 1.5488945146006134}\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0, 1),\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 0, 10),\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 0, 10),\n",
    "        \"random_state\": 42\n",
    "    }\n",
    "\n",
    "    model = xgb.XGBRegressor(**params)\n",
    "    model.fit(X_train_clean, y_train_clean)\n",
    "    y_pred = model.predict(X_test_df)\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    return rmse\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "best_params = study.best_params\n",
    "optimized_xgb = xgb.XGBRegressor(**best_params)\n",
    "optimized_xgb.fit(X_train_clean, y_train_clean)\n",
    "\n",
    "y_pred_xgb = optimized_xgb.predict(X_test_df)\n",
    "rmse_xgb = np.sqrt(mean_squared_error(y_test, y_pred_xgb))\n",
    "\n",
    "print(f\" Optimized XGBoost Model - RMSE: {rmse_xgb:.2f}\")\n",
    "print(f\" Optimized XGBoost Model - Best Parameters: {best_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Cross-Validation RMSE Scores: [0.14326249 0.20814351 0.34096241 0.19246225 0.23112977]\n",
      "🔹 Mean CV RMSE: 0.22\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cv_rmse = np.sqrt(-cross_val_score(optimized_xgb, X_train_clean, y_train_clean, \n",
    "                                   scoring='neg_mean_squared_error', cv=5))\n",
    "print(f\"🔹 Cross-Validation RMSE Scores: {cv_rmse}\")\n",
    "print(f\"🔹 Mean CV RMSE: {cv_rmse.mean():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Optimized XGBoost Model - RMSE: 0.18\n",
      "🔹 Optimized XGBoost Model - MSE: 0.03\n",
      "🔹 Optimized XGBoost Model - MAE: 0.14\n",
      "🔹 Optimized XGBoost Model - R² Score: 0.97\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "xgboost_model = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    n_estimators=706,\n",
    "    learning_rate=0.2698713756699468,\n",
    "    max_depth=11,\n",
    "    min_child_weight=8,\n",
    "    subsample=0.5825600800688615,\n",
    "    colsample_bytree=0.8786123732854083,\n",
    "    gamma=0.005010945539420749,\n",
    "    reg_lambda=9.064078512341762,\n",
    "    reg_alpha=1.5488945146006134,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgboost_model.fit(X_train_clean, y_train_clean)\n",
    "\n",
    "y_pred_xgboost = xgboost_model.predict(X_test_df)\n",
    "\n",
    "rmse_xgboost = np.sqrt(mean_squared_error(y_test, y_pred_xgboost))\n",
    "mse_xgboost = mean_squared_error(y_test, y_pred_xgboost)\n",
    "mae_xgboost = mean_absolute_error(y_test, y_pred_xgboost)\n",
    "r2_xgboost = r2_score(y_test, y_pred_xgboost)\n",
    "\n",
    "print(f\" Optimized XGBoost Model - RMSE: {rmse_xgboost:.2f}\")\n",
    "print(f\" Optimized XGBoost Model - MSE: {mse_xgboost:.2f}\")\n",
    "print(f\" Optimized XGBoost Model - MAE: {mae_xgboost:.2f}\")\n",
    "print(f\" Optimized XGBoost Model - R² Score: {r2_xgboost:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
